<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> CS-330 Lecture 2: Transfer Learning and Meta-Learning | Lars Quaedvlieg </title> <meta name="author" content="Lars C.P.M. Quaedvlieg"> <meta name="description" content="This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to learn how to transfer knowledge from one task to another, discuss what it means for two tasks to share a common structure, and start thinking about meta learning."> <meta property="og:site_name" content="Lars Quaedvlieg"> <meta property="og:type" content="article"> <meta property="og:title" content="Lars Quaedvlieg | CS-330 Lecture 2: Transfer Learning and Meta-Learning"> <meta property="og:url" content="https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-tl-ml/"> <meta property="og:description" content="This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to learn how to transfer knowledge from one task to another, discuss what it means for two tasks to share a common structure, and start thinking about meta learning."> <meta property="og:image" content="/assets/img/profile.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="CS-330 Lecture 2: Transfer Learning and Meta-Learning"> <meta name="twitter:description" content="This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to learn how to transfer knowledge from one task to another, discuss what it means for two tasks to share a common structure, and start thinking about meta learning."> <meta name="twitter:image" content="/assets/img/profile.png"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?004339841cbc43800ab5ac9276b4716d"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-tl-ml/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "CS-330 Lecture 2: Transfer Learning and Meta-Learning",
            "description": "This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to learn how to transfer knowledge from one task to another, discuss what it means for two tasks to share a common structure, and start thinking about meta learning.",
            "published": "March 03, 2024",
            "authors": [
              
              {
                "author": "Lars C.P.M. Quaedvlieg",
                "authorURL": "https://lars-quaedvlieg.github.io/",
                "affiliations": [
                  {
                    "name": "EPFL",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Lars Quaedvlieg </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Curriculum Vitae </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>CS-330 Lecture 2: Transfer Learning and Meta-Learning</h1> <p>This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to learn how to transfer knowledge from one task to another, discuss what it means for two tasks to share a common structure, and start thinking about meta learning.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#transfer-learning">Transfer learning</a> </div> <ul> <li> <a href="#transfer-learning-through-fine-tuning">Transfer learning through fine-tuning</a> </li> </ul> <div> <a href="#introduction-to-meta-learning">Introduction to meta learning</a> </div> <ul> <li> <a href="#a-probabilistic-view-on-meta-learning">A probabilistic view on meta learning</a> </li> <li> <a href="#how-does-meta-learning-work">How does meta learning work?</a> </li> <li> <a href="#a-general-recipe-for-meta-learning-algorithms">A general recipe for meta learning algorithms</a> </li> </ul> </nav> </d-contents> <p>The goal of this lecture is to learn how to transfer knowledge from one task to another, discuss what it means for two tasks to share a common structure, and start thinking about meta learning. If you missed the previous lecture, which was about multi-task learning, you can head over <a href="/blog/2024/cs330-stanford-mtl/">here</a> to view it.</p> <p>As always, since I am still new to this blogging thing, reach out to me if you have any feedback on my writing, the flow of information, or whatever! You can contact me through <a href="https://www.linkedin.com/in/lars-quaedvlieg/" rel="external nofollow noopener" target="_blank">LinkedIn</a>. ☺</p> <p>The link to the lecture slides can be found <a href="https://cs330.stanford.edu/materials/cs330_finetune_transfer_meta_learning_problem_setup_2023.pdf" rel="external nofollow noopener" target="_blank">here</a>.</p> <h2 id="transfer-learning">Transfer learning</h2> <p>In contrast to multi-task learning, which tackles several tasks $\mathcal{T}_1, \cdots, \mathcal{T}_i$ simultaneously, transfer learning takes a sequential approach. It focuses on mastering a specific task $\mathcal{T}_b$ after the knowledge has been acquired from source task(s) $\mathcal{T}_a$. A common assumption is that $\mathcal{D}_a$ cannot be accessed during the transfer.</p> <p>Transfer learning is a valid solution to multi-task learning, because it can sequentially apply knowledge from one task to another. This is unlike multi-task learning, which requires simultaneous learning of all tasks.</p> <p>It is advantageous in the case of a large dataset $\mathcal{D}_a$, where continuous retraining is not feasible. Transfer learning makes sense here by utilizing the acquired knowledge without the need for repetitive training on the large dataset. Additionally, when you do not need to train two tasks simultaneously, you might opt to solve it them sequentially using transfer learning.</p> <h3 id="transfer-learning-through-fine-tuning">Transfer learning through fine-tuning</h3> \[\phi \leftarrow \theta - \alpha \nabla_\theta \mathcal{L}(\theta, \mathcal{D}_\mathrm{tr})\;.\] <p>One method of transfer learning involves the fine-tuning of a pre-trained model with parameters $\theta$. This process starts with a model whose parameters have been <b>initially trained on a large, diverse dataset</b>, such as ImageNet <d-cite key="huh2016makes"></d-cite>. The usefulness of fine-tuning lies in its ability to adapt these pre-trained parameters to a new task $\mathcal{T}_b$ by continuing the training process with a dataset $\mathcal{D}_\mathrm{tr}$ specific to that task. Typically, this involves many iterations of gradient descent steps, where the pre-trained model's parameters $\theta$ are updated by moving in the direction that minimizes the loss $\mathcal{L}$. This optimization process is depicted in the equation above.</p> <p>When utilizing fine-tuning for transfer learning, there are several common design choices that are usually considered:</p> <ul> <li>Opting for a <strong>lower learning rate</strong> to prevent the overwriting of the knowledge captured during pre-training.</li> <li>Employing even <strong>smaller learning rates for the earlier layers</strong> of the network, preserving more generic features.</li> <li>Initially <strong>freezing the early layers</strong> of the model, then gradually unfreezing them as training progresses.</li> <li> <strong>Reinitializing the last layer</strong> to tailor it to the specifics of the new task.</li> <li> <strong>Searching over hyperparameters</strong> using cross-validation to find the optimal configuration.</li> <li>Making smart choices about the <strong>model architecture</strong>.</li> </ul> <figure class="figure col-sm-11 float-right"> <img src="/assets/img/blog/cs330/3/pretraining_datasets.png" class="img-fluid" alt="Alt text."> <figcaption class="figure-caption text-center"> Aggregate performance of a model across 10 finetuning datasets when it is (i) randomly initialized (ii) pretrained on upstream corpus (BookWiki) (iii) pretrained on the finetuning dataset itself.</figcaption> </figure> <p>❗However, this common knowledge does not always hold true. For example, when using unsupervised pre-trained objectives, you may not require diverse data for pre-training. The figure above shows that a model that was pretrained on the downstream dataset performs similarly to an upstream corpus such as BookWiki <d-cite key="krishna2022downstream"></d-cite>.</p> <figure class="figure col-sm-11 float-right"> <img src="/assets/img/blog/cs330/3/layer_tuning.png" class="img-fluid" alt="Alt text."> <figcaption class="figure-caption text-center"> How fine-tuning different layers has different effects.</figcaption> </figure> <p>Furthermore, depending on the downstream task, it may be better to tune the first or middle layers, rather than the last layers. For example, for image corruption, it makes more sense to fine-tune the first layer of the model, since it’s more of an input-level shift in data distribution <d-cite key="lee2022surgical"></d-cite>. The figure below shows more of these examples. Chelsea’s advice is to first <strong>train the last layer</strong>, and then <strong>fine-tune the entire network</strong> <d-cite key="kumar2022fine"></d-cite>, since fine-tuning can distort pre-trained features.</p> <figure class="figure col-sm-12 float-right"> <img src="/assets/img/blog/cs330/3/tl_dataset_size.png" class="img-fluid" alt="Alt text."> <figcaption class="figure-caption text-center">The effect of fine-tuning with different dataset sizes.</figcaption> </figure> <p>However, one big disadvantage to fine-tuning is that <strong>it does not work well for very small target datasets</strong>. An example of this can be seen in the figure above. Luckily, this is where meta learning comes into play!</p> <hr> <h2 id="introduction-to-meta-learning">Introduction to meta learning</h2> <p>With transfer learning, we initialize a model and then hope that it helps to solve the target task, by for example fine-tuning it. With meta-learning, we are asking the question of whether we can <strong>explicitly optimize for transferability</strong>. Thus, given a set of training tasks, can we optimize the ability to learn these tasks quickly, so that we can learn <em>new</em> tasks quickly too.</p> <p>When learning a task, we are very roughly mapping a task dataset to a set of model parameters through a function $\mathcal{D}^\mathrm{tr}_i \rightarrow \theta$. In meta learning, we are asking whether we can optimize this function for a small $\mathcal{D}_i^\mathrm{tr}$.</p> <p>There are two ways to view meta-learning algorithms:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <p><b>(1) Mechanistic view</b></p> <p>Construct a deep network that can read in an entire dataset and make predictions for new datapoints. Training this network uses a meta-dataset, which itself consists of many datasets, each for a different task.</p> </div> <div class="col-sm mt-3 mt-md-0"> <p><b>(2) Probabilistic view</b></p> <p>Extract shared prior knowledge from a set of tasks that allows for efficient learning of new tasks. Then, learning a new task uses this prior and a (small) training set to infer the most likely posterior parameters.</p> </div> </div> <h3 id="a-probabilistic-view-on-meta-learning">A probabilistic view on meta learning</h3> <div> <figure class="figure col-sm-5 float-right"> <img src="/assets/img/blog/cs330/3/mtl_graphical_model.png" class="img-fluid" alt="Alt text."> <figcaption class="figure-caption text-center">Graphical model of multi-task- and meta-learning.</figcaption> </figure> <p>Expanding on the probabilistic (Bayesian) view of meta learning, let’s look at a graphical model for multi-task and meta learning. To quickly recap what a graphical model represents, consider two random variables $X$ and $Y$. If there is an arrow from $X$ to $Y$ in the graphical model, it means that $p(Y\vert X) \neq P(Y)$, meaning that the random variable $Y$ is dependent on $X$. Furthermore, you can nest certain variables (the rounded squared squares with $i$ and $j$) in the figure on the right, if you would like to repeat them for different indices.</p> </div> <p>Now we can start interpreting the graphical model for multi-task learning. Merging the training and testing sets, we immediately see that the target variable of datapoint $j$ for task $i$, denoted as $y_{i,j}$, is dependent on both the input data $x_{i,j}$ and the task-specific “true” parameter(s) $\phi_i$. The only difference between the training and testing data is that the target variables of the test dataset are not observed, whilst the others are all latent variables (unobserved). As we saw, for each task, we have task-specific true parameters $\phi_i$. However, if we share some <strong>common structure</strong> between multiple tasks, we can condition these parameters on this common structure. Hence, $\theta$ defines the parameters related to the shared structure between different tasks.</p> <p>This shared structure means that task parameters $\phi_{i_1}, \phi_{i_2}$ become independent when conditioning on the shared parameters $\theta$: $\phi_{i_1} \perp \phi_{i_2} \vert \theta$. Furthermore, the entropy of $p(\phi_i \vert \theta)$ is lower than $p(\phi_i)$, as there is less distributional noise from common structure.</p> <p>Let’s now have a thought exercise. If we can identify $\theta$, then when should learning $\phi_i$ be faster than learning from scratch? Let’s think about one extreme. If the shared information fully describes the task-specific information, we would see that $p(\phi_i \vert \theta) = p(\phi_i \vert \phi_i) = 1$. From that, we can see that it is faster if there is a lot of common information about the task that is captured by $\theta$. In a more general case, if the entropy $\mathcal{H}(p(\phi_i\vert\theta)) = 0$, meaning that you can predict $\phi_i$ with 100% accuracy given $\theta$, you can learn the fastest. However, this does not necessarily mean that $\phi_i = \theta$!</p> <p>From all of this, we can define <strong>structure</strong> as a <strong>statistical dependence</strong> on <strong>shared latent information $\theta$.</strong> Let’s now see some examples of the type of information that $\theta$ may contain.</p> <ul> <li>In a multi-task sinusoid problem, $\theta$ corresponds to the family of sinusoid functions, which is everything except the phase and amplitude.</li> <li>In a multi-language machine translation problem, $\theta$ corresponds to the family of all language pairs.</li> </ul> <p>Note that $\theta$ is <strong>narrower</strong> than the space of all possible functions!</p> <p>We will discuss this probabilistic view on meta learning more in later lectures, but for the remainder of this lecture, we will switch back to the mechanistic view.</p> <h3 id="how-does-meta-learning-work">How does meta learning work?</h3> <figure class="figure col-sm-12 float-right"> <img src="/assets/img/blog/cs330/3/ml_example.png" class="img-fluid" alt="Alt text."> <figcaption class="figure-caption text-center">Example of a meta-learning object classification problem.</figcaption> </figure> <p>Let’s consider an image classification problem, where you have different tasks. In the problem above, the different tasks contain different images to classify. For the (meta-)training process, we have tasks $\mathcal{T}_1, \mathcal{T}_2, \cdots, \mathcal{T}_n$ and we would like to do meta-testing on a new task $\mathcal{T}_\mathrm{test}$. The goal is to learn to solve task $\mathcal{T}_\mathrm{test}$ more quickly than from scratch. We can then test after training on the few examples from the new (in this case testing) task $\mathcal{T}_\mathrm{test}$. Of course, this problem settings generalizes to any other machine learning problem like regression, language generation, skill learning, etc.</p> <p>The <b>key assumption</b> here is that meta-training tasks and meta-testing tasks are drawn i.i.d. from the same task distribution $\mathcal{T}_1, \cdots, \mathcal{T}_n,\mathcal{T}_\mathrm{test} \sim p(\mathcal{T})$, meaning that tasks must share structure.</p> <p>Analogous to more data in machine learning, the more tasks, the better! You can say that meta learning is transfer learning with many source tasks.</p> <p>The following is some terminology for different things you will hear when talking about meta learning:</p> <ul> <li>The task-specific training set $\mathcal{D}_i^\mathrm{tr}$ is often referred to as the <em>support set</em> or the <em>context</em>.</li> <li>The task test dataset $\mathcal{D}_i^\mathrm{test}$ is called the <em>query set</em>.</li> <li> <em>k-shot learning</em> refers to learning with <strong>k</strong> examples per class.</li> </ul> <h3 id="a-general-recipe-for-meta-learning-algorithms">A general recipe for meta learning algorithms</h3> <p>Let’s formalize meta supervised learning in a mechanistic view. We are looking for a function $y^\mathrm{ts} = f_\theta(\mathcal{D}^\mathrm{tr}, x^\mathrm{ts})$, which is trained on the data $\{\mathcal{D}_i\}_{i=1,\cdots,n}$. This formulation reduces the meta-learning problem to the design and optimization of $f_\theta$.</p> <p>To approach a problem using meta learning, you will need to decide on two steps:</p> <ol> <li>What is my form of $f_\theta(\mathcal{D}^\mathrm{tr}, x^\mathrm{ts})$?</li> <li>How do I optimize the meta-parameters $\theta$ with respect to the maximum-likelihood objective using meta-training data.</li> </ol> <p>The following lectures will focus on core methods for meta learning and unsupervised pre-trained methods!</p> <hr> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/blog/cs330/2024-03-03-tl-ml.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Lars C.P.M. Quaedvlieg. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-JKH10LEP3Y"></script> <script defer src="/assets/js/google-analytics-setup.js"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>