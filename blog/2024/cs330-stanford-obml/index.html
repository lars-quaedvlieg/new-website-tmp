<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> CS-330 Lecture 4: Optimization-Based Meta-Learning | Lars Quaedvlieg </title> <meta name="author" content="Lars C.P.M. Quaedvlieg"> <meta name="description" content="This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to understand the basics of optimization-based meta learning techniques. You will also learn about the trade-offs between black-box and optimization-based meta learning!"> <meta property="og:site_name" content="Lars Quaedvlieg"> <meta property="og:type" content="article"> <meta property="og:title" content="Lars Quaedvlieg | CS-330 Lecture 4: Optimization-Based Meta-Learning"> <meta property="og:url" content="https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-obml/"> <meta property="og:description" content="This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to understand the basics of optimization-based meta learning techniques. You will also learn about the trade-offs between black-box and optimization-based meta learning!"> <meta property="og:image" content="/assets/img/blog/cs330/5/oml_approach.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="CS-330 Lecture 4: Optimization-Based Meta-Learning"> <meta name="twitter:description" content="This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to understand the basics of optimization-based meta learning techniques. You will also learn about the trade-offs between black-box and optimization-based meta learning!"> <meta name="twitter:image" content="/assets/img/blog/cs330/5/oml_approach.png"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?004339841cbc43800ab5ac9276b4716d"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-obml/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "CS-330 Lecture 4: Optimization-Based Meta-Learning",
            "description": "This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to understand the basics of optimization-based meta learning techniques. You will also learn about the trade-offs between black-box and optimization-based meta learning!",
            "published": "March 10, 2024",
            "authors": [
              
              {
                "author": "Lars C.P.M. Quaedvlieg",
                "authorURL": "https://lars-quaedvlieg.github.io/",
                "affiliations": [
                  {
                    "name": "EPFL",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Lars Quaedvlieg </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Curriculum Vitae </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>CS-330 Lecture 4: Optimization-Based Meta-Learning</h1> <p>This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to understand the basics of optimization-based meta learning techniques. You will also learn about the trade-offs between black-box and optimization-based meta learning!</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#overall-approach">Overall approach</a> </div> <div> <a href="#compare-optimization-based-vs-black-box">Compare: optimization-based vs. black-box</a> </div> <ul> <li> <a href="#comparing-performances">Comparing performances</a> </li> </ul> <div> <a href="#challenges-and-solutions">Challenges and solutions</a> </div> <div> <a href="#case-study-of-land-cover-classification">Case study of land cover classification</a> </div> </nav> </d-contents> <p>The goal of this lecture is to understand the <strong>basics of optimization-based meta learning</strong> techniques. You will also learn about the <strong>trade-offs</strong> between black-box and optimization-based meta learning! If you missed the previous lecture, which was about black-box meta learning and in-context learning with GPT-3, you can head over <a href="/blog/2024/cs330-stanford-bbml-icl/">here</a> to view it.</p> <p>As always, since I am still new to this blogging thing, reach out to me if you have any feedback on my writing, the flow of information, or whatever! You can contact me through <a href="https://www.linkedin.com/in/lars-quaedvlieg/" rel="external nofollow noopener" target="_blank">LinkedIn</a>. ☺</p> <p>The link to the lecture slides can be found <a href="https://cs330.stanford.edu/materials/cs330_optbased_metalearning_2023.pdf" rel="external nofollow noopener" target="_blank">here</a>.</p> <h2 id="overall-approach">Overall approach</h2> <p>In the previous post, we looked into black-box meta learning. To recap, this approach attempts to output task-specific parameters or contextual information with some meta-model. One major benefit of this approach is its <strong>expressiveness</strong>. However, it also requires solving a challenging optimization problem, which is incredibly data-inefficient.</p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/5/oml_approach.png" class="img-fluid" alt="Alt text."> <figcaption class="figure-caption text-center">Computation pipeline for optimization-based meta learning.</figcaption> </figure> <p>In this post, we will focus on <strong>optimized-based meta learning</strong>. The key idea behind this is to <strong>embed optimization</strong> inside the inner learning process. In the figure above, an example of this idea is depicted. With some initial model $f_\theta$, we will run gradient descent on the datapoints in $\mathcal{D}_i^\mathrm{tr}$ to produce the task-specific network with parameters $\phi_i$. In summary, the goal will be to find the model parameters $\theta$ such that optimizing these parameters to specific tasks is as <strong>effective</strong> and <strong>efficient</strong> as possible.</p> <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/5/fine_tune_loss.png" class="img-fluid" alt="Alt text."> <figcaption class="figure-caption text-center">Results on model fine-tuning with respect to dataset size.</figcaption> </figure> <p>As recalled from a previous post, we are trying to do a similar thing in fine-tuning. Specifically, in fine-tuning, we are using a pre-trained model with parameters $\theta$ to find new task-specific parameters $\phi \leftarrow \theta - \alpha \nabla_\theta \mathcal{L}(\theta, \mathcal{D}^\mathrm{tr})$. We also saw that fine-tuning often performed much better than training from scratch. However, fine-tuning generally needs a lot of data in order to adapt well to a new task, meaning it doesn’t perform well with few-shot learning.</p> </div> <p>You may now see that our proposed optimization-based meta learning approach tries to optimize pre-trained parameters $\theta$, such that it does well in the few-shot regime, unlike transfer learning through fine-tuning. If we now adapt the meta learning objective function from the previous lecture with this fine-tuning, we obtain the following:</p> \[\min_\theta \sum_{\mathcal{T}_i}\mathcal{L}(\theta - \alpha \nabla_\theta\mathcal{L}(\theta, \mathcal{D}^\mathrm{tr}_i), \mathcal{D}^\mathrm{test}_i)\;.\] <p>As you can see in the equation, you are trying to find the parameters $\theta$, such that performing fine-tuning $\theta - \alpha \nabla_\theta\mathcal{L}(\theta, \mathcal{D}^\mathrm{tr}_i)$ on a dataset $\mathcal{D}^\mathrm{tr}_i$ of a task $\mathcal{T}_i$ performs well on the task-specific test set $\mathcal{D}_i^\mathrm{test}$. If these datasets are small, the objective function will still need to optimize pre-training the model for effective fine-tuning on these tasks, meaning that it might work in the few-shot regime.</p> <p>We can write down the general training pipeline with the following steps:</p> <ol> <li>Sample a task $\mathcal{T}_i$.</li> <li>Sample disjoint datasets $\mathcal{D}_i^\mathrm{tr}$ and $\mathcal{D}_i^\mathrm{test}$ from $\mathcal{D}_i$.</li> <li>Optimize $\phi_i \leftarrow \theta - \alpha \nabla_\theta\mathcal{L}(\theta, \mathcal{D}_i^\mathrm{tr})$.</li> <li>Update $\theta$ using $\nabla_\theta \mathcal{L}(\phi_i, \mathcal{D}_i^\mathrm{test})$.</li> </ol> <p>Notice that we only optimize $\phi_i$ to then use it to update parameters $\theta$. This means that $\phi_i$ is discarded and re-computed at each iteration. Additionally, we can optimize for different parameters, such as $\alpha$ as well by including it as a parameter in $\theta$. Besides computational efficiency of this, there is another problem. If we expand $\nabla_\theta \mathcal{L}(\phi_i, \mathcal{D}_i^\mathrm{test})$, we get <strong>second-order</strong> derivates. Let’s show this below:</p> \[\begin{align*} \nabla_\theta\mathcal{L}(\phi_i, \mathcal{D}_i^\mathrm{test}) &amp;= \nabla_{\bar{\phi}}\mathcal{L}(\bar{\phi}, \mathcal{D}_i^\mathrm{test})\bigg|_{\bar{\phi} = \phi_i} \frac{\partial\phi_i}{\partial\theta} \\ &amp;= \nabla_{\bar{\phi}}\mathcal{L}(\bar{\phi}, \mathcal{D}_i^\mathrm{test})\bigg|_{\bar{\phi} = \phi_i}\left(I - \alpha \nabla^2_\theta\mathcal{L}(\theta, \mathcal{D}^\mathrm{tr}_i)\right)\;. \end{align*}\] <p>Unfortunately, we can see the Hessian $\nabla^2_\theta\mathcal{L}(\theta, \mathcal{D}^\mathrm{tr}_i)$. We would have to compute this, but luckily, $\nabla_{\bar{\phi}}\mathcal{L}(\bar{\phi}, \mathcal{D}_i^\mathrm{test})\vert_{\bar{\phi} = \phi_i}$ is a row vector. Due to Hessian-vector products, we can compute $\nabla_\theta\mathcal{L}(\phi_i, \mathcal{D}_i^\mathrm{test})$ without computing the whole Hessian. Let’s show this below:</p> \[\begin{align*} g(x + \Delta x) &amp;\approx g(x) + H(x)\Delta x \\ g(x + rv) &amp;\approx g(x) + rH(x)v \\ H(x)v &amp;\approx \frac{g(x+rv) - g(x)}{r}\;. \end{align*}\] <p>In the first line, we are using a simple Taylor expansion to approximate the function $g(x+\Delta x)$, which is the gradient function. Then, we rewrite $\Delta x$ as with scalar $r$ and vector $v$. We can finally rearrange the equation to get to our result. The last line shows that we can approximate a Hessian-vector product by using <strong>two gradient evaluations</strong>. Whilst this is still more than one, it is much more computationally efficient than computing the entire Hessian matrix.</p> <p>Since $\nabla_{\bar{\phi}}\mathcal{L}(\bar{\phi}, \mathcal{D}_i^\mathrm{test})\vert_{\bar{\phi} = \phi_i}\left(I - \alpha \nabla^2_\theta\mathcal{L}(\theta, \mathcal{D}^\mathrm{tr}_i)\right) = v -\alpha vH$, you can see that we can use the previous result to compute the gradient, rather then computing the whole Hessian.</p> <p>A common misconception is that you will get higher than 2nd-order derivatives when computing multiple iterations of $\phi_i \leftarrow \theta - \alpha \nabla_\theta\mathcal{L}(\theta, \mathcal{D}_i^\mathrm{tr})$. However, this is not true, since the derivatives will be sequential rather than nested. You can try this yourself if you want to test your knowledge! If you do this correctly, you will see that doing more iterations will increase the amount of memory used <strong>linearly</strong>, and the amount of compute necessary <strong>linearly</strong> as well. In practice, usually you do not need more than 5 inner gradient steps, which is usually fine for few-shot learning tasks. In future posts, we will discuss methods that work for hundreds of inner gradient steps.</p> <div> <figure class="figure col-sm-5 float-right"> <img src="/assets/img/blog/cs330/5/maml_viz.png" class="img-fluid" alt="Alt text."> <figcaption class="figure-caption text-center">Visualization of the parameter space of MAML.</figcaption> </figure> <p>This approach, which is called <b>Model-Agnostic Meta-Learning</b><d-cite key="finn2017model"></d-cite> (MAML), is also represented in the figure on the right. Here, $\phi_1^*, \phi_2^*, \phi_3^*$ are the optimal parameters for tasks $1, 2, 3$.</p> </div> <p>At meta-test time, we can follow the following steps, which are very similar to meta-training:</p> <ol> <li>Sample a task $\mathcal{T}_i$.</li> <li>Given training dataset $\mathcal{D}_j^\mathrm{tr}$.</li> <li>Optimize $\phi_j \leftarrow \theta - \alpha \nabla_\theta\mathcal{L}(\theta, \mathcal{D}_j^\mathrm{tr})$.</li> <li>Make predictions on new datapoints $f_{\phi_j}(x)$.</li> </ol> <hr> <h2 id="compare-optimization-based-vs-black-box">Compare: optimization-based vs. black-box</h2> <p>In the previous section, we got some intuition behind the Model-Agnostic Meta-Learning algorithm. For now, it looks like this method is completely different from black-box meta learning, but let’s spend some times trying to understand the connection between the two. Let’s write down both objectives again.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <p><b>Black-box meta learning</b></p> <p>$y^\mathrm{ts} = f_\mathrm{black-box}(\mathcal{D}^\mathrm{tr}_i, x^\mathrm{ts})$.</p> </div> <div class="col-sm mt-3 mt-md-0"> <p><b>Model-agnostic meta learning</b> (MAML)</p> <p>$y^\mathrm{ts} = f_\mathrm{MAML}(\mathcal{D}^\mathrm{tr}_i, x^\mathrm{ts}) = f_{\phi_i}(x^\mathrm{ts})$, where $\phi_i = \theta - \alpha \nabla_\theta \mathcal{L}(\theta ,\mathcal{D}_i^\mathrm{tr})$.</p> </div> </div> <p>If you look at both equations, both equations look more similar than it may seem. Both methods are still only functions of $\mathcal{D}^\mathrm{tr}_i$ and $x^\mathrm{ts}$. The main difference is that MAML uses an <strong>“embedded” gradient operator</strong> in its <strong>computation graph</strong>, which is just a directed graph of all computations that is done by the function.</p> <p>Keeping this idea of a computation graph in mind, you might think of the idea to mix and match different components of the computation graph. This idea has been explored in various works. For example, one paper learned the initialization parameters $\theta$, but replaced the gradient updates with a learned network<d-cite key="ravi2016optimization"></d-cite>, meaning that $\phi_i = \theta - f(\theta, \mathcal{D}_i^\mathrm{tr}, \nabla_\theta\mathcal{L})$. It turns out this approach is not very practical, but it has a nice conceptual meaning that goes along well with the idea of a computation graph.</p> <h3 id="comparing-performances">Comparing performances</h3> <p>We now wish to compare the two different approaches to each other. First, let’s think about the following: If we have a test task $\mathcal{T}_j$ that is a bit different from the tasks that we trained our models on, which approach do you think will be better?</p> <p>We hope to see that optimization-based meta learning performs better in this case, because it’s explicitly optimizing the effectiveness of fine-tuning to new tasks. With black-box meta learning, we are essentially just training a model to output model parameters or context, which does not guarantee to work for unseen tasks (though we hope it will!).</p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/5/adaptation_res.png" class="img-fluid" alt="Alt text."> <figcaption class="figure-caption text-center">Results of different meta learning algorithms on augmented samples from the Omniglot dataset.</figcaption> </figure> <p>We will test this by looking at the Omniglot image classification problem again. However, we will try to see what happens when we vary the tasks after training the models on them. In this case, digits from the datasets are warped to see a form of out-of-distribution performance. In the figure above, which is taken from<d-cite key="finn2017meta"></d-cite>, you can see that, as expected, an optimization-based method like MAML performs much better than black-box-based approaches like SNAIL <d-cite key="mishra2017simple"></d-cite> or MetaNet <d-cite key="munkhdalai2017meta"></d-cite> for these types of problems.</p> <p>You might think that this structure comes at a cost of expressiveness. But, in <d-cite key="finn2017meta"></d-cite>, they showed that MAML can approximate any function of $\mathcal{D}^\mathrm{tr}_i, x^\mathrm{ts}$ given some assumptions (i.e. you need a very deep network for this to hold). However, despite these assumptions, this result shows that MAML benefits of the inductive bias of optimizing fine-tuning explicitly without losing any of the expressive power.</p> <hr> <h2 id="challenges-and-solutions">Challenges and solutions</h2> <p>Whilst optimization-based meta learning methods may sound perfect from this post, there are quite some challenges when using these methods. We will list some of them and discuss them in more detail.</p> <ol> <li> <p><strong>Bi-level optimization can exhibit instabilities.</strong></p> <p>This can be the case because you have nested optimizations that are heavily dependent on each other. Some unexpected result in one level of the optimization can result in an unexpected effect, which will then be propagated throughout training. There are multiple simple tricks that can be used to try to stabilize training:</p> <ul> <li>Automatically learn inner vector learning rate, tune outer learning rate <d-cite key="li2017meta"></d-cite> <d-cite key="behl2019alpha"></d-cite>.</li> <li>Optimize only a subset of the parameters in the inner loop <d-cite key="zhou2018deep"></d-cite> <d-cite key="zintgraf2019fast"></d-cite>.</li> <li>Decouple inner learning rate, batch normalization statistics per-step <d-cite key="antoniou2018train"></d-cite>.</li> <li>Introduce context variables for increased expressive power <d-cite key="finn2017one"></d-cite> <d-cite key="zintgraf2019fast"></d-cite>.</li> </ul> </li> <li> <p><strong>Backpropagating through many inner gradient steps is compute- &amp; memory-intensive.</strong></p> <p>As we said before, the amount of inner gradient steps with MAML is usually only around 5, due to the amount of compute and memory required to optimize over such a computation graph. Again, there are some tricks to try to address this, but we will discuss more options in the lecture about large-scale meta learning.</p> <ul> <li>(Crudely) approximate $\frac{\delta\phi_i}{\delta\theta}$ as the identity matrix <d-cite key="nichol2018reptile"></d-cite>. This seems to work surprisingly well for simple few-shot learning problems, but it doesn’t scale to more complex problems.</li> <li>Only optimize the last layer of weights using ridge regression, logistic regression <d-cite key="bertinetto2018meta"></d-cite>, or support vector machines <d-cite key="lee2019meta"></d-cite>. This can lead to a closed form convex optimization problem on top of the meta-learned features.</li> <li>Derive the meta-gradient using the implicit function theorem <d-cite key="rajeswaran2019meta"></d-cite>. This way you can compute the full gradient without differentiating through the entire computation graph of the multiple iterations.</li> </ul> </li> </ol> <p>Let’s summarize the upsides and downsides:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <p><b>Upsides</b></p> <ul> <li>Positive inductive bias at the start of meta learning.</li> <li>Extrapolates better via structure of optimization.</li> <li>Maximally expressive for a sufficiently deep network.</li> <li>Model-agnostic!</li> </ul> </div> <div class="col-sm mt-3 mt-md-0"> <p><b>Downsides</b></p> <ul> <li>Typically requires second-order optimization.</li> <li>Can be compute and/or memory intensive.</li> <li>Can be prohibitively expensive for large models.</li> </ul> </div> </div> <hr> <h2 id="case-study-of-land-cover-classification">Case study of land cover classification</h2> <p>We will now study an example of optimization-based meta learning for a pretty cool problem: land cover classification <d-cite key="russwurm2020meta"></d-cite>! Imagine that you are given a bunch of <strong>satellite data of different terrains</strong>, and your goal is to predict <strong>what the land is used for</strong> by segmenting it into different classes. Research like this can for example be used to understand how different climates change over time, or urban planning. Unfortunately, it is very expensive to label these images, leading to small datasets.</p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/5/landmark_problem.png" class="img-fluid" alt="Alt text."> <figcaption class="figure-caption text-center">Depiction of the use of MAML for land cover classification.</figcaption> </figure> <p>As you can see in the image above, we have terrains from different regions of the world, which can look very different, but probably still share some geological structural similarities. Given this description, try to think to yourself: Do you think meta learning would be a good approach to this problem?</p> <p>If we let our tasks correspond to these different regions, we can try to use optimization-based meta learning to learn to effectively and efficiently fine-tune to unknown regions of the world! For these new regions, we would manually segment small amount of data, and then use our model to take care of the rest.</p> <p>The paper looks at two datasets:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <p>SEN12MS <d-cite key="schmitt2019sen12ms"></d-cite>, which contains geographic metadata, making it easy to construct good meta-train, meta-val, and meta-test sets.</p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/5/sen12ms.png" class="img-fluid" alt="Alt text."> <figcaption class="figure-caption text-center">Example of data in the SEN12MS dataset.</figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <p>DeepGlobe <d-cite key="demir2018deepglobe"></d-cite>, which did not provide this metadata. Instead, clustering was used to create datasets of similar terrains.</p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/5/deepglobe.png" class="img-fluid" alt="Alt text."> <figcaption class="figure-caption text-center">Example of data in the DeepGlobe dataset.</figcaption> </figure> </div> </div> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/5/use_case_res.png" class="img-fluid" alt="Alt text."> <figcaption class="figure-caption text-center">Results of different types of approaches to the land cover classification problem.</figcaption> </figure> <p>The results of using MAML on these two datasets is shown in the figure above. As you can see, MAML is more efficient and effective than (dark blue) pre-training on meta-training data and fine-tuning and (light blue) training on the new terrain from scratch. Hopefully these results give you a good idea of the potential of meta learning for few-shot learning!</p> <hr> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/blog/cs330/2024-03-10-obml.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Lars C.P.M. Quaedvlieg. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-JKH10LEP3Y"></script> <script defer src="/assets/js/google-analytics-setup.js"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>