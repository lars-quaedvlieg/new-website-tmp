<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> CS-330 Lecture 5: Few-Shot Learning via Metric Learning | Lars Quaedvlieg </title> <meta name="author" content="Lars C.P.M. Quaedvlieg"> <meta name="description" content="This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to to understand the third form of meta learning: non-parametric few-shot learning. We will also compare the three different methods of meta learning. Finally, we give practical examples of meta learning, in domains such as imitation learning, drug discovery, motion prediction, and language generation!"> <meta property="og:site_name" content="Lars Quaedvlieg"> <meta property="og:type" content="article"> <meta property="og:title" content="Lars Quaedvlieg | CS-330 Lecture 5: Few-Shot Learning via Metric Learning"> <meta property="og:url" content="https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-fsl-ml/"> <meta property="og:description" content="This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to to understand the third form of meta learning: non-parametric few-shot learning. We will also compare the three different methods of meta learning. Finally, we give practical examples of meta learning, in domains such as imitation learning, drug discovery, motion prediction, and language generation!"> <meta property="og:image" content="/assets/img/blog/cs330/6/matching_network.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="CS-330 Lecture 5: Few-Shot Learning via Metric Learning"> <meta name="twitter:description" content="This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to to understand the third form of meta learning: non-parametric few-shot learning. We will also compare the three different methods of meta learning. Finally, we give practical examples of meta learning, in domains such as imitation learning, drug discovery, motion prediction, and language generation!"> <meta name="twitter:image" content="/assets/img/blog/cs330/6/matching_network.png"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?004339841cbc43800ab5ac9276b4716d"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-fsl-ml/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "CS-330 Lecture 5: Few-Shot Learning via Metric Learning",
            "description": "This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to to understand the third form of meta learning: non-parametric few-shot learning. We will also compare the three different methods of meta learning. Finally, we give practical examples of meta learning, in domains such as imitation learning, drug discovery, motion prediction, and language generation!",
            "published": "March 14, 2024",
            "authors": [
              
              {
                "author": "Lars C.P.M. Quaedvlieg",
                "authorURL": "https://lars-quaedvlieg.github.io/",
                "affiliations": [
                  {
                    "name": "EPFL",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Lars Quaedvlieg </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Curriculum Vitae </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>CS-330 Lecture 5: Few-Shot Learning via Metric Learning</h1> <p>This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to to understand the third form of meta learning: non-parametric few-shot learning. We will also compare the three different methods of meta learning. Finally, we give practical examples of meta learning, in domains such as imitation learning, drug discovery, motion prediction, and language generation!</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#quick-recap">Quick recap</a> </div> <div> <a href="#non-parametric-few-shot-learning">Non-parametric few-shot learning</a> </div> <ul> <li> <a href="#siamese-networks">Siamese networks</a> </li> <li> <a href="#matching-networks">Matching networks</a> </li> <li> <a href="#prototypical-models">Prototypical models</a> </li> <li> <a href="#more-advanced-models">More advanced models</a> </li> </ul> <div> <a href="#properties-of-meta-learning-algorithms">Properties of meta-learning algorithms</a> </div> <div> <a href="#examples-of-meta-learning-in-practice">Examples of meta learning in practice</a> </div> <ul> <li> <a href="#land-cover-classification">Land-cover classification</a> </li> <li> <a href="#student-feedback-generation">Student feedback generation</a> </li> <li> <a href="#low-resource-molecular-property-prediction">Low-resource molecular property prediction</a> </li> <li> <a href="#one-shot-imitation-learning">One-shot imitation learning</a> </li> <li> <a href="#dermatological-image-classification">Dermatological image classification</a> </li> <li> <a href="#few-shot-human-motion-prediction">Few-shot human motion prediction</a> </li> </ul> </nav> </d-contents> <p>The goal of this lecture is to understand the third form of meta learning: <strong>non-parametric few-shot learning</strong>. We will also compare the three different methods of meta learning. Finally, we give practical examples of meta learning, in domains such as <strong>imitation learning</strong>, <strong>drug discovery</strong>, <strong>motion prediction</strong>, and <strong>language generation</strong>! If you missed the previous lecture, which was about optimization-based meta learning, you can head over <a href="/blog/2024/cs330-stanford-obml/">here</a> to view it.</p> <p>As always, since I am still new to this blogging thing, reach out to me if you have any feedback on my writing, the flow of information, or whatever! You can contact me through <a href="https://www.linkedin.com/in/lars-quaedvlieg/" rel="external nofollow noopener" target="_blank">LinkedIn</a>. ☺</p> <p>The link to the lecture slides can be found <a href="https://cs330.stanford.edu/materials/cs330_nonparametric_2023.pdf" rel="external nofollow noopener" target="_blank">here</a>.</p> <h2 id="quick-recap">Quick recap</h2> <p>So far, we have discussed two approaches to meta learning: black-box meta learning, and optimization-based meta learning.</p> <ol> <li> <p><a href="/blog/2024/cs330-stanford-bbml-icl/">Black-box meta learning</a>.</p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/6/bbml_recap.png" class="img-fluid" alt="Alt text."> <figcaption class="figure-caption text-center">Computation pipeline for black-box meta learning.</figcaption> </figure> <p>In <em>black-box meta learning</em>, we attempt to train some sort of meta-model to output task-specific parameters or contextual information, which can then be used by another model to solve that task. We saw that this method is <strong>very expressive</strong> (e.g. it can model many tasks). However, it also requires solving a <strong>challenging optimization problem</strong>, which is incredibly data-inefficient.</p> </li> <li> <p><a href="/blog/2024/cs330-stanford-obml/">Optimization-based meta learning</a>.</p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/6/obml_recap.png" class="img-fluid" alt="Alt text."> <figcaption class="figure-caption text-center">Computation pipeline for optimization-based meta learning.</figcaption> </figure> <p>We then talked about <em>optimization-based meta learning</em>, which embeds an optimization process within the inner learning process. This way, you can learn to find parameters to a model such that optimizing these parameters to specific tasks is as effective and efficient as possible. We saw that model-agnostic meta learning <strong>preserves expressiveness</strong> over tasks, but it remains <strong>memory-intensive</strong>, and requires solving a <strong>second-order optimization</strong> problem.</p> </li> </ol> <h2 id="non-parametric-few-shot-learning">Non-parametric few-shot learning</h2> <p>In the previous two approaches to meta learning, we only talked about parametric methods <d-footnote>A parametric model assumes a specific form for the underlying function between variables, using a finite number of parameters, while a non-parametric model makes fewer assumptions about the function form, potentially using an infinite number of parameters to model the data more flexibly.</d-footnote>. However, what if we can avoid the optimization process in the inner learning loop of optimization-based meta learning methods? If this is possible, we do not have to solve a second-order optimization problem anymore. For this reason, we will look into replacing the parametric models in the inner learning loop with <strong>non-parametric models</strong>, which don’t require to be optimized. Specifically, we will try to <strong>use parametric meta learners that produce non-parametric learners</strong>.</p> <div> <figure class="figure col-sm-5 float-right"> <img src="/assets/img/blog/cs330/6/parametric_example.png" class="img-fluid" alt="Alt text."> </figure> <p>One benefit of non-parametric methods is that they generally work well in low data regimes, making it a great opportunity for few-shot learning problems at meta-test time. Nevertheless, during meta-training time, we would still like to use a parametric learner to exploit the potentially large amounts of data.</p> </div> <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/6/l2_loss_example.png" class="img-fluid" alt="Alt text."> <figcaption class="figure-caption text-center">Comparison between $\ell_2$-distances of two augmented images to the original.</figcaption> </figure> <p>The key idea behind non-parametric approaches is to compare the task-specific test data to the data in the train dataset. We will continue using the example of the few-shot image classification problem, as in the previous posts. If you want to compare images to each other, you need to come up with a certain <b>metric</b> to do so.</p> </div> <p>The simplest idea might be to utilize the $\ell_2$-distance. Unfortunately, it is not that simple. If you look at the figure above, you can see the image of a woman on the right and two augmented versions on the left. When you calculate the $\ell_2$-distance between the original image and the augmented image, the distance between the blurry image and the original one is smaller than the other distortion, even though this may resemble the original image more. For this problem, you could use a different metric, such as a <em>perceptual loss function</em> <d-cite key="johnson2016perceptual"></d-cite>, but in general, it might be worthwhile to learn the metric from the data.</p> <p>In this post, we will discuss three different ways of doing metric learning, starting with the easiest and building our way up. Firstly, we will talk about the most basic model: Siamese networks.</p> <h3 id="siamese-networks">Siamese networks</h3> <figure class="figure col-sm-12 float-right"> <img src="/assets/img/blog/cs330/6/siamese_network.png" class="img-fluid" alt="Alt text."> <figcaption class="figure-caption text-center">Example architecture of a Siamese network. It takes two images as an input and outputs a binary label whether they belong to the same class or not.</figcaption> </figure> <p>With a Siamese network, the goal is to learn whether two images belong to the same class or not. The input to the model is two images, and it outputs whether it thinks they belong to the same class. However, the penultimate activations correspond to a learned distance metric between these images. At meta-train time, you are simply trying to minimize the binary cross-entropy loss.</p> <p>At meta-test time, you need to compare the test image $x_\mathrm{test}$ against every image in the test-time training dataset $\mathcal{D}^\mathrm{tr}$, and then select the class of the image that has the highest probability. This corresponds to the equation below (for simplicity of the equation, we assume that only one sample will have $f_\theta(x_j^\mathrm{test}, x_k) &gt; 0.5$). Furthermore, $1$ corresponds to the indicator function.</p> \[\hat{y}_j^\mathrm{test} := \sum_{(x_k, y_k) \sim \mathcal{D}^\mathrm{tr}} 1(f_\theta(x_j^\mathrm{test}, x_k) &gt; 0.5)y_k\;.\] <p>With this method, there is a mismatch between meta-training and meta-testing. During meta-training, you are solving a binary classification problem, whilst during meta-testing, you are solving an $N$-way classification problem. You cannot phrase meta-training in the same way, since the indicator function $1$ makes $\hat{y}_j^\mathrm{test}$ non-differentiable. We will try to resolve this by introducing <strong>matching networks</strong>.</p> <h3 id="matching-networks">Matching networks</h3> <p>In the previous equation above, we saw that at meta-test time, we use the class of the most similar training example as the estimate of the test sample. In order to get rid of the mismatch between meta-training and meta-testing, we can rephrase the meta-testing objective similarly to what we saw. Let’s say we instead modify the procedure to use a mix of class predictions as a class estimate. This would result in the equation below.</p> \[\hat{y}_j^\mathrm{test} := \sum_{(x_k, y_k) \sim \mathcal{D}^\mathrm{tr}} f_\theta(x_j^\mathrm{test}, x_k)y_k\;.\] <p>At meta-train time, we can now use the same objective, and backpropagate through the cross-entropy loss $y_j^\mathrm{test} \log(\hat{y}_j^\mathrm{test}) + (1-y_j^\mathrm{test})\log(1-\hat{y}_j^\mathrm{test})$. This way, both meta-training and meta-testing are aligned with the same procedure. Our meta-training process would become:</p> <ol> <li>Sample task $\mathcal{T}_i$.</li> <li>Sample two images per class, giving $D_i^\mathrm{tr}, D_i^\mathrm{test}$.</li> <li> <p>Compute $\hat{y}^\mathrm{test} = \sum_{(x_k, y_k) \sim \mathcal{D}^\mathrm{tr}_i} f_\theta(x^\mathrm{test}, x_k)y_k$.</p> </li> <li>Backpropagate the loss with respect to $\theta$.</li> </ol> <figure class="figure col-sm-12 float-right"> <img src="/assets/img/blog/cs330/6/matching_network.png" class="img-fluid" alt="Alt text."> <figcaption class="figure-caption text-center">Example architecture of a matching network. It takes the entire training dataset as an input with a testing image and predicts the most likely class for the testing image given the training data.</figcaption> </figure> <p>This idea corresponds to so-called “matching networks” <d-cite key="vinyals2016matching"></d-cite>. Here, we embed each training image into some latent space using a bidirectional LSTM $g_\theta$. Then, we encode the test image using a shared convolutional encoder $h_\theta$ and perform the dot product between the latent training vectors and the latent test vector, resulting in $f_\theta(x^\mathrm{ts}, x_k)$. Finally, we take the dot products with the labels to obtain the prediction $\hat{y}^\mathrm{ts}.$ This way meta-training and meta-testing match, which resulted in a better performance than something like Siamese networks.</p> <p>Let’s stand still with what we’re doing for a second and think about how this approach is non-parametric. If we recall from parametric models, we would always compute task-specific parameters $\phi_i \leftarrow f_\theta(\mathcal{D}_i^\mathrm{tr})$. However, now have integrated the parameters $\phi$ out by computing $\hat{y}_j^\mathrm{test} := \sum_{(x_k, y_k) \sim \mathcal{D}^\mathrm{tr}} f_\theta(x_j^\mathrm{test}, x_k)y_k$ directly by comparing to the training dataset, making it non-parametric.</p> <div> <figure class="figure col-sm-4 float-right"> <img src="/assets/img/blog/cs330/6/matching_network_disadvantage.png" class="img-fluid" alt="Alt text."> <figcaption class="figure-caption text-center">Example of a disadvantage of a matching network.</figcaption> </figure> <p>In the meta-training procedure described, we would sample two images per class. But what would happen if we sampled more than two images (ignoring potential class imbalance)? Well, with matching networks, each sample of each class is evaluated independently with $f_\theta(x_j^\mathrm{test}, x_k)$ instead of together. This could lead to strange results if the majority of a class has a low confidence but there is an outlier with a high confidence, overpowering the correct label. This can be depicted in the right figure. Imagine if you want to predict the label of the black square. The dot-product score with the red sample might be so high that it overpowers the other samples, even though it is more likely part of the blue class. We will try to resolve this by calculating prototypical embeddings that average class information.</p> </div> <h3 id="prototypical-models">Prototypical models</h3> <div> <figure class="figure col-sm-5 float-right"> <img src="/assets/img/blog/cs330/6/proto_model_example.png" class="img-fluid" alt="Alt text."> <figcaption class="figure-caption text-center">Example of a prototype network, which uses aggregation over the embeddings of each class.</figcaption> </figure> <p>Prototypical models <d-cite key="snell2017prototypical"></d-cite> will work quite similarly to what we have previously seen, but try to aggregate class information in order to prevent outliers. The figure on the right depicts this. Formally, we introduce class prototypes $c_n = \frac{1}{K} \sum_{(x,y)\in\mathcal{D}_i^\mathrm{tr}} 1(y_k=n)f_\theta(x_k)$. After we compute these class-averaged embeddings, a model will try to estimate the class of the test point by using something like Softmax probability, resulting in the equation below, where $d$ was the Euclidean or Cosine distance. Nevertheless, it could even be a learned network as we have previously seen.</p> </div> \[p_\theta(y=n|x) = \frac{\exp(-d(f_\theta(x), c_n))}{\sum_{n^\prime}\exp(-d(f_\theta(x), c_{n^\prime}))}\;.\] <p>As opposed to the matching networks, we are now using the same embedding function $f_\theta$ for both the training and testing datapoints.</p> <h3 id="more-advanced-models">More advanced models</h3> <p>The models that we talked about today are already quite expressive for non-parametric meta learning, but they all do <strong>some form of embedding followed by nearest-neighbours</strong>. However, sometimes you might need to reason about more complex relationships between datapoints. Let’s briefly discuss a few more recent works that approach this problem.</p> <ol> <li> <strong>Relation networks</strong> <d-cite key="sung2018learning"></d-cite>. <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/6/relation_net.png" class="img-fluid" alt="Alt text."> </figure> <p>The idea is to learn non-linear relation modules on the embedding. They first embed the images and then compute this relation score, which corresponds to the distance function $d$ that we saw with prototypical models.</p> </div> </li> <li> <strong>Infinite mixture of prototypes</strong> <d-cite key="allen2019infinite"></d-cite>. <div> <figure class="figure col-sm-3 float-right"> <img src="/assets/img/blog/cs330/6/mixture_of_prots.png" class="img-fluid" alt="Alt text."> </figure> <p>The idea is to learn an infinite mixture of prototypes, which is useful when classes are not easy to cluster nicely. For example, some breeds of cats might look similar to dogs, which would not be good when averaging class embeddings. In this case, we can have multiple prototypes per class.</p> </div> </li> <li> <strong>Graph neural networks</strong> <d-cite key="garcia2017few"></d-cite>. <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/6/message_passing_npm.png" class="img-fluid" alt="Alt text."> </figure> <p>The idea is to do message passing on the embeddings instead of doing something as simple as nearest neighbours. This way, you can figure out relationships between different examples (i.e. by learning edge weights), and do more complex aggregation.</p> </div> </li> </ol> <h2 id="properties-of-meta-learning-algorithms">Properties of meta-learning algorithms</h2> <p>Now that we have seen all three different types of meta learning algorithms, we can compare each approach to see which problems might benefit from which approach. Let’s first quickly summarize all approaches.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <p><b>Black-box meta learning.</b></p> <figure class="figure col-sm-6"> <img src="/assets/img/blog/cs330/6/bbml_model_small.png" class="img-fluid" alt="Alt text."> </figure> <p>$y^\mathrm{ts} = f_\theta(\mathcal{D}_i^\mathrm{tr}, x^\mathrm{ts})$.</p> </div> <div class="col-sm mt-3 mt-md-0"> <p><b>Optimization-based meta learning.</b></p> <p>$y^\mathrm{ts} = f_\mathrm{MAML}(\mathcal{D}_i^\mathrm{tr}, x^\mathrm{ts}) = f_{\phi_i}(x^\mathrm{ts})$, where $\phi_i = \theta - \alpha \nabla_\theta \mathcal{L}(\theta, \mathcal{D}^\mathrm{tr})$.</p> </div> <div class="col-sm mt-3 mt-md-0"> <p><b>Non-parametric meta learning.</b></p> <p>$y^\mathrm{ts} = f_\mathrm{PN}(\mathcal{D}_i^\mathrm{tr}, x^\mathrm{ts}) = \mathrm{softmax}(-d(f_\theta(x^\mathrm{ts}), c_n))$, where $c_n = \frac{1}{K} \sum_{(x,y)\in\mathcal{D}_i^\mathrm{tr}} 1(y_k=n)f_\theta(x_k)$.</p> </div> </div> <p>As you can see, all these methods share this perspective of a computational graph that we discussed in earlier posts. You can easily mix-and-match different components of these computation graphs. Below are some examples of paper that try this:</p> <ol> <li>Gradient descent on relation network embeddings.</li> <li>Both condition on data and run gradient descent <d-cite key="rusu2018meta"></d-cite>.</li> <li>Model-agnostic meta learning, but initialize last layer as a prototype network during meta-training <d-cite key="triantafillou2019meta"></d-cite>.</li> </ol> <p>Let’s make a table of the benefits and downsides of each method that we have discussed up to this point in the series:</p> <table> <thead> <tr> <th>Black-box</th> <th>Optimization-based</th> <th>Non-parametric</th> </tr> </thead> <tbody> <tr> <td>[+] Complete expressive power</td> <td>[~] Expressive for very deep models (in a supervised learning setting)</td> <td>[+] Expressive for most architectures</td> </tr> <tr> <td>[-] Not consistent</td> <td>[+] Consistent, reduces to gradient descent</td> <td>[~] Consistent under certain conditions</td> </tr> <tr> <td>[+] Easy to combine with a variety of learning problems</td> <td>[+] Positive inductive bias at the start of meta learning, handles varying and large number of classes well</td> <td>[+] Entirely feedforward, computationally fast and easy to optimize</td> </tr> <tr> <td>[-] Challenging optimization problem (no inductive bias at initialization)</td> <td>[-] Second-order optimization problem</td> <td>[-] Harder to generalize for varying number of classes</td> </tr> <tr> <td>[-] Often data-inefficient</td> <td>[-] Compute- and memory-intensive</td> <td>[-] So far limited to classification</td> </tr> </tbody> </table> <ul> <li> <strong>Expressive power</strong>: The ability of $f$ to model a range of learning procedures.</li> <li> <strong>Consistency</strong>: Learned learning procedure will monotonically improve with more data</li> <li> <strong>Uncertainty awareness</strong>: Ability to reason about ambiguity during learning.</li> </ul> <p>We have not yet discussed the uncertainty awareness of methods, but it plays an important role in active learning, calibrated uncertainty, reinforcement learning, and principled Bayes approaches. We will discuss this later on in the series!</p> <h2 id="examples-of-meta-learning-in-practice">Examples of meta learning in practice</h2> <p>In this section, we will very briefly talk about 6 different problem settings where meta learning has been used, some of which we have already seen in previous posts. This should give you a good idea of some different applications, and show you that it can be utilized in many different domains.</p> <h3 id="land-cover-classification">Land-cover classification</h3> <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/6/paper_land_cover.png" class="img-fluid" alt="Alt text."> </figure> <p>The goal of this paper <d-cite key="russwurm2020meta"></d-cite> is to classify and segment satellite images in different regions of the world. Every region corresponds to a task, and the datasets are thus images from a particular region. The problem is that manually segmenting this data is expensive, so the authors use meta learning to quickly be able to segment new regions given limited training data on these regions.</p> <p><b>Model</b>: Optimization-based (model-agnostic meta learning)</p> </div> <h3 id="student-feedback-generation">Student feedback generation</h3> <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/6/paper_student_feedback_results.png" class="img-fluid" alt="Alt text."> </figure> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/6/paper_student_feedback_example.png" class="img-fluid" alt="Alt text."> </figure> <p>The goal of this paper <d-cite key="wu2021prototransformer"></d-cite> is to automatically provide students with feedback on coding assignments for high-quality Computer Science education. The different tasks corresponded to different rubrics for different assignments or exams. The datasets were then constructed of the solutions of the students (in this paper, they were always Python programs).</p> <p><b>Supervised baseline</b>: Train a classifier per task, using same pre-trained CodeBERT <d-cite key="feng2020codebert"></d-cite>.</p> <p>Outperforms supervised learning by 8-17%, and more accurate than human TA on held-out rubric! However, there is room for improvement on a held-out exam.</p> <p><b>Model</b>: Non-parametric (prototypical network with pre-trained Transformer, task information, and side information).</p> </div> <h3 id="low-resource-molecular-property-prediction">Low-resource molecular property prediction</h3> <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/6/paper_molecule_results.png" class="img-fluid" alt="Alt text."> </figure> <p>The goal of this paper <d-cite key="nguyen2020meta"></d-cite> is to predict certain chemical properties and activities of different molecules in Silico models, which could potentially be useful for low-resolution drug discovery problems. The tasks here correspond to different chemical properties and activations, and the corresponding datasets are different instances of these properties and activations.</p> <p><b>Model</b>: Optimization-based MAML, first-order MAML, and an ANIL Gated graph neural net base model.</p> </div> <h3 id="one-shot-imitation-learning">One-shot imitation learning</h3> <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/6/paper_imitation.png" class="img-fluid" alt="Alt text."> </figure> <p>The goal of this paper <d-cite key="yu2018one"></d-cite> is to do one-shot imitation learning for object manipulation by using video demonstrations of a human. The tasks would be different manipulation problems. The training dataset would be the human demonstration, and the testing dataset would be the tele-operated demonstration.</p> <p><b>Note</b>: See that they training and testing datasets do not need to be sampled independently from the overall dataset for meta learning to work!</p> <p><b>Model</b>: Model-agnostic meta learning with learned inner loss function.</p> </div> <h3 id="dermatological-image-classification">Dermatological image classification</h3> <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/6/paper_derm_res.png" class="img-fluid" alt="Alt text."> </figure> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/6/paper_derm.png" class="img-fluid" alt="Alt text."> </figure> <p>The goal of this paper <d-cite key="prabhuprototypical"></d-cite> is to perform dermatological image classification that is good for all different skin conditions, which are the tasks in this case. The datasets consist of images of these skin conditions from different people.</p> <p><b>Model</b>: Non-parametric prototype networks with multiple prototypes per class using clustering objective.</p> <p>Results show that the clustering prototype networks perform better than normal ones and competitive against a ResNet model that is pre-trained on ImageNet and fine-tuned on 200 classes with balancing. This is a very strong baseline with access to more info during training, and it requires re-training for new classes.</p> </div> <h3 id="few-shot-human-motion-prediction">Few-shot human motion prediction</h3> <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/6/paper_motion.png" class="img-fluid" alt="Alt text."> </figure> <p>The goal of this paper <d-cite key="gui2018few"></d-cite> is to do few-shot motion prediction using meta learning, which could potentially be useful for autonomous driving and human-robot interaction. The tasks are different humans and different motivation. The corresponding train dataset $\mathcal{D}^\mathrm{tr}_i$ is composed of the past $K$ seconds of the motion, and the test set $\mathcal{D}^\mathrm{test}_i$ is composed of the future second(s) of the motion.</p> <p><b>Note</b>: See that they training and testing datasets do not need to be sampled independently from the overall dataset for meta learning to work!</p> <p><b>Model</b>: Optimization-based/black-box hybrid, MAML with additional learned update rule and a recurrent neural net base model.</p> </div> <hr> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/blog/cs330/2024-03-14-fsl-ml.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Lars C.P.M. Quaedvlieg. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-JKH10LEP3Y"></script> <script defer src="/assets/js/google-analytics-setup.js"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>