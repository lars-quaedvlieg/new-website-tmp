<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> A Deep Dive into Behavioural Foundation Models with Meta Motivo | Lars Quaedvlieg </title> <meta name="author" content="Lars C.P.M. Quaedvlieg"> <meta name="description" content="In this post, I aim to explore the evolution of behavioral foundation models (BFMs) and their role in reinforcement learning, robotics, and zero-shot policy adaptation. We will begin by understanding Forward-Backward Representations (FB) and how they enable learning compact, reusable representations of environments. Then, we will examine how Meta Motivo, the first behavioral foundation model for humanoid control, leverages FB representations and imitation learning to perform whole-body control tasks with zero-shot generalization. Along the way, we will connect these ideas to broader trends in unsupervised RL, successor measures, and policy optimization, making the case for why BFMs are a promising direction for future AI systems that generalize across diverse tasks without retraining."> <meta property="og:site_name" content="Lars Quaedvlieg"> <meta property="og:type" content="website"> <meta property="og:title" content="Lars Quaedvlieg | A Deep Dive into Behavioural Foundation Models with Meta Motivo"> <meta property="og:url" content="https://lars-quaedvlieg.github.io//work_in_progress/2025-02-20-meta-motivo-deep-dive/"> <meta property="og:description" content="In this post, I aim to explore the evolution of behavioral foundation models (BFMs) and their role in reinforcement learning, robotics, and zero-shot policy adaptation. We will begin by understanding Forward-Backward Representations (FB) and how they enable learning compact, reusable representations of environments. Then, we will examine how Meta Motivo, the first behavioral foundation model for humanoid control, leverages FB representations and imitation learning to perform whole-body control tasks with zero-shot generalization. Along the way, we will connect these ideas to broader trends in unsupervised RL, successor measures, and policy optimization, making the case for why BFMs are a promising direction for future AI systems that generalize across diverse tasks without retraining."> <meta property="og:image" content="/assets/img/blog/meta-motivo/fb-cpr-components.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="A Deep Dive into Behavioural Foundation Models with Meta Motivo"> <meta name="twitter:description" content="In this post, I aim to explore the evolution of behavioral foundation models (BFMs) and their role in reinforcement learning, robotics, and zero-shot policy adaptation. We will begin by understanding Forward-Backward Representations (FB) and how they enable learning compact, reusable representations of environments. Then, we will examine how Meta Motivo, the first behavioral foundation model for humanoid control, leverages FB representations and imitation learning to perform whole-body control tasks with zero-shot generalization. Along the way, we will connect these ideas to broader trends in unsupervised RL, successor measures, and policy optimization, making the case for why BFMs are a promising direction for future AI systems that generalize across diverse tasks without retraining."> <meta name="twitter:image" content="/assets/img/blog/meta-motivo/fb-cpr-components.png"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?004339841cbc43800ab5ac9276b4716d"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lars-quaedvlieg.github.io//work_in_progress/2025-02-20-meta-motivo-deep-dive/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "A Deep Dive into Behavioural Foundation Models with Meta Motivo",
            "description": "In this post, I aim to explore the evolution of behavioral foundation models (BFMs) and their role in reinforcement learning, robotics, and zero-shot policy adaptation. We will begin by understanding Forward-Backward Representations (FB) and how they enable learning compact, reusable representations of environments. Then, we will examine how Meta Motivo, the first behavioral foundation model for humanoid control, leverages FB representations and imitation learning to perform whole-body control tasks with zero-shot generalization. Along the way, we will connect these ideas to broader trends in unsupervised RL, successor measures, and policy optimization, making the case for why BFMs are a promising direction for future AI systems that generalize across diverse tasks without retraining.",
            "published": "February 20, 2025",
            "authors": [
              
              {
                "author": "Lars C.P.M. Quaedvlieg",
                "authorURL": "https://lars-quaedvlieg.github.io/",
                "affiliations": [
                  {
                    "name": "EPFL",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Lars Quaedvlieg </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Curriculum Vitae </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>A Deep Dive into Behavioural Foundation Models with Meta Motivo</h1> <p>In this post, I aim to explore the evolution of behavioral foundation models (BFMs) and their role in reinforcement learning, robotics, and zero-shot policy adaptation. We will begin by understanding Forward-Backward Representations (FB) and how they enable learning compact, reusable representations of environments. Then, we will examine how Meta Motivo, the first behavioral foundation model for humanoid control, leverages FB representations and imitation learning to perform whole-body control tasks with zero-shot generalization. Along the way, we will connect these ideas to broader trends in unsupervised RL, successor measures, and policy optimization, making the case for why BFMs are a promising direction for future AI systems that generalize across diverse tasks without retraining.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#the-successor-measure-predicting-state-occupancy-in-rl">The Successor Measure: Predicting State Occupancy in RL</a> </div> <ul> <li> <a href="#motivation-why-do-we-need-the-successor-measure">Motivation: Why Do We Need the Successor Measure?</a> </li> <li> <a href="#formal-definition-of-the-successor-measure">Formal Definition of the Successor Measure</a> </li> <li> <a href="#the-successor-measure-as-a-bellman-equation">The Successor Measure as a Bellman Equation</a> </li> <li> <a href="#expressing-the-q-value-function-using-the-successor-measure">Expressing the Q-Value Function Using the Successor Measure</a> </li> </ul> <div> <a href="#forward-backward-representations-structuring-the-successor-measure">Forward-Backward Representations: Structuring the Successor Measure</a> </div> <ul> <li> <a href="#expressing-the-q-function-with-fb-representations">Expressing the Q-Function with FB Representations</a> </li> <li> <a href="#fb-representations-for-multiple-policies">FB Representations for Multiple Policies</a> </li> </ul> <div> <a href="#learning-forward-backward-representations">Learning Forward-Backward Representations</a> </div> <ul> <li> <a href="#learning-via-temporal-difference-loss">Learning via Temporal Difference Loss</a> </li> <li> <a href="#training-the-policy-with-fb-representations">Training the Policy with FB Representations</a> </li> </ul> <div> <a href="#zero-shot-inference-with-fb-representations">Zero-Shot Inference with FB Representations</a> </div> <ul> <li> <a href="#imitation-learning-with-fb-representations">Imitation Learning with FB Representations</a> </li> <li> <a href="#limitations-of-fb-inference">Limitations of FB Inference</a> </li> </ul> <div> <a href="#optimizing-fb-representations-for-real-world-use">Optimizing FB Representations for Real-World Use</a> </div> </nav> </d-contents> <p><strong>TODO: Address figure credits!!</strong></p> <p><strong>Disclaimer:</strong> The figures presented in this blog are <strong>not</strong> created by me. I have simply copied them from the corresponding papers, and they will be credited appropriately throughout the post.</p> <figure class="figure float-right"> <img src="/assets/img/blog/meta-motivo/meta-motivo-overview.png" class="img-fluid" alt="Alt text."> <figcaption class="figure-caption text-center">Meta Motivo is the first behavioral foundation model for humanoid that can solve whole-body control tasks such as tracking, pose-reaching, and reward optimization through zero-shot inference. The model is trained with a novel unsupervised reinforcement learning regularizing zero-shot forward-backward policy learning with imitation of unlabled motions.</figcaption> </figure> <p>Reinforcement learning (RL) has traditionally relied on training separate policies for each task, but this approach doesn’t scale well—every new task requires costly retraining. The goal of <strong>behavioral foundation models</strong> (BFMs) is to develop <strong>general-purpose policies</strong> that can quickly adapt to new tasks without starting from scratch. <strong>Meta Motivo</strong>, one of the first BFMs for humanoid control, achieves this by learning a structured representation of behaviors that allows for <strong>zero-shot generalization</strong> across diverse movement tasks.</p> <p>But how do we learn such representations? At the heart of this approach is the <strong>successor measure</strong>, a mathematical tool that describes how an agent moves through its environment under a given policy. Instead of storing individual trajectories, the successor measure provides a <strong>compressed, reusable representation</strong> of future state distributions, making it possible to <strong>efficiently transfer knowledge across tasks</strong>.</p> <p>To approximate these measures in a structured way, we introduce <strong>Forward-Backward (FB) Representations</strong>, a framework that breaks down future state predictions into two components:</p> <ol> <li>A <strong>forward embedding</strong> that captures how actions influence future states.</li> <li>A <strong>backward embedding</strong> that captures how states are reached from past actions.</li> </ol> <p>Once we have this framework, we’ll explore how it enables <strong>fast imitation learning</strong>, allowing agents to learn from demonstrations efficiently. Finally, we’ll connect all these principles and introduce <strong>Meta Motivo’s optimization pipeline</strong>, showing how it unifies these ideas to enable rapid adaptation to new tasks.</p> <p>Note that, in this post, <em>I will assume that the reader is familiar with the reinforcement learning framework</em>, since I believe that the topic of this blog requires quite some background knowledge, mainly about the general framework and bellman equations. This post will also be quite math-heavy and proof-oriented, as we will derive key equations and discuss the theoretical foundations behind these methods. However, if anything is unclear, feel free to reach out to me for any questions or remarks!</p> <h2 id="the-successor-measure-predicting-state-occupancy-in-rl">The Successor Measure: Predicting State Occupancy in RL</h2> <p>One of the most fundamental problems in reinforcement learning (RL) is understanding what happens after taking an action. If an agent takes an action in a given state, where will it go next? More importantly, what is the long-term impact of that action?</p> <p>Traditionally, RL algorithms try to answer these questions using value functions, which estimate the expected reward an agent will collect in the future. But what if we didn’t care about rewards at all and simply wanted to understand the structure of an environment? This is where the successor measure comes in.</p> <p>The successor measure is a way to describe how an agent moves through the state space over time. It captures the probability of visiting different states in the future, weighted by how soon they are reached. Once we have this information, we can quickly adapt to new tasks without needing to relearn everything from scratch.</p> <p>In this section, we’ll build up the formal definition of the successor measure, derive its key properties, and explain why it is a powerful tool for learning general representations of environments.</p> <h3 id="motivation-why-do-we-need-the-successor-measure">Motivation: Why Do We Need the Successor Measure?</h3> <p>Let’s say we have an RL agent navigating a maze. At every step, it moves based on a policy $\pi$. If we ask:</p> <ul> <li><em>“Which states will this agent visit most often?”</em></li> <li><em>“How likely is it to reach a particular goal?”</em></li> <li><em>“How much time will it spend in different areas of the maze?”</em></li> </ul> <p>the successor measure gives us a precise way to answer these questions.</p> <p>Instead of memorizing specific paths through the maze, we want to learn a generalized understanding of how the agent moves. This allows us to transfer knowledge between different tasks. If we later change the goal of the agent (e.g., moving from one exit to another), we don’t need to start from scratch—we can simply re-use the successor measure to quickly compute the best policy.</p> <p>The key insight here is that an agent’s movement pattern depends only on the environment dynamics and its policy, not on the specific reward function. This means that if we can learn a good representation of movement dynamics, we can adapt to new tasks much faster. We will show this later in this section.</p> <h3 id="formal-definition-of-the-successor-measure">Formal Definition of the Successor Measure</h3> <p>Mathematically, the successor measure tracks how much time an agent spends in different states when following a given policy $\pi$.</p> <p>For a <strong>reward-free Markov Decision Process</strong> (MDP) $\mathcal{M} = \left(\mathcal{S}, \mathcal{A}, P, \gamma\right)$, the successor measure is defined as follows. Given a state $s \in \mathcal{S}$ and an action $a \in \mathcal{A}$, the successor measure of a set $X \subseteq \mathcal{S}$ is:</p> \[M^\pi(X \vert s, a) := \sum_{t=0}^\infty \gamma^t \mathrm{Pr}(s_{t+1} \in X \vert s, a, \pi)\;.\] <p>This measures the discounted probability that the agent will visit any state in $X$ at some future time step.</p> <p>Let’s break this down piece by piece:</p> <ul> <li>$X$ is a set of states that we are interested in.</li> <li>$\mathrm{Pr}(s_{t+1} \in X \vert s, a, \pi)$ is the probability that, after $t$ steps, the agent is in $X$, given that it started in $s$ and took action $a$.</li> <li>$\gamma^t$ is the discount factor ($0 &lt; \gamma &lt; 1$) that reduces the weight of distant future states, making states which are visited sooner more important.</li> </ul> <p>If $X$ contains just a single state $s^\prime$, then $M^\pi(\{s^\prime\} \vert s, a)$ tells us how much discounted probability mass is assigned to $s^\prime$ in the future. This represents the total discounted probability of visiting $s^\prime$ at any future time step. In other words, it is a measure of occupancy probability rather than raw visit counts.</p> <p>For the people familiar with measure theory, you can show that $M^\pi(\cdot \vert s, a)$ is a <strong>probability measure</strong>, meaning it assigns a value to any subset $X \subseteq \mathcal{S}$ in a way that satisfies the properties of a measure:</p> <ul> <li> <strong>Non-negativity</strong>: $\forall X \subseteq \mathcal{S} \quad M^\pi(X \vert s, a) \geq 0$.</li> <li> <strong>Countable additivity</strong>: For disjoint sets $X_1, X_2, \dots$, we have $M^\pi(\cup_i X_i \vert s, a) = \sum_i M^\pi(X_i \vert s, a)$.</li> <li> <strong>Normalization</strong>: if $X = \mathcal{S}$, then $M^\pi(S \vert s, a)$ represents the total discounted probability mass distributed over all future states.</li> </ul> <h3 id="the-successor-measure-as-a-bellman-equation">The Successor Measure as a Bellman Equation</h3> <p>One of the most useful properties of the successor measure is that it satisfies a <strong>recursive relationship</strong>, similar to the Bellman equation used in reinforcement learning. Specifically, the successor measure follows:</p> \[M^\pi(X \vert s, a) = \mathrm{Pr}(X \vert s, a) + \gamma \mathbb{E}_{s^\prime \sim P(\cdot \vert s, a), a^\prime \sim \pi(\cdot \vert s^\prime)}\left[M^\pi(X \vert s^\prime, a^\prime)\right]\;.\] <p>This equation tells us that the total expected future occupancy of $X$ can be broken into two parts:</p> <ol> <li> <strong>Immediate transitions</strong>: The probability of moving directly into $X$ in the next step, given by $P(X \vert s,a)$.</li> <li> <strong>Future discounted occupancy</strong>: The expected future measure $M^\pi(X \vert s^\prime, a^\prime)$, weighted by $\gamma$ and averaged over the next states $s^\prime$ and actions $a^\prime$.</li> </ol> <p>This equation is extremely powerful because, like the Bellman equations, it lets us compute $M^\pi$ recursively instead of summing over infinite time steps. This forms the foundation for the loss function used in Meta Motivo’s optimization pipeline, which will be discussed in <strong>TODO ADD SECTION</strong>!</p> <p>For people interested in the proof of this equality, we will prove it in the box below:</p> <div class="panel-group"> <div class="panel panel-default"> <div class="panel-heading"> <h4 class="panel-title"> <a data-toggle="collapse" href="#collapse1">Toggle Proof.</a> </h4> </div> <div id="collapse1" class="panel-collapse collapse"> <div class="panel-body"> <p><b>Proof</b>: By definition, we have $$M^\pi(X \vert s, a) = \sum_{t=0}^\infty \gamma^t \mathrm{Pr}(s_{t+1} \in X \vert s, a, \pi)\;.$$</p> <p>Splitting the sum into the first step ($t = 0$) and all later steps ($t \geq 1$), we get $$M^\pi(X \vert s, a) = \mathrm{Pr}(s_1 \in X \vert s, a) + \sum_{t=1}^\infty \gamma^t \mathrm{Pr}(s_{t+1} \in X \vert s, a, \pi)\;.$$ where we notice that $\mathrm{Pr}(s_1 \in X \vert s, a) = \mathrm{Pr}(X \vert s, a)$.</p> <p>By marginalizing over $s^\prime$ and $a^\prime$ and using the <b>law of total probability</b>, we can rewrite the latter part of the equation as $$\mathrm{Pr}(s_{t+1} \in X \vert s, a, \pi) = \sum_{s^\prime}P(s^\prime\vert s, a)\sum_{a^\prime}\pi(a^\prime \vert s^\prime)\mathrm{Pr}(s_{t+1} \in X \vert s^\prime, a^\prime, \pi)\;.$$</p> <p>Finally, we notice that this is an expectation over $s^\prime$ and $a^\prime$, thus we can write it as $$M^\pi(X \vert s, a) = \mathrm{Pr}(X \vert s, a) + \gamma \mathbb{E}_{s^\prime \sim P(\cdot \vert s, a), a^\prime \sim \pi(\cdot \vert s^\prime)}\left[\mathrm{Pr}(s_{t+1} \in X \vert s^\prime, a^\prime, \pi)\right]\;.$$</p> <p>Which gives rise to the measure-valued Bellman equation which we aimed to prove: $$M^\pi(X \vert s, a) = \mathrm{Pr}(X \vert s, a) + \gamma \mathbb{E}_{s^\prime \sim P(\cdot \vert s, a), a^\prime \sim \pi(\cdot \vert s^\prime)}\left[M^\pi(X \vert s^\prime, a^\prime)\right]\;\square$$</p> </div> </div> </div> </div> <h3 id="expressing-the-q-value-function-using-the-successor-measure">Expressing the Q-Value Function Using the Successor Measure</h3> <p>So far, we’ve defined the successor measure $M^\pi(X \vert s, a)$ as the discounted probability mass of visiting a set of states $X$ in the future under policy $\pi$. But how does this relate to the actual goal of reinforcement learning, which is to maximize cumulative reward?</p> <p>It turns out that the $Q$-value function, which tells us the expected return for taking an action in a state, can be directly computed from the successor measure. This connection is extremely useful because it allows us to express policy evaluation in terms of state occupancies, without needing to explicitly simulate future trajectories. Furthermore, as you will see, it allows us to decouple the successor measure from the reward function, which will allow for learning without any reward function.</p> <p>By definition, the action-value function for a given reward function $r: \mathcal{S} \rightarrow \mathbb{R}$ is:</p> \[Q^\pi_r(s,a) = \mathbb{E}\left[\sum_{t=0}^\infty\gamma^t r(s_{t+1})\vert s, a, \pi\right]\;.\] <p>Rewriting this expectation in terms of the successor measure, we obtain:</p> \[Q^\pi_r(s,a) = \int_{s^\prime \in \mathcal{S}} M^\pi(ds^\prime \vert s, a)r(s^\prime)\;.\] <p>This equation tells us that the Q-value function is just an integral over the successor measure, weighted by the reward function $r(s^\prime)$. It is important to note that here $M^\pi(ds^\prime \vert s, a)$ represents an infinitesimal probability mass assigned to the small region around $s^\prime$.</p> <p>We will once again prove this equivalence in the proof box below.</p> <div class="panel-group"> <div class="panel panel-default"> <div class="panel-heading"> <h4 class="panel-title"> <a data-toggle="collapse" href="#collapse2">Toggle Proof.</a> </h4> </div> <div id="collapse2" class="panel-collapse collapse"> <div class="panel-body"> <p><b>Proof</b>: Using linearity of expectation: $$Q^\pi_r(s, a) = \sum_{t=0}^{\infty} \gamma^t \mathbb{E}\left[r(s_{t+1}) \mid s, a, \pi\right]\;.$$</p> <p>By the <b>law of total probability</b>, the expectation over future states can be rewritten as: $$\mathbb{E}[r(s_{t+1}) \mid s, a, \pi] = \int_{s^\prime} \Pr(s_{t+1} = s^\prime \mid s, a, \pi) r(s^\prime) ds^\prime\;.$$</p> <p>Substituting this into the summation: $$Q^\pi_r(s, a) = \sum_{t=0}^{\infty} \gamma^t \int_{s^\prime} \Pr(s_{t+1} = s^\prime \mid s, a, \pi) r(s^\prime) ds^\prime\;.$$</p> <p>Rearranging the summation and the integral (assuming that this is possible): $$Q^\pi_r(s, a) = \int_{s^\prime} \sum_{t=0}^{\infty} \gamma^t \Pr(s_{t+1} = s^\prime \mid s, a, \pi) r(s^\prime) ds^\prime\;.$$</p> <p>From our <b>definition of the successor measure</b>, we recognize that: $$M^\pi(ds^\prime \mid s, a) = \sum_{t=0}^{\infty} \gamma^t \Pr(s_{t+1} = s^\prime \mid s, a, \pi) ds^\prime\;.$$</p> <p>Thus, we obtain: $$Q^\pi_r(s, a) = \int_{s^\prime \in S} M^\pi(ds^\prime \mid s, a) r(s^\prime)\;\square$$ which is exactly what we wanted to prove.</p> </div> </div> </div> </div> <p>Intuitively, this representation of the Q-function means the following:</p> <ul> <li>$M^\pi(s^\prime \vert s, a)$ represents how much influence state $s^\prime$ has on the future when starting from $(s, a)$.</li> <li>The reward function $r(s^\prime)$ determines how valuable each state is.</li> <li>The Q-function simply combines these two quantities, giving more weight to states that are both frequently visited and have high rewards.</li> </ul> <p>This formulation is powerful because it means that once we know the successor measure, we can compute Q-values for any reward function immediately—without re-running reinforcement learning. If we change the reward function, we only need to update the integral, rather than recomputing the entire policy from scratch.</p> <p>Now that we’ve seen that Q-values can be computed directly from the successor measure, a natural question arises:</p> <blockquote> <p>How can we efficiently store and compute the successor measure without needing to explicitly track every possible future state?</p> </blockquote> <p>This is where Forward-Backward (FB) Representations come into play. Instead of storing a full probability measure over future states, we will learn compact representations that approximate the successor measure efficiently.</p> <p>In the next section, we introduce FB Representations, which allow us to express the successor measure as the inner product of two learned functions. This will give us a structured way to reuse learned knowledge across multiple tasks while keeping computations efficient.</p> <h2 id="forward-backward-representations-structuring-the-successor-measure">Forward-Backward Representations: Structuring the Successor Measure</h2> <p>In the previous section, we introduced the successor measure, which captures how an agent moves through an environment when following a given policy. While this measure is a powerful tool for understanding state occupancies, storing and computing it explicitly can be infeasible—especially in large or continuous state spaces. If we wanted to store $M^\pi(s^\prime \vert s, a)$ exactly for every possible pair $(s,a)$, we would need to keep track of an entire distribution over future states for every starting point. This quickly becomes computationally intractable.</p> <p>Instead of explicitly storing $M^\pi$, we can learn a compact, structured representation of it using Forward-Backward ( FB) Representations. The key idea behind FB Representations is to approximate the successor measure using two components:</p> <ul> <li>A <strong>forward embedding</strong> $F^\pi(s,a)$ which captures how state-action pairs influence future states.</li> <li>A <strong>backward embedding</strong> $B(s^\prime)$ which captures how states are reached from past states and actions.</li> </ul> <p>This allows us to efficiently approximate the successor measure as an inner product between these learned embeddings. The FB representation aims to learn a finite-rank approximation:</p> \[M^\pi(X \vert s, a) \approx \int_{s^\prime \in X} F^\pi(s, a)^\intercal B(s^\prime) \rho(ds^\prime)\;.\] <p><strong>TODO: Is there some bound of closeness on this term?</strong></p> <p>where:</p> <ul> <li>$\rho^\pi(X) = (1-\gamma)\mathbb{E}_{s\sim\mu, a\sim\pi(\cdot, s)}\left[M^\pi(X\vert s,a)\right]$ is the stationary discounted distribution of $\pi$ ($\mu$ is a distribution over initial states).</li> <li>$F^\pi: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}^d$ is the forward embedding, encoding how actions influence future states.</li> <li>$B: \mathcal{S} \rightarrow \mathbb{R}^d$ is the backward embedding, encoding how states contribute to state occupancy.</li> </ul> <p>This formulation means that, instead of storing a high-dimensional probability measure, we only need to store and update two low-dimensional representations, making computation and generalization across tasks significantly easier.</p> <h3 id="expressing-the-q-function-with-fb-representations">Expressing the Q-Function with FB Representations</h3> <p>Using the approximation to the successor measure above, we can now express the Q-value function in terms of FB representations. Recall that the Q-function is computed as:</p> \[Q^\pi_r(s,a) = \int_{s^\prime \in \mathcal{S}} M^\pi(ds^\prime \vert s, a)r(s^\prime)\;.\] <p>Substituting the FB decomposition, we get:</p> \[Q^\pi_r(s,a) = \int_{s^\prime \in \mathcal{S}} F^\pi(s, a)^\intercal B(s^\prime) \rho(ds^\prime)r(s^\prime)\;.\] <p>If we now define a <strong>task encoding vector</strong> as:</p> \[z = \mathbb{E}_{s \sim \rho}\left[B(s)r(s)\right]\;.\] <p>We can rewrite the Q-function as:</p> \[Q^\pi_r(s,a) = F^\pi(s, a)^\intercal z\;.\] <p>This means that once we have learned $F^\pi(s,a)$ and $B(s)$, we can compute Q-values for <strong>any reward function</strong> instantly by computing the appropriate task vector $z$. This is extremely powerful, as it allows an agent to generalize across multiple tasks without retraining (zero-shot). The task encoding vector $z$ acts as a mapping of the reward onto the backward embedding $B$.</p> <h3 id="fb-representations-for-multiple-policies">FB Representations for Multiple Policies</h3> <p>Instead of learning a single policy, we can generalize this framework to a <strong>distribution of policies</strong> indexed by a latent variable $z$ which controls task-specific behavior. This approach, proposed by <strong>TODO</strong>, learns a <strong>unified representation space</strong> for both embeddings and policies.</p> <p>The key equations for this generalization are:</p> \[\begin{cases} M^\pi_z(X \mid s, a) \approx \int_{s^\prime \in X} F(s, a, z)^\top B(s^\prime) \rho(ds^\prime), &amp; \forall s \in S, a \in A, X \subset S, z \in Z, \\ \pi_z(s) = \arg\max_a F(s, a, z)^\top z, &amp; \forall (s, a) \in S \times A, z \in Z. \end{cases}\] <p>where:</p> <ul> <li>$ Z \subseteq \mathbb{R}^d $ is the space of <strong>task encoding vectors</strong> (e.g., a unit hypersphere with radius $\sqrt{d}$).</li> <li>$ \pi_z(s) $ is the <strong>policy associated with task encoding $ z $</strong>.</li> <li>The <strong>same task encoding $ z $</strong> parameterizes both $ F $ and $ \pi $ ensuring <strong>consistency across tasks</strong>.</li> </ul> <p>With this representation, The policy $\pi_z(s) = \arg\max_a F(s, a, z)^\top z$ naturally follows from the learned representations, providing an efficient way to optimize for different tasks with any reward function.</p> <p><strong>TODO: Go into more detail on why this is possible</strong>.</p> <h2 id="learning-forward-backward-representations">Learning Forward-Backward Representations</h2> <p>So far, we have established that FB representations allow us to efficiently approximate the successor measure using a low-dimensional factorization. However, to be useful in practice, we need to learn the forward and backward embeddings $F(s,a,z)$ and $B(s)$ in a way that ensures they accurately capture the successor structure.</p> <p>The key idea is to train these representations to satisfy the Bellman equation for successor measures, which we saw in section <strong>TODO</strong>, ensuring that they correctly predict the future state distribution. This leads to a temporal difference (TD) loss function, which aligns $F$ and $B$ through self-supervised learning.</p> <h3 id="learning-via-temporal-difference-loss">Learning via Temporal Difference Loss</h3> <p>To ensure that $ F $ and $ B $ satisfy the successor measure recursion, we minimize the <strong>Bellman residual</strong>, which comes from the previously seen equation from section <strong>TODO</strong>:</p> \[M^\pi(s^+ \mid s, a) = P(s^+ \mid s, a) + \gamma \mathbb{E}_{s^\prime, a\prime \sim \pi} \left[M^\pi(s^+ \mid s^\prime, a^\prime)\right].\] <p>The <strong>learning objective</strong> is then constructed by taking the squared Bellman residual as the primary loss term with an added regularization term:</p> \[\mathcal{L}_{\text{FB}}(F, B) = \mathbb{E}_{z \sim \nu, (s,a,s^\prime) \sim \rho, s^+ \sim \rho, a^\prime \sim \pi_z(s^\prime)} \left[ \left( F(s, a, z)^\top B(s^+) - \gamma \overline{F}(s^\prime, a^\prime, z)^\top \overline{B}(s^+) \right)^2 \right]\] \[- 2\mathbb{E}_{z \sim \nu, (s,a,s^\prime) \sim \rho} \left[ F(s, a, z)^\top B(s^\prime) \right]\;.\] <p>where:</p> <ul> <li>$\nu$ is a distribution over $Z$, and $ (s, a, s^+) \sim \rho $ represents <strong>sampled transitions</strong> from the environment.</li> <li>$ \overline{F}, \overline{B} $ denote <strong>stop-gradient</strong> operations to prevent instability in training.</li> <li>The first expectation term ensures that $ F(s, a, z)^\top B(s^+) $ aligns with the <strong>Bellman recursion</strong>.</li> <li>The second expectation term acts as a <strong>regularization term</strong>, ensuring that $ F(s, a, z) $ and $ B(s) $ form a structured representation space by attempting to maximize the cosine similarity between them.</li> </ul> <p>The derivation of this loss can once again be found again below.</p> <div class="panel-group"> <div class="panel panel-default"> <div class="panel-heading"> <h4 class="panel-title"> <a data-toggle="collapse" href="#collapse3">Toggle Proof.</a> </h4> </div> <div id="collapse3" class="panel-collapse collapse"> <div class="panel-body"> <p><b>Proof</b>: Substituting the <b>FB decomposition</b> $ M^\pi(s^\prime \mid s, a) \approx F(s, a, z)^\top B(s^\prime) $, we approximate the successor measure equation as: $$F(s, a, z)^\top B(s^+) \approx P(s^+ \mid s, a) + \gamma \mathbb{E}_{s^\prime, a^\prime \sim \pi} \left[ F(s^\prime, a^\prime, z)^\top B(s^+) \right]\;.$$</p> <p>Since we are not explicitly modeling the transition probability $ P(s^+ \mid s, a) $, we replace it with a <b>learned approximation</b> directly through $ F(s, a, z)^\top B(s^+) $, leading to the residual: $$\delta(s, a, s^+, s^\prime, a^\prime, z) = \left( F(s, a, z)^\top B(s^+) - \gamma F(s^\prime, a^\prime, z)^\top B(s^+) \right)\;.$$</p> <p>Minimizing this **Bellman residual** ensures that $ F(s, a, z) $ correctly models the <b>recursive nature</b> of the successor measure.</p> </div> </div> </div> </div> <p>By minimizing this loss, we ensure that <strong>FB representations accurately approximate the successor measure while maintaining temporal consistency</strong>.</p> <h3 id="training-the-policy-with-fb-representations">Training the Policy with FB Representations</h3> <p>Since the policy is defined as:</p> \[\pi_z(s) = \arg\max_a F(s, a, z)^\top z,\] <p>in <strong>continuous action spaces</strong>, the $ \arg\max $ operation is not directly differentiable. Instead, we approximate it via <strong>policy learning</strong> by training an actor network to minimize:</p> \[\mathcal{L}_{\text{actor}}(\pi) = -\mathbb{E}_{z \sim \nu, s \sim \rho, a \sim \pi_z(s)} \left[ F(s, a, z)^\top z \right].\] <p>This loss function ensures that the policy $ \pi_z(s) $ selects actions that <strong>maximize the learned Q-function</strong>. By minimizing $ \mathcal{L}_{\text{actor}} $, the policy learns to align its actions with those that maximize future state occupancy for a given task encoding $ z $. This method allows the policy to efficiently adapt to different tasks by leveraging the learned FB representations.</p> <h2 id="zero-shot-inference-with-fb-representations">Zero-Shot Inference with FB Representations</h2> <p><strong>TODO: Get a bit more into specifics and details here</strong></p> <p>A major advantage of FB representations is their ability to generalize across <strong>multiple tasks without retraining</strong>, enabling <strong>zero-shot inference</strong>. Given a dataset of reward samples $\{(s_i, r_i)\}_{i=1}^{n}$, the optimal <strong>task encoding for reward maximization</strong> is computed as:</p> \[z_r = \frac{1}{n} \sum_{i=1}^{n} r(s_i) B(s_i).\] <p>This allows an agent to <strong>instantaneously derive the optimal policy</strong> for any reward function simply by computing $z_r$, without requiring additional reinforcement learning. Similarly, for goal-reaching tasks, where the agents needs to reach a certain state $s \in \mathcal{S}$, the optimal <strong>goal-conditioned task encoding</strong> is:</p> \[z_s = \frac{1}{n} \sum_{i=1}^{n} r(s_i) B(s_i) = 1\left[s_i = s\right] B(s_i) = B(s).\] <p>This means that an agent can <strong>directly execute the policy</strong> $\pi_{z_s}$ to reach a specific state without additional planning.</p> <h3 id="imitation-learning-with-fb-representations"><strong>Imitation Learning with FB Representations</strong></h3> <p>FB representations also provide a framework for <strong>imitation learning</strong>, allowing an agent to infer a policy directly from expert demonstrations. Given a trajectory $\tau = (s_1, \dots, s_n)$ collected from an expert policy, the <strong>zero-shot inference</strong> of the task encoding is given by:</p> \[z_\tau = \mathbb{E}_{\text{FB}}(\tau) = \frac{1}{n} \sum_{i=1}^{n} B(s_i).\] <p>This inferred $ z_\tau $ serves as a <strong>representation of the expert’s behavior</strong>, allowing the agent to execute $ \pi_{z_\tau} $, effectively mimicking the expert without requiring additional fine-tuning.</p> <p>Zero-shot inference via FB representations enables agents to rapidly adapt to <strong>new rewards, goal-reaching tasks, and imitation learning</strong>, making it a highly <strong>flexible and scalable approach</strong> to policy learning.</p> <h3 id="limitations-of-fb-inference">Limitations of FB Inference</h3> <p>While FB models provide a powerful approach for generalization, their effectiveness depends on:</p> <ul> <li> <strong>State coverage</strong>: If the training dataset $\rho$ does not adequately cover the environment, FB models may struggle to generalize.</li> <li> <strong>Embedding capacity</strong>: When the embedding dimension $d$ is small, the learned representations may collapse to a few policies, limiting adaptability.</li> <li> <strong>Offline training bias</strong>: Since FB models are trained on pre-collected data, they may inherit biases from the dataset, limiting real-world performance.</li> </ul> <p>However, when trained with sufficient data and representation capacity, FB models can learn optimal policies for any reward function and enable fast adaptation in diverse settings.</p> <h2 id="optimizing-fb-representations-for-real-world-use">Optimizing FB Representations for Real-World Use</h2> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/blog/2025-02-20-meta-motivo.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Lars C.P.M. Quaedvlieg. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-JKH10LEP3Y"></script> <script defer src="/assets/js/google-analytics-setup.js"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>