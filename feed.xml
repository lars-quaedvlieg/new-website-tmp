<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://lars-quaedvlieg.github.io//feed.xml" rel="self" type="application/atom+xml"/><link href="https://lars-quaedvlieg.github.io//" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-21T14:16:31+00:00</updated><id>https://lars-quaedvlieg.github.io//feed.xml</id><title type="html">Lars Quaedvlieg</title><subtitle>The portfolio website of Lars C.P.M. Quaedvlieg </subtitle><entry><title type="html">Foundation Models for Sequential Decision-Making</title><link href="https://lars-quaedvlieg.github.io//blog/2024/foundation-model-decision-making/" rel="alternate" type="text/html" title="Foundation Models for Sequential Decision-Making"/><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://lars-quaedvlieg.github.io//blog/2024/foundation-model-decision-making</id><content type="html" xml:base="https://lars-quaedvlieg.github.io//blog/2024/foundation-model-decision-making/"><![CDATA[<figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/open-x-dataset.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Figure taken from [3]</figcaption> </figure> <p>In this post, I was hoping to give some sort of timeline of advances made in fields like reinforcement learning and robotics that I believe could be important to know about or might be important to realize AI embodied in the real world. We will start by reviewing some classical papers that combine Transformers with reinforcement learning for multi-task control. Then, we will look at some more recent advances that are used in <strong>simulated</strong> open-ended environments, and from there on we will move on to advances in control for real-world robotics with robotics foundation models. Finally, we will talk about some challenges that these current approaches have and what future opportunities exist for people interested in the field.</p> <p><strong>Discussed topics:</strong></p> <ul> <li> <p>(Multi-Game) Decision Transformers.</p> </li> <li> <p>Foundation Models for Sequential Decision-Making.</p> </li> <li> <p>Example of Foundation Models in Massively Open-Ended Environments.</p> </li> <li> <p>Foundation Models and Robotics.</p> </li> </ul> <h2 id="classical-papers-transformers-for-reinforcement-learning">Classical Papers: Transformers for Reinforcement Learning</h2> <p>In this section, we will talk about two classical papers in the field of reinforcement learning that were the first working approaches that utilized transformers as a backbone for (multi-game) control.</p> <h3 id="decision-transformers-2021">Decision Transformers (2021)</h3> <p>The first paper, called the <strong>Decision Transformer</strong> [1], acknowledges that <em>reinforcement learning can trivially be framed as a sequence modelling problem</em>. Your sequence would consist of observation, action, reward, and terminal state tokens. In the paper, instead of using the rewards, the authors decide to use the <em>return-to-go</em>, <em>$\hat{R_t} = \sum_{t^\prime = t}^T r_{t^\prime}$</em>, since it pre-trains on trajectories that were collected beforehand, so they can be computed. This means that the method does not train online! Their sequences look as follows:</p> \[\tau = (\hat{R_1}, s_1, a_1, \hat{R_2}, s_2, a_2, \dots, \hat{R_T}, s_T, a_T)\;.\] <p><strong>Note:</strong> If you would do this in an online settings, you would need to estimate the return, since you cannot compute the return-to-go’s. This makes it <strong>a lot more difficult</strong>, and current methods struggle with this!</p> <h4 id="architecture-and-training">Architecture and Training</h4> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/decision-transformer-architecture.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Architecture of the Decision Transformer.</figcaption> </figure> <p>In the figure above, you can see how these sequences are encoded into a causal transformer. Note that each <em>modality</em> (e.g. return, state, and action) are encoded with their own embedding scheme, since different modalities might benefit from different embeddings. In addition to this embedding, they add a positional embedding that is <strong>per-timestep</strong>. In contrary to text-based models, one timestep in this sequence is a 3-tuple $(\hat{R}_t, s_t, a_t)$, so these receive the same positional embedding.</p> <p>The model attempts to predict actions using a linear decoder on top of the observation encodings of the Transformer backbone. The model is trained using a simple <strong>behaviour cloning</strong> loss, which is the mean-squared error between prediction and actual actions. One model is trained per environment, so it is not a multi-task method!</p> <h4 id="inference-online">Inference (Online)</h4> <p>Since you want the model to output actions with the <strong>highest estimated return-to-go</strong>, you cannot simply sample actions naively from the transformer. Furthermore, you do not have these return-to-go’s at inference time, since you are now running online experiments, and you do not know your return in advance.</p> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/decision-transformer-inference.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Inference with the Decision Transformer.</figcaption> </figure> <p>In the paper, the authors normalized the returns to be in a $[-1, 1]$ range. Hence, by manually setting the first return-to-go $\hat{R}_1 = 1$, you are explicitly conditioning the model to generate the actions are yield the highest return-to-go.</p> <h4 id="experiments-and-results">Experiments and Results</h4> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/decision-transformer-res1.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center"></figcaption> </figure> <p>The authors evaluated this method on an offline reinforcement learning dataset for Atari called the DQN-Replay Atari dataset and benchmarked it against various offline reinforcement learning algorithms. The method seems to perform comparably to CQL, a powerful offline RL method, for 3 out of 4 games, which is not bad given the simplicity of the method!</p> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/decision-transformer-res2.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center"></figcaption> </figure> <p>They also benchmark on some other environments where CQL and BC do not perform as well, and show the consistency of their method across multiple tasks!</p> <h3 id="multi-game-decision-transformers-2022">Multi-Game Decision Transformers (2022)</h3> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/mg-decision-transformers-games.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center"></figcaption> </figure> <p>The follow-up work [2] on the Decision Transformer paper tries to develop a multi-game agent that has a Transformer backbone, which they call the “Multi-Game Decision Transformer”. The setting stays similar to the original paper; the model is still pre-trained using datasets of trajectories that are collected by policies of various skill levels. In this paper, the Transformer is also scaled to up to $200$M parameters.</p> <p>In contrast to the original paper, this approach tries to do meta-learning: after pre-training a model, you should be able to use it to efficiently fine-tune the parameters to new games. This concept is quite similar to how you can fine-tune LLMs for task-specific purposes.</p> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/mg-decision-transformers-architecture.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Architecture of the Multi-Game Decision Transformer.</figcaption> </figure> <p>In this paper, the sequence is constructed in a slightly different manner:</p> \[\tau = (\dots, o_t^1, \cdots o_t^M, \hat{R}_t, a_t, r_t, \cdots)\;.\] <p>Note that the return-to-go is no longer at the front of the sequence. Furthermore, there are multiple observation tokens $o_t^1, \cdots, o_t^M$ that correspond to $M$ tokenized patches in the image observations. They also include the rewards in the trajectory sequence. This design allows <strong>predicting the return distribution and sampling from it</strong>, instead of relying on a user to manually select an expert-level return at inference time.</p> <p>They also scale the reward in a $[-1, 1]$ range and quantize the returns into bins of ${-20, -19, \cdots, 100}$.</p> <h4 id="training">Training</h4> <p>The paper focusses on evaluation in the Atari domain. They also use the DQN-Replay Atari dataset, from which they select $41$ games for training and $5$ holdout games for out-of-distribution generalization. For each game, they use $50$ policy checkpoints which each contain $1,000,000$. In total, this corresponds to $4.1$ billion environment steps.</p> <h4 id="inference-online-1">Inference (Online)</h4> <p>As described above, the training datasets contain a mix of expert and non-expert behaviours, thus directly generating actions from the model imitating the data is unlikely to consistently produce expert behaviour.</p> <p>Instead, the authors propose an inference-time method of sampling actions that are estimated to have a high return-to-go. They assume there is a binary classifier $P(\mathrm{expert}_t \vert \dots)$ that classifies whether a behaviour is expert-level before taking an action $a_t$. This can be written as:</p> \[P(\mathrm{expert}_t \vert R_t) \propto \exp(\kappa \frac{R_t - R_{\mathrm{low}}}{R_{\mathrm{high}} - R_{\mathrm{low}}})\;.\] <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/mg-decision-transformers-sampling.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center"></figcaption> </figure> <p>We want to sample a return $R$ so we can continue sampling actions at inference time, since we do not have access to a return-to-go. Hence, we want to model a distribution $P(R_t \vert\mathrm{expert}_t)$. We will do this using Bayes’ rule:</p> \[P(R_t \vert\mathrm{expert}_t) \propto P(\mathrm{expert}_t \vert R_t)P(R_t \vert \cdots)\\ \log P(R_t \vert\mathrm{expert}_t) \propto \log P(R_t \vert \dots) + \kappa \frac{R_t - R_{\mathrm{low}}}{R_{\mathrm{high}} - R_{\mathrm{low}}}\;.\] <p>Using these two distributions, you can now sample $R_t$ from $P(R_t \vert\mathrm{expert}_t)$ and obtain expert-level trajectories. This is also depicted in the figure above.</p> <h4 id="experiments-and-results-1">Experiments and Results</h4> <p>The nice thing about this paper is that it has very extensive results on the Atari domain, which we will take a look at here. They ask the following questions:</p> <ul> <li> <p>How do different online and offline methods perform in the multi-game regime?</p> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/mg-decision-transformers-res1.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center"></figcaption> </figure> <blockquote> <p>The single-game specialists are still most performant. Among multi-game generalist models, the Multi-Game Decision Transformer model comes closest to specialist performance. Multi-game online RL with non-transformer models comes second, while they struggled to get good performance with offline non-transformer models.</p> </blockquote> </li> <li> <p>How do different methods scale with model size?</p> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/mg-decision-transformers-res2.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center"></figcaption> </figure> <blockquote> <p>Multi-Game Decision Transformer performance reliably increases over more than an order of magnitude of parameter scaling, whereas the other methods either saturate, or have much slower performance growth.</p> </blockquote> </li> <li> <p>How effective are different methods at transfer to novel games?</p> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/mg-decision-transformers-res3.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center"></figcaption> </figure> <blockquote> <p>Pretraining with the DT objective performs the best across all games. All methods with pretraining outperform training CQL from scratch, which verifies the hypothesis that pretraining on other games should indeed help with rapid learning of a new game</p> </blockquote> </li> <li> <p>Does expert action inference improve upon behavioural cloning?</p> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/mg-decision-transformers-res4.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center"></figcaption> </figure> <blockquote> <p>Using sampling expert behaviours, they see significant improvement over the training data in a number of games.</p> </blockquote> </li> <li> <p>Does training on expert and non-expert data bring benefits over expert-only training?</p> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/mg-decision-transformers-res5.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center"></figcaption> </figure> <blockquote> <p>They observe that (1) Training only on expert data improves behavioural cloning; (2) Training on full data, including expert and non-expert data, improves Decision Transformer; (3) Decision Transformer with full data outperforms behavioural cloning trained on expert data.</p> </blockquote> </li> </ul> <h2 id="foundation-models-for-sequential-decision-making-2023">Foundation Models for Sequential Decision-Making (2023)</h2> <p>Now that we have seen some classical approaches to doing reinforcement learning with Transformers with models that scale up to $200$M parameters, we will take a look at approaches that utilize (multi-modal) foundation models for sequential decision-making. This section will be based on a great survey [3] to help relate this topic to what we have seen in the classical approaches. The following is the motivation behind this survey:</p> <blockquote> <p>Foundation models, which are trained on vast and varied data, show great potential in vision and language tasks and are increasingly used in real-world settings, interacting with humans and navigating environments. New training paradigms are developing to enhance these models’ ability to engage with other agents and perform complex reasoning, utilizing large, multimodal datasets. This survey explores how foundation models can be applied to decision making, presenting conceptual and technical resources to guide research. It also reviews current methods like prompting and reinforcement learning that integrate these models into practical applications, highlighting challenges and future research opportunities.</p> </blockquote> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/fmdm-background.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center"></figcaption> </figure> <p>On a high level, the goal is to enable pre-trained foundation models to quickly accomplish decision-making problems by interacting with an environment or external entity, similar to reinforcement learning.</p> <h3 id="generative-models-of-behaviour">Generative Models of Behaviour</h3> <p>Generally, generative models are applied to static text or image data $x \in \mathcal{D}$. On the other hand, for decision-making, you are concerned with task-specific <em>interactive</em> data $\tau \in \mathcal{D}_\mathrm{RL}$, which often distinguishes states, actions, and reward labels.</p> <p>If you want to build a generally capable systems, your dataset will need to include diverse behaviours. For example, in robotics, you might need a set of so-called <em>behaviour priors</em> such as “pick up objects” or “move objects” which can be composed to complete tasks. As we saw in the classical approaches, usually a model is fit using an imitation learning objective.</p> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/fmdm-gen-models.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center"></figcaption> </figure> <p>The figure above shows some different ways of modelling behaviours, improvements, environment, and long-term futures given trajectories $\tau$. Unfortunately, many of these approaches rely on near-expert data to perform well, often due to limits in extrapolation beyond the training dataset.</p> <p>Luckily, one key advantage to generative modelling of behaviour lies in scaling up: even though tasks often have different observations and rewards, there are often shared meaningful behaviours across different tasks. This makes it worth it to train agents on massive behaviour datasets, since they might extract generalizable behaviours from them.</p> <ol> <li>One way of scaling and training generalizable models is through combining multiple task-specific datasets $\mathcal{D}_\mathrm{RL}$, but in the real world, there is a lack of data that is directly posed in such a way that it can be frames as a reinforcement learning problem.</li> <li>Instead, it might be worth it to look into exploiting internet-scale text and video data for training these models, since there is an abundant quantity of this data. The main downside is that this data often does not have explicit actions or numerical rewards. Nevertheless, you can still extract useful behaviour priors from them. This works because goals are often also formulated in natural language, even though this means that they can be quite sparse. One example of this is the goal “Mine diamonds” in Minecraft. Multiple papers have approached this problem in unique ways. For example, GATO [4] approach this issue with universal tokenization, so that data without actions can be jointly trained using large sequence models. Other approaches try to apply <em>inverse dynamics</em> to label large video data [5, 6].</li> <li>Another approach that has been used a lot in the past is <em>large-scale online learning</em>, which typically uses access to massive online game simulators such as DoTA, StarCraft, and Minecraft. The agents that are trained on these environments are typically optimized using online reinforcement learning.</li> </ol> <p>Furthermore, generative models of behaviour can also be extended to model meta-level processes, such as exploration and self-improvement if the dataset itself contains these behaviours.</p> <blockquote> <p>Similar to algorithm distillation, which prompts an agent with its prior learning experience, corrective re-prompting also treats long-horizon planning as an in-context learning problem, but uses corrective error information as prompts, essentially incorporating feedback from the environment as an auxiliary input to improve the executability of a derived plan. [7]</p> </blockquote> <h3 id="foundation-models-as-representation-learners">Foundation Models as Representation Learners</h3> <p>As we saw, one way to use foundation models for decision making is by <strong>leveraging representation learning for knowledge compression</strong>. On one hand, foundation models can extract representations from broad image and text data, $D$, resulting in a plug-and-play style of knowledge transfer to vision and language based decision making tasks. On the other hand, foundation models can also be used to support task-specific representation learning via task-specific objectives and interactive data, $\mathcal{D}_\mathrm{RL}$.</p> <ul> <li><strong>Plug-and-Play:</strong> Off-the-shelf foundation models pretrained on Internet-scale text and image data can be used as pre-processors or initializers for various perceptual components of decision making agents. <ul> <li>Even in the case where an agent’s states, actions, and rewards do not consist of images or text, pretrained language models, perhaps surprisingly, have still been found useful as policy initializers for offline RL [8], online RL [9], and structured prediction tasks [10].</li> </ul> </li> <li><strong>Vision and Language as Task Specifiers:</strong> An important special case of plug-and-play foundation models is to use text commands or visual inputs as task specifiers to learn more robust, general, and multi-task policies. <ul> <li>Using vision and language task specifiers to prompt for desirable agent behaviours requires additional data such as text descriptions or goal images of a given task.</li> <li>However, prompting for desirable outcomes from a large language model has significant potential but is also an open problem in itself, whose complexity is exacerbated in decision making scenarios with external entities and world dynamics.</li> </ul> </li> </ul> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/fmdm-objectives.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center"></figcaption> </figure> <p>Unlike vision-language foundation models that can learn from a broad data collection $D$ but lack the notion of decision making, foundation model techniques and architectures (as opposed to the pretrained models themselves) can be used to optimize objectives uniquely devised for sequential decision making on the basis of task-specific interactive data $D_\mathrm{RL}$. The figure above visually illustrates different representation learning objectives.</p> <p>One important thing to note is that unlike generative foundation models that can directly produce action or next state samples, foundation models as representation learners are only directed to extract representations of states, actions, and dynamics; hence <strong>they require additional finetuning or model-based policy optimization</strong> to achieve strong decision making performance.</p> <h3 id="large-language-models-as-agents-and-environments">Large Language Models as Agents and Environments</h3> <p>In this section, we consider a special case where pretrained large language models can serve as agents or environments. Treating language models as agents, on one hand, enables learning from environment feedback produced by humans, tools, or the real world, and on the other hand enables new applications such as information retrieval and web navigation to be considered under a sequential decision making framework.</p> <p>There are a few different ways that LLMs can be used and are currently used as agents and environments:</p> <ul> <li><strong>Interacting with humans:</strong> You can frame the dialogue as an MDP and optimize dialogue agents using alignment techniques. Here, you have task-specific dialogue data $\mathcal{D}_\mathrm{RL}$.</li> <li><strong>Interacting with Tools:</strong> Language model agents that generate API calls (to invoke external tools and receive responses as feedback to support subsequent interaction) can be formulated as a sequential decision making problem analogous to the dialogue formulation.</li> <li><strong>Language Models as Environments:</strong> Iterative prompting can be characterized as an MDP that captures the interaction between a prompt provider and a language model environment. Under this formulation, various schemes for language model prompting can be characterized by high-level actions that map input strings to desired output strings using the language model.</li> </ul> <h2 id="foundation-models-for-minecraft">Foundation Models for Minecraft</h2> <p>Minecraft is an amazing game for the evaluation of embodied life-long learning agents, since it’s an open-ended open-world game that consists of a lot of possible tasks. These tasks can be rather easy, such as building a simple house, or more complicated like finishing the game’s storyline. The <strong>MineRL</strong> [11] competition <em>was</em> a competition track at NeurIPS where people have to build agents that solve various problems in the domain of Minecraft. These are some prior challenges and suites for the game:</p> <ul> <li><strong>MineRL Diamond Competition (2019, 2020, 2021)</strong>: The challenge was to develop an AI agent capable of obtaining a diamond in Minecraft, emphasizing sample-efficient reinforcement learning.</li> <li><strong>BASALT (2022): F</strong>ocused on agents learning from human feedback and demonstrations without rewards. The tasks included finding a cave, building a village house, and more.</li> <li><strong>MineDojo (2022)</strong>: A framework built for embodied agent research. It features a simulation suite with 1000s of open-ended and language-prompted tasks, where the AI agents can freely explore a procedurally generated 3D world with diverse terrains to roam, materials to mine, tools to craft, structures to build, and wonders to discover.</li> </ul> <h3 id="voyager-2023">Voyager (2023)</h3> <p>Voyager [12] by NVIDIA is the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention! It related to the previous section on LLMs as agents, as it interacts with GPT-4 via black-box queries. It consists of three components:</p> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/voyager-pipeline.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center"></figcaption> </figure> <ol> <li> <p><strong>An automatic curriculum that maximizes open-ended exploration.</strong></p> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/voyager-curriculum.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center"></figcaption> </figure> <p>The model is able to automatically construct new tasks in the curriculum by querying GPT-4, which comes up with tasks on an appropriate difficulty.</p> </li> <li> <p><strong>An ever-growing skill library of executable code for storing and retrieving complex behaviours.</strong></p> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/voyager-skill-library.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center"></figcaption> </figure> <p>Each time GPT-4 generates and verifies a new skill, they add it to the skill library, represented by a vector database. The key is the embedding vector of the program description (generated by GPT-3.5), while the value is the program itself.</p> <p>When faced with a new task proposed by the automatic curriculum, they first leverage GPT-3.5 to generate a general suggestion for solving the task, which is combined with environment feedback as the query context. Subsequently, they perform querying to identify the top-5 relevant skills.</p> </li> <li> <p><strong>A new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement.</strong></p> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/voyager-self-verification.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center"></figcaption> </figure> <p>Instead of manually coding success checkers for each new task proposed by the automatic curriculum, they instantiate another GPT-4 agent for self-verification. By providing VOYAGER’s current state and the task to GPT-4, they ask it to act as a critic and inform them whether the program achieves the task. In addition, if the task fails, it provides a critique by suggesting how to complete the task.</p> </li> </ol> <h4 id="experiments-and-results-2">Experiments and Results</h4> <p>The experiments have very good results; they achieve a new state-of-the-art in all of the benchmarks for Minecraft among other LLM-based agents, especially regarding long tasks that require a specific curriculum to achieve.</p> <div> <figure class="figure col-sm-7 float-right"> <img src="/assets/img/blog/foundation-models-decision/voyager-res.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Progression through the Minecraft skill tree.</figcaption> </figure> <ol> <li>Their method is able to unlock components of the tech tree that are quite involved, which other methods are nowhere close to achieving.</li> <li>The method is able to explore and traverse diverse terrains for $2.3$x longer distances compared to baselines.</li> </ol> </div> <h3 id="minedreamer-2024">MineDreamer (2024)</h3> <p>This [13] is a very recent (Mar 2024) approach, which I wanted to discuss as it is more similar to the previous section on Foundation Models as Representation Learners.</p> <p>The method essentially addresses the issue that foundation models for sequential decision-making struggle to enable agents to follow textual instructions steadily, due to the fact that:</p> <ol> <li>Many textual instructions are abstract for low-level control and models struggle to effectively understand.</li> <li>Many textual instructions are sequential, and executing them may require considering the current state and breaking down the task into multiple stages for step-by-step completion.</li> </ol> <p>On a high level, instead of using a pre-trained LLM as an agent, they fine-tune LLaVa to output goals and introduce the idea of <strong>Chain-of-Imagination</strong> to address the above issues.</p> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/minedreamer-architecture.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center"></figcaption> </figure> <p>The architecture this model, which they call the <b>imaginator</b>, encodes the current environment observation along with a textual task instruction. For goal understanding, they add $k$ <code>[GOAL]</code> tokens to the end of instruction and input them with current observation into LLaVA. Then LLaVA generates hidden states for the <code>[GOAL]</code> tokens, which the Q-Former processes to produce the feature $f^*$. Subsequently, the image encoder combines its output with $f^*$ in a diffusion model for instruction-based future goal imagination generation.</p> <p>This allows MineDreamer to decompose its potentially sparse problem instruction into feasible subgoals, which are imaged using diffusion. An overview of the Chain-of-Imagination mechanism is depicted in the figure below:</p> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/minedreamer-imagination.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center"></figcaption> </figure> <p>The imaginator imagines a goal imagination based on the instruction and current observation. The Prompt Generator transforms this into a precise visual prompt, considering both the instruction and observed image. The Visual Encoder encodes the current observation, integrates it with this prompt, and inputs this into VPT, a video-pretrained model. VPT then determines the agent’s next action, leading to a new observation, and the cycle continues.</p> <h4 id="experiments-and-results-3">Experiments and Results</h4> <p>Interestingly, the method appears to <em>swiftly</em> adapt to new instructions and achieves higher success rates than any other fine-tuned method. It also shows that the method is great for <em>command-switching for long-horizon tasks</em>, as it surpasses previous methods by quite a large margin. This can be seen in the figure below.</p> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/minedreamer-res.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center"></figcaption> </figure> <h2 id="foundation-models-and-robotics">Foundation Models and Robotics</h2> <p>In this last section of papers, we will briefly look at various works that try to train generalist robotics policies.</p> <h3 id="open-x-embodiment-2023">Open-X Embodiment (2023)</h3> <p>Robotics comes with a unique problem: Conventionally, robotic learning methods <strong>train a separate model for every application</strong>, every robot, and even every environment. One big question is whether it is possible to train a “generalist” X-robot policy that can be adapted efficiently to new <strong>robots, tasks, and environments</strong>? Unfortunately, there is not enough data to pre-train a policy for this goal. For this reason, many companies and academic labs formed a joint collaboration to develop the Open-X Embodiment dataset [14]. This is a massive dataset with currently $22$ embodiments, $527$ skills, and $60$ datasets, consisting of a total of $160,266$ tasks.</p> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/open-x-architecture.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">RT-X Models Architectures.</figcaption> </figure> <p>They train a high-capacity model called “RT-2-X” on this dataset, and show that it exhibits positive transfer between embodiments, tasks, and improves on the capabilities of several robotics. The model takes images and a text instruction as input and outputs discretized end-effector actions.</p> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/rt-2-architecture.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">RT-2 Model Architecture.</figcaption> </figure> <p>The way it does this is through fine-tuning a model called “RT-2”. This model, introduced in [15], is a family of large vision-language-action models (VLAs) trained on Internet-scale vision and language data along with robotic control data. RT-2 casts the tokenized actions to text tokens, e.g., a possible action may be “1 128 91 241 5 101 127”. As such, any pretrained vision-language model can be finetuned for robotic control, thus leveraging the backbone of VLMs and transferring some of their generalization properties. A figure that visualizes this method is depicted below.</p> <h4 id="experiments-and-results-4">Experiments and Results</h4> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/open-x-res1.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center"></figcaption> </figure> <p>The RT-1-X mean success rate is $50$% higher than that of either the Original Method or RT-1. The performance increase can be attributed to co-training on the robotics data mixture. The lab logos indicate the physical location of real robot evaluation, and the robot pictures indicate the embodiment used for the evaluation.</p> <div> <figure class="figure col-sm-7 float-right"> <img src="/assets/img/blog/foundation-models-decision/open-x-res2.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center"></figcaption> </figure> <p>For large-scale datasets (Bridge and RT-1 paper data), RT-1-X underfits and performs worse than the Original Method and RT-1. RT-2-X model with significantly many more parameters can obtain strong performance in these two evaluation scenarios.</p> </div> <p>In summary, the results showed that the RT-1-X policy has a $50$% higher success rate than the original, state-of-the-art methods contributed by different collaborating institutions, while the bigger vision-language-model-based version (RT-2-X) demonstrated ∼$3$× generalization improvements over a model trained only on data from the evaluation embodiment.</p> <h3 id="octo-2023">Octo (2023)</h3> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/octo-overview.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Overview of the Octo framework.</figcaption> </figure> <p>Building on top of the Open-X Embodiments dataset, Octo [16] is a transformer-based diffusion policy, pretrained on $800$k robot episodes. It supports flexible task and observation definitions and can be quickly finetuned to new observation and action spaces. They introduce two initial versions of Octo, Octo-Small ($27$M parameters) and Octo-Base ($93$M parameters).</p> <p>The design of the Octo model emphasizes flexibility and scale: the model is designed to support a variety of commonly used robots, sensor configurations, and actions, while providing a generic and scalable recipe that can be trained on large amounts of data. Octo supports both natural language instructions and goal images, observation histories, and multi-modal action distributions via diffusion decoding.</p> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/octo-architecture.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Octo architecture.</figcaption> </figure> <p>Furthermore, Octo was designed specifically to support efficient finetuning to new robot setups, including robots with different actions and different combinations of cameras and proprioceptive information. This design was selected specifically to make Octo a flexible and broadly applicable generalist robotic policy that can be utilized for a variety of downstream robotics applications and research projects.</p> <h4 id="experiments-and-results-5">Experiments and Results</h4> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/octo-results.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center"></figcaption> </figure> <p>Out-of-the-box, Octo can control multiple robots in environments from the pretraining data. When using natural language to specify tasks, it outperforms RT-1-X: the current best, openly available generalist robotic policy. It performs similarly to RT-2-X, a $55$-billion parameter model (which is much larger!).</p> <p>On the WidowX tasks, Octo achieved even better performance with goal image conditioning: $25$% higher on average. This is likely the case because goal images provide more information about how to achieve the task.</p> <p>Finally, Octo leads to better policies than starting from scratch or with pretrained weights, with an average success rate improvement of $55$% across the four evaluation tasks. Each task uses ~$100$ target demonstrations.</p> <h3 id="aloha-2023">Aloha (2023)</h3> <p>Finally, I wanted to briefly talk about a recent trending work on learning fine-grained bimanual manipulation with low-cost hardware called ALOHA. Since robotics can get expensive very quickly, this paper focusses on enabling low-cost ($$20$k budget for the setup) to do bimanual manipulation using Transformers.</p> <p>It is capable of teleoperating precise tasks such as threading a zip tie, dynamic tasks such as juggling a ping pong ball, and contact-rich tasks such as assembling the chain in the NIST board.</p> <figure class="figure float-right"> <img src="/assets/img/blog/foundation-models-decision/aloha-architecture.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Architecture of Aloha.</figcaption> </figure> <p>The paper introduces so-called “<strong>Action Chunking with Transformers</strong>”, which predicts a sequence of actions (“an action chunk”) instead of a single action like standard behaviour cloning, but do note that the model is still <strong>learning to imitate instructions</strong>. There is no reinforcement learning or anything to optimize the trajectory.</p> <p>The ACT policy is trained as the decoder of a Conditional VAE. It synthesizes images from multiple viewpoints, joint positions, and style variable $z$ with a transformer encoder, and predicts a sequence of actions with a transformer decoder. The encoder of CVAE compresses action sequence and joint observation into $z$, the “style” of the action sequence. At test time, the CVAE encoder is discarded and $z$ is simply set to the mean of the prior (i.e. zero).</p> <h4 id="experiments-and-results-6">Experiments and Results</h4> <p>The videos below show real-time rollouts of ACT policies, imitating from $50$ demonstrations for each task. For the four evaluation tasks, ACT obtains 96%, 84%, 64%, 92% success respectively.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <video width="320" height="240" controls=""> <source src="https://tonyzhaozh.github.io/aloha/resources/slot_battery.mp4" type="video/mp4"/> Your browser does not support the video tag. </video> </div> <div class="col-sm mt-3 mt-md-0"> <video width="320" height="240" controls=""> <source src="https://tonyzhaozh.github.io/aloha/resources/open_lid.mp4" type="video/mp4"/> Your browser does not support the video tag. </video> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <video width="320" height="240" controls=""> <source src="https://tonyzhaozh.github.io/aloha/resources/prep_tape.mp4" type="video/mp4"/> Your browser does not support the video tag. </video> </div> <div class="col-sm mt-3 mt-md-0"> <video width="320" height="240" controls=""> <source src="https://tonyzhaozh.github.io/aloha/resources/put_on_shoe.mp4" type="video/mp4"/> Your browser does not support the video tag. </video> </div> </div> <p>In additional experiments they also show that the policy is <em>reactive</em> and <em>robust</em> to previously unseen environmental disturbances instead of just memorizing the training data.</p> <h2 id="challenges-and-opportunities">Challenges and Opportunities</h2> <p>There are still <strong>plenty</strong> of challenges in these fields, and I have made and taken a small list of challenges from some of the papers we discussed:</p> <ul> <li>How to Leverage or Collect Datasets. <ul> <li>The broad datasets from vision and language $D$ and the task specific interactive datasets $D_\mathrm{RL}$ can be of distinct modalities and structures. For instance, when $D$ consists of videos, it generally does not contain explicit action labels indicating the cause-effect relationship between different frames, nor does it contain explicit reward labels indicating which videos are better than others, whereas actions and rewards are key components of $D_\mathrm{RL}$.</li> </ul> </li> <li>How to Structure Environments and Tasks. <ul> <li>Unlike vision and language where images or texts can serve as a universal task interface, decision making faces environment diversity where different environments operate under distinct state action spaces (e.g., the joint space and continuous controls in MuJoCo are fundamentally different from the image space and discrete actions in Atari), thereby preventing knowledge sharing and generalization.</li> </ul> </li> <li>Improving Foundation Models. <ul> <li>Long-context and External Memory.</li> <li>Combining multiple foundation models.</li> <li>Grounding foundation models in the world.</li> </ul> </li> <li>Improving Decision Making. <ul> <li>How to extract desirable behaviour.</li> <li>Offline to online.</li> </ul> </li> <li>Robots with very different sensing and actuation modalities.</li> <li>Going beyond behaviour cloning for robotics.</li> </ul> <hr/> <h2 id="bibliography">Bibliography</h2> <p>[1] Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., … &amp; Mordatch, I. (2021). Decision transformer: Reinforcement learning via sequence modeling. <em>Advances in neural information processing systems</em>, <em>34</em>, 15084-15097.</p> <p>[2] Lee, K. H., Nachum, O., Yang, M. S., Lee, L., Freeman, D., Guadarrama, S., … &amp; Mordatch, I. (2022). Multi-game decision transformers. <em>Advances in Neural Information Processing Systems</em>, <em>35</em>, 27921-27936.</p> <p>[3] Yang, S., Nachum, O., Du, Y., Wei, J., Abbeel, P., &amp; Schuurmans, D. (2023). Foundation models for decision making: Problems, methods, and opportunities. <em>arXiv preprint arXiv:2303.04129</em>.</p> <p>[4] Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., … &amp; de Freitas, N. (2022). A generalist agent. <em>arXiv preprint arXiv:2205.06175</em>.</p> <p>[5] Baker, B., Akkaya, I., Zhokov, P., Huizinga, J., Tang, J., Ecoffet, A., … &amp; Clune, J. (2022). Video pretraining (vpt): Learning to act by watching unlabeled online videos. <em>Advances in Neural Information Processing Systems</em>, <em>35</em>, 24639-24654.</p> <p>[6] Venuto, D., Yang, S., Abbeel, P., Precup, D., Mordatch, I., &amp; Nachum, O. (2023, July). Multi-environment pretraining enables transfer to action limited datasets. In <em>International Conference on Machine Learning</em> (pp. 35024-35036). PMLR.</p> <p>[7] Raman, S. S., Cohen, V., Rosen, E., Idrees, I., Paulius, D., &amp; Tellex, S. (2022, November). Planning with large language models via corrective re-prompting. In <em>NeurIPS 2022 Foundation Models for Decision Making Workshop</em>.</p> <p>[8] Reid, M., Yamada, Y., &amp; Gu, S. S. (2022). Can wikipedia help offline reinforcement learning?. <em>arXiv preprint arXiv:2201.12122</em>.</p> <p>[9] Li, S., Puig, X., Paxton, C., Du, Y., Wang, C., Fan, L., … &amp; Zhu, Y. (2022). Pre-trained language models for interactive decision-making. <em>Advances in Neural Information Processing Systems</em>, <em>35</em>, 31199-31212.</p> <p>[10] Lu, K., Grover, A., Abbeel, P., &amp; Mordatch, I. (2022, June). Frozen pretrained transformers as universal computation engines. In <em>Proceedings of the AAAI conference on artificial intelligence</em> (Vol. 36, No. 7, pp. 7628-7636).</p> <p>[11] Guss, W. H., Castro, M. Y., Devlin, S., Houghton, B., Kuno, N. S., Loomis, C., … &amp; Vinyals, O. (2021). The minerl 2020 competition on sample efficient reinforcement learning using human priors. <em>arXiv preprint arXiv:2101.11071</em>.</p> <p>[12] Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., … &amp; Anandkumar, A. (2023). Voyager: An open-ended embodied agent with large language models. <em>arXiv preprint arXiv:2305.16291</em>.</p> <p>[13] Zhou, E., Qin, Y., Yin, Z., Huang, Y., Zhang, R., Sheng, L., … &amp; Shao, J. (2024). MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control. <em>arXiv preprint arXiv:2403.12037</em>.</p> <p>[14] Padalkar, A., Pooley, A., Jain, A., Bewley, A., Herzog, A., Irpan, A., … &amp; Jain, V. (2023). Open x-embodiment: Robotic learning datasets and rt-x models. <em>arXiv preprint arXiv:2310.08864</em>.</p> <p>[15] Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, X., Choromanski, K., … &amp; Zitkovich, B. (2023). Rt-2: Vision-language-action models transfer web knowledge to robotic control. <em>arXiv preprint arXiv:2307.15818</em>.</p> <p>[16] Team, O. M., Ghosh, D., Walke, H., Pertsch, K., Black, K., Mees, O., … &amp; Levine, S. (2023). Octo: An open-source generalist robot policy.</p>]]></content><author><name>Lars C.P.M. Quaedvlieg</name></author><category term="decision-making"/><summary type="html"><![CDATA[In this post, I'm hoping to give some sort of timeline of advances made in fields like reinforcement learning and robotics that I believe could be important to know about or might be important to realize AI embodied in the real world.e will start by reviewing some classical papers that combine Transformers with reinforcement learning for multi-task control. Then, we will look at some more recent advances that are used in simulated open-ended environments, and from there on we will move on to advances in control for real-world robotics with robotics foundation models.]]></summary></entry><entry><title type="html">CS-330 Lecture 8: Variational Inference</title><link href="https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-var-inf/" rel="alternate" type="text/html" title="CS-330 Lecture 8: Variational Inference"/><published>2024-03-30T00:00:00+00:00</published><updated>2024-03-30T00:00:00+00:00</updated><id>https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-var-inf</id><content type="html" xml:base="https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-var-inf/"><![CDATA[<p>This post will talk about <strong>variational inference</strong>, which is a way of approximating complex distributions through Bayesian inference. We will go from talking about latent variable models all the way to amortized variational inference! If you missed the previous post, which was about automatic task construction for unsupervised meta learning, you can head over <a href="/blog/2024/cs330-stanford-upt-rbm/">here</a> to view it.</p> <p>The link to the lecture slides can be found <a href="https://cs330.stanford.edu/materials/cs330_variational_inference_2023.pdf">here</a>.</p> <p>This lecture is taught in order to be able to discuss <strong>Bayesian meta learning</strong> in the next part of the series. However, it is a bit different from the rest of the content, so feel free to skip it if you’re already comfortable with this topic!</p> <h1 id="probabilistic-models">Probabilistic models</h1> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/9/simple_model.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Simple example of a probabilistic model.</figcaption> </figure> <p>Machine learning is all about probabilistic models! In supervised learning, we try to learn a distribution $p(y\vert x)$ over a target variable $y$ using data from $p(x)$. This conditional distribution depends on the assumptions that you make about this target variable $y$. For example, in classification, you might treat $y$ as a categorical variable, which means that $p(y\vert x)$ comes from a discrete <strong>categorical distribution</strong>. However, we also often assume that $p(y\vert x)$ comes from a <strong>Gaussian distribution</strong>. Note that instead of outputting a single value for $y$, out model predicts the distribution $p(y\vert x)$.</p> <p>The previous two examples are <em>very common</em>, but simple, distributions. For some problems, more complex distributions are necessary in order to formulate the problem effectively. As we will see later on, variational inference will allow us to find solutions for these complex distributions!</p> <p>First, let’s also very quickly discuss some terminology. Using Bayes’ rule, we have the following equation for a parameter $\theta$ and some evidence $X$:</p> \[p(\theta\vert X) = \frac{p(X\vert \theta)p(\theta)}{p(X)}\;.\] <p>In this equation,</p> <ol> <li>$p(\theta\vert X)$ is called the <em>posterior</em> distribution. It is the probability after the evidence $X$ is considered.</li> <li>$p(\theta)$ is called the <em>prior</em> distribution. It is the probability before the evidence $X$ is considered.</li> <li>$p(X\vert \theta)$ is called the <em>likelihood</em>. It is the probability of the evidence, given that $\theta$ is true.</li> <li>$p(X)$ is called the <em>marginal</em>. It is the probability of the evidence under any circumstance.</li> </ol> <p>The process of training probabilistic models comes from this idea of likelihood. Given that we observe some data $\mathcal{D} = \{x_1, x_2, \cdots, x_N\} \sim X$, we want to learn the data distribution $p(x)$. However, we will consider a parameterized form $p(x\vert \theta) = p_\theta(x)$. The goal becomes to maximize the likelihood of observing the samples in $\mathcal{D}$ given $\theta$:</p> \[\max_\theta p_\theta(x_1, x_2, \cdots, x_N) = \max_\theta \prod_i p_\theta(x_i)\;.\] <p>This assumes independence $x_i \perp x_j \in \mathcal{D}$. One more trick: Since the $\log$-function is a monotonically increasing function, we can rewrite this objective function without changing the optimal parameters $\theta^*$:</p> \[\theta^* \leftarrow \arg\max_\theta \frac{1}{N} \sum_i\log p_\theta(x_i)\;.\] <p>This will help a lot, since we got rid of the long chain of multiplications, which could be catastrophic for gradient-based optimization methods. This method is fundamental to statistics, and is called <strong>maximum likelihood estimation.</strong></p> <p>For simple distributions, such as the categorical and Gaussian distributions that we saw, there are closed-form evaluations of this function. The maximum likelihood estimate of the categorical distribution results in the <strong>cross-entropy</strong> loss, and the one for the Gaussian distributions is the <strong>mean-squared error</strong> loss.</p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/9/diffusion-example.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Example of a text-to-image model.</figcaption> </figure> <p>For some problems, assuming the data comes from these distributions is just too simple. For example, <strong>generative models</strong> over images, text, video, or other data may need a more complex distribution. An example of a text-to-video use-case is depicted above <d-cite key="villegas2022phenaki"></d-cite>. For this, a Gaussian distribution might just be too simple. Another example is the class of problems that require a <strong>multimodal</strong> distribution.</p> <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/9/problem-meta.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Ambiguity in meta learning problems.</figcaption> </figure> <p>For meta learning, we are so far using a deterministic (i.e. point estimate) of the distribution $p(\phi_i\vert \mathcal{D}^\mathrm{tr}_i, \theta)$. This could be a problem when few-shot learning problems are <b>ambiguous</b>. One example is depicted in the figure on the right. Depending on the representation that the model learns, a point estimate might either learn to distinguish samples based on their <i>youth</i> or whether they are <i>smiling</i>. The goal is ambiguous from the training dataset on the left. Therefore, it would be nice to learn to <b>generate hypotheses</b> by sampling from $p(\phi_i\vert \mathcal{D}^\mathrm{tr}_i, \theta)$. This can be important for <i>safety-critical</i> few-shot learning, learning to <i>active learn</i> <d-cite key="woodward2017active"></d-cite>, and <i>learning how to explore</i> in meta reinforcement learning.</p> </div> <p>The main question of this lecture: <em>Can we model and train more complex distributions?</em> We will use variational inference to answer this question!</p> <h2 id="latent-variable-models">Latent variable models</h2> <div> <figure class="figure col-sm-5 float-right"> <img src="/assets/img/blog/cs330/9/gmm.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Example of a fitted Gaussian Mixture Model.</figcaption> </figure> <p>Before we get into variational inference, we will talk about what <b>latent variable</b> models are. We will start by using a few examples and building out the idea. Let’s say we are given the data from $p(x)$ in the right figure. As you can see, fitting a Gaussian distribution would not work very well in this case.</p> <p>One common method to model these “clustered” points is by using a <b>Gaussian mixture model</b>. The distribution of such a model follows the following formula:</p> </div> \[p(x) = \sum_zp(x\vert z)p(z)\;.\] <p>In this distribution, we introduce latent (hidden) variables $z$. In this example, we let $p(x\vert z)$ be a normal distribution, and $p(z)$ be a discrete categorical distribution. Notice that in this case, the latent variables model the clusters that datapoints belong to, and the conditional distribution $p(x\vert z)$ treats each individual cluster as a normal distribution. Since $z$ is a distribution, a datapoint can be part of a <em>mixture</em> of those gaussian distributions, hence the name of the model.</p> <p>Furthermore, this is also possible for conditional distributions, i.e.:</p> \[p(y \vert x) = \sum_z p(y\vert x,z)p(z \vert x)\;.\] <div> <figure class="figure col-sm-4 float-right"> <img src="/assets/img/blog/cs330/9/mixture-density-network.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Mixture Density Network in practice.</figcaption> </figure> <p>This has the name <b>mixture density network</b>. An example of such a network is shown on the right. Notice that the model outputs the parameters of the distributions instead of a direct value of $y$, which is the length of the paper in this case.</p> <p>Now that we have seen some examples, let’s generalize it to continuous distributions. Let's observe the equation below:</p> <p>$$ p(x) = \int p(x\vert z)p(z)dz\;. $$</p> </div> <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/9/latent-model.png" class="img-fluid" alt="Alt text."/> </figure> <p>The core idea stays the same: <b>represent a complex distribution by composing two simple distributions</b>. More often than not, $p(x\vert z)$ and $p(z)$ will both be represented as normal distributions.</p> <p>As you can see in the right figure, we need to <b>sample</b> from $p(z)$ in order to get a sample from $p(x\vert z)$.</p> </div> <p>However, now, a few questions arise:</p> <ol> <li> <p>How can we generate a sample from $p(x)$ after the model is trained?</p> <div class="panel-group"> <div class="panel panel-default"> <div class="panel-heading"> <h4 class="panel-title"> <a data-toggle="collapse" href="#collapse1">Toggle answer.</a> </h4> </div> <div id="collapse1" class="panel-collapse collapse"> <div class="panel-body"><p><b>Answer</b>: As we said before, you need to sample a $z$ and then use it to compute $p(x \vert z)$, which you then sample from.</p></div> </div> </div> </div> </li> <li> <p>How do we evaluate the likelihood of a given sample $x_i$ (e.g. $p(x_i)$)?</p> <div class="panel-group"> <div class="panel panel-default"> <div class="panel-heading"> <h4 class="panel-title"> <a data-toggle="collapse" href="#collapse2">Toggle answer.</a> </h4> </div> <div id="collapse2" class="panel-collapse collapse"> <div class="panel-body"><p><b>Answer</b>: To compute $p(x_i)$, we need to sample <b>many</b> $z$ from the distribution $p(z)$ in order to get a good <i>approximation</i> of the integral that defines the distribution $p(x) = \int p(x\vert z)p(z)dz$.</p></div> </div> </div> </div> </li> </ol> <p>Now that we know how to evaluate and sample from latent variable models, let’s look into how we can train these models. Rewriting the maximum likelihood objective with the latent variable model, we obtain the objective function below:</p> \[\theta^* \leftarrow \arg\max_\theta \frac{1}{N} \sum_i\log \left( \int p_\theta(x_i\vert z)p(z)dz\right)\;.\] <p>In order to optimize this, we need to find the gradient of this objective. However, the integral in the logarithm is <em>intractable</em>, since it usually does not have a nice closed-form expression, in contrary to the simple distributions we have seen before. Approximating the integral by sampling from $p(z)$ is incredibly inefficient.</p> <p>There exist many papers that use latent variable models, and most of them have (slightly) different ways of training them:</p> <ul> <li>Generative adversarial networks (GANs) <d-cite key="goodfellow2020generative"></d-cite></li> <li>Variation autoencoders (VAEs) <d-cite key="kingma2013auto"></d-cite></li> <li>Normalizing flow models <d-cite key="kobyzev2020normalizing"></d-cite></li> <li>Diffusion models <d-cite key="ho2020denoising"></d-cite></li> </ul> <p>Note that autoregressive models do not use latent variables, and we model the target as a categorical distribution, which has the closed-form cross-entropy objective as maximum likelihood estimator.</p> <p>In this lecture, we will focus on methods that use <strong>variational inference</strong>. They have a number of benefits and are probably the most common methods to train latent variable models.</p> <h1 id="variational-inference">Variational inference</h1> <p>In this section, we will introduce variational inference, which is a way of formulating a lower bound on the log-likelihood objective. Furthermore, since we will be optimizing this lower bound, we will look into the <em>tightness</em> of the bound.</p> <p>We will look at an alternative formulation of the log-likelihood objective, which is called the <em>expected</em> log-likelihood:</p> \[\theta^* \leftarrow \arg\max_\theta \frac{1}{N} \sum_i \mathbb{E}_{z \sim p(z \vert x_i)}[\log p_\theta(x_i, z)]\;.\] <div> <figure class="figure col-sm-5 float-right"> <img src="/assets/img/blog/cs330/9/q-approx.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Approximation of $p(z\vert x_i)$.</figcaption> </figure> <p>It is very similar to what we have seen, but now we sample the latent variable with $p(z\vert x_i)$ to evaluate the logarithm of the joint distribution $p_\theta(x_i, z)$. The intuition behind this formula is that we can make an educated guess of $z$ by using $p(z\vert x_i)$ instead of doing random sampling from $p(z)$. In the figure on the right, this can be seen as mapping $x_i$ back to the latent distribution $p(z)$.</p> <p>However, there is a problem. Unfortunately, we do not have access to the distribution $p(z \vert x_i)$. Therefore, we will try to approximate this distribution with the <b>variational distribution</b> $q_i(z) := \mathcal{N}(\mu_i, \sigma_i)$. Note that this is just an estimate, and it will not perfectly model the distribution, but it will help with quickly optimizing the objective function, since we can find likely latent variables given the samples!</p> </div> <p>Let’s try to now bound $\log p(x_i)$ and introduce $q_i(z)$!</p> \[\begin{align*} \log p(x_i) &amp;= \log \int p(x_i \vert z)p(z) dz \\ &amp;= \log \int p(x_i \vert z)p(z) \frac{q_i(z)}{q_i(z)} dz \\ &amp;= \log \mathbb{E}_{z \sim q_i}\left[\frac{p(x_i \vert z)p(z)}{q_i(z)}\right] \\ &amp;\geq \mathbb{E}_{z \sim q_i}\left[\log\frac{p(x_i \vert z)p(z)}{q_i(z)}\right] \\ &amp;= \mathbb{E}_{z \sim q_i}\left[\log p(x_i \vert z) + \log p(z) - \log q_i(z)\right] \\ &amp;= \mathbb{E}_{z \sim q_i}\left[\log p(x_i \vert z) + \log p(z)\right] + \mathcal{H}(q_i(z))\;. \end{align*}\] <p>In the equation above, we just introduced $q_i(z)$ by adding the fraction $\frac{q_i(z)}{q_i(z)}$, since it equals $1$. Then, we simple rewrote it as an expectation over $q_i(z)$ instead of $p(z)$. This is much nicer than before, since we can actually compute this expectation instead of evaluating an integral. Then, we used Jensen’s inequality to get a lower bound on the objective. We finally did some simple algebra to simplify it. Note that $\mathcal{H}$ is the entropy function. This bound is called the <strong>evidence lower-bound (ELBO)</strong>.</p> <p>Let’s spend time to talk about the intuition behind this bound. Since it forms a lower-bound on the original objective, maximizing the <strong>ELBO</strong> will also maximize the towards the optimal value of original objective. However, there might be some gap, but we will discuss this later on.</p> <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/9/elbo-part1.png" class="img-fluid" alt="Alt text."/> </figure> <p>The term $\mathbb{E}_{z \sim q_i}\left[\log p(x_i \vert z) + \log p(z)\right]$ essentially tries to maximize the probability $p(x_i, z)$ for a given $z$. This is highlighted on the figure in the right.</p> </div> <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/9/elbo-part2.png" class="img-fluid" alt="Alt text."/> </figure> <p>The second term then tries to maximize the entropy $\mathcal{H}(q_i(z))$. Since the entropy is a measure of randomness (e.g. a high entropy corresponds to a high randomness), this term will try to make the fit as random as possible. you can see this as the yellow part in the figure on the right.</p> </div> <p>Hopefully this gives some intuition behind the objective!</p> <h2 id="kullbackleibler-divergence">Kullback–Leibler divergence</h2> <p>Let’s take a brief detour and talk about a divergence called the <strong>Kullback–Leibler (KL) divergence</strong>. It is a divergence between two distributions, and it can be denoted by the following equation:</p> \[\begin{align*} D_\mathrm{KL}(q \Vert p) &amp;= \mathbb{E}_{x \sim q}\left[\log \frac{q(x)}{p(x)}\right] \\ &amp;= -\mathbb{E}_{x \sim q}\left[\log p(x)\right] - \mathcal{H}(q(x))\;. \end{align*}\] <p>It can be seen as a <em>difference</em> between distributions. But, in the last line, you can see that it also measures how <em>small</em> the expected log probability of $p$ under distribution $q$, <em>minus</em> the entropy of $q$. We will again build some intuition on this.</p> <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/9/kl.png" class="img-fluid" alt="Alt text."/> </figure> <p>However, note that we are <b>minimizing</b> the KL-divergence, since we want the distributions to be as similar as possible. In this case, we are maximizing the log probability under the other distribution, and we are also maximizing the entropy, which will lead to a similar intuition as we saw previously on the <b>ELBO</b>.</p> </div> <h2 id="tightness-of-the-lower-bound">Tightness of the lower bound</h2> <p>Now that you have seen the similarities between the KL divergence and the ELBO objectives, let’s try to put them together. We will try to do this by rewriting the KL divergence and uncovering the ELBO objective function. Recall that the ELBO objective is</p> \[\mathcal{L}_i(p, q_i)= \mathbb{E}_{z \sim q_i}\left[\log p(x_i \vert z) + \log p(z)\right] + \mathcal{H}(q_i(z))\;.\] <p>Further recall that we approximated $q_i(z) \approx p(z\vert x_i)$ earlier in order to be able to sample $z$, since we do not have access to $p(z\vert x_i)$. Intuitively, it makes sense that we want $q_i(z)$ to be as close as possible to $p(z\vert x_i)$. Let’s now compute the KL divergence between these distributions to see how well $q_i(z)$ approximates.</p> \[\begin{align*} D_\mathrm{KL}(q_i(z) \Vert p(z\vert x_i)) &amp;= \mathbb{E}_{z \sim q_i(z)}\left[\log\frac{q_i(z)}{p(z\vert x_i)}\right] \\ &amp;= \mathbb{E}_{z \sim q_i(z)}\left[\log\frac{q_i(z)p(x_i)}{p(z, x_i)}\right] \\ &amp;= - \mathbb{E}_{z \sim q_i(z)}\left[\log p(z, x_i)\right] - \mathcal{H}(q_i) + \mathbb{E}_{z \sim q(z)}\left[\log p(x_i)\right] \\ &amp;= - \mathcal{L}_i(p, q_i) + \log p(x_i)\;. \end{align*}\] <p>From the first to the second line, we use that $p(z\vert x_i) = \frac{p(x_i, z)}{p(x_i)}$. Then, from the second to third line, we simplify using the rules of the logarithm and already substitute in the entropy. In the final line, we first use that $\log p(z, x_i) = \log p(x_i\vert z)p(z) = \log p(x_i) + \log(z)$, and we use the tower property ($X \perp Y \Rightarrow \mathbb{E}_Y[X] = X$). The two entropies $\mathcal{H}(q_i)$ also cancel out.</p> <p>Now, we can rewrite the equation to see the following final form:</p> \[\log p(x_i) = D_\mathrm{KL}(q_i(z) \Vert p(z\vert x_i)) + \mathcal{L}_i(p, q_i)\;.\] <p>We can finally see that when $D_\mathrm{KL}(q_i(z) \Vert p(z\vert x_i)) = 0$, the ELBO bound is tight! Thus, it depends on how well $q_i(z)$ approximates the actual conditional distribution $p(z\vert x_i)$.</p> <p>We obtain the final optimization objective for variational inference:</p> \[\max_{\theta, q_i} \frac{1}{N} \sum_i \mathcal{L}_i(p_\theta, q_i)\;.\] <p>The training process is as follows:</p> <ol> <li>Sample mini-batch of $x_1, \cdots, x_N$.</li> <li>Compute $\nabla_\theta \mathcal{L}_i$. <ol> <li>Sample $z_1, \cdots, z_m \sim q_i(x_i)$.</li> <li>Calculate $\nabla_\theta \frac{1}{m}\sum_j\left[\log p(x_i \vert z_j)\right]$.</li> </ol> </li> <li> <p>Update $q_i$ with respect to $\mathcal{L}_i$ (For example, if $q_i := \mathcal{N}(\mu_i, \sigma_i)$, then we get $\nabla_{\mu_i} \mathcal{L}_i$ and $\nabla_{\sigma_i} \mathcal{L}_i$).</p> </li> </ol> <h1 id="amortized-variational-inference">Amortized variational inference</h1> <p>Unfortunately, there is another problem. In this method, we have a $q_i$ for <strong>every datapoint</strong> $x_i$. This is not really feasible for problems with large datasets, since there will be $\vert \theta \vert + (\vert \mu_i \vert + \vert \sigma_i \vert) \times N$ parameters.</p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/9/amortized.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">The two models for amortized variational inference.</figcaption> </figure> <p>Instead of having a single $q_i$ per sample, we can <strong>train a network</strong> $q_\phi(z \vert x_i) \approx p(z \vert x_i)$! We will essentially obtain two networks. This model would output the necessary parameters $\mu_\phi(x_i)$ and $\sigma_\phi(x_i)$. This technique is called <strong>amortized variational inference</strong>.</p> <p>In this case, we will obtain the new training process as follows:</p> <ol> <li>Sample mini-batch of $x_i$.</li> <li>Calculate $\nabla_\theta \mathcal{L}(p_\theta(x_i \vert z), q_\phi(z \vert x_i))$: <ol> <li>Sample $z \sim q_\phi(z \vert x_i)$.</li> <li>$\nabla_\theta \mathcal{L} \approx \nabla_\theta \log p_\theta(x_i \vert z)$,</li> </ol> </li> <li>$\theta \leftarrow \theta + \nabla_\theta \mathcal{L}$.</li> <li>$\phi \leftarrow \phi + \nabla_\phi \mathcal{L}$.</li> </ol> <p>Now, we need to look more at $\nabla_\phi \mathcal{L} = \nabla_\phi \mathbb{E}_{z \sim q_\phi}\left[\log p_\theta(x_i \vert z) + \log p(z)\right] + \mathcal{H}(q_\phi(z))$. Let’s call $r(x_i, z) = \log p_\theta(x_i \vert z) + \log p(z)$. The question now becomes how do we calculate $\nabla_\phi \mathbb{E}_{z \sim q_i}\left[r(x_i, z)\right]$?</p> <p>Unfortunately, this is non-differentiable, as it depends on samples from $q_i$. Luckily there is a technique called the <strong>reparameterization trick</strong> for the normal distribution, which works as follows:</p> \[q_\phi(z\vert x)=\mathcal{N}(\mu(x), \sigma(x)) = \mu(x) + \epsilon \sigma(x)\;.\] <p>In the equation above, $\epsilon \sim \mathcal{N}(0, 1)$. We can now rewrite the gradient of the bottleneck as</p> \[\nabla_\phi \mathbb{E}_{z \sim q_i}\left[r(x_i, z)\right] = \nabla_\phi \mathbb{E}_{\epsilon \sim \mathcal{N}(0, 1)}\left[r(x_i, \mu(x_i) + \epsilon \sigma(x_i))\right]\;.\] <p>Since $\epsilon$ is independent of $\phi$, as you can see in the equations above, we can do backpropagation after applying this trick! However, we still need to sample $\epsilon_1, \cdots \epsilon_m$ in order to approximate the expectation. In practice, it seems that sampling once works well! This is likely the case because the normal distribution is quite centred around its mean, so one sample is often representative enough of an approximation.</p> <p>The benefits to this methods are that, even though the proofs might be a bit non-trivial, it is <strong>very easy to implement</strong>. Furthermore, is has <strong>low variance</strong>. Unfortunately though, the reparameterization trick only works with continuous (normal) latent variables. However, there are papers that address this, such as vector-quantized variational autoencoders <d-cite key="van2017neural"></d-cite>.</p> <h1 id="practical-examples">Practical examples</h1> <h2 id="variational-autoencoders">Variational autoencoders</h2> <p>We previous saw the following ELBO objective:</p> \[\mathcal{L}_i= \mathbb{E}_{z \sim q_\phi}\left[\log p_\theta(x_i \vert z) + \log p(z)\right] + \mathcal{H}(q_\phi(z))\;.\] <p>With some simply algebra, this can actually be rewritten into</p> \[\mathcal{L}_i = \mathbb{E}_{z \sim q_\phi}\left[\log p_\theta(x_i \vert z)\right] - D_\mathrm{KL}(q_\phi(z \vert x_i) \Vert p(z))\;.\] <p>In this case, for normal random variables, $D_\mathrm{KL}(q_\phi(z \vert x_i) \Vert p(z))$ has a convenient analytical form! Using the reparameterization trick and by sampling one $\epsilon$, the final objective can be written as</p> \[\max_{\theta, \phi} \frac{1}{N} \sum_i \log p_\theta(x_i \vert \mu_\phi(x_i) + \epsilon \sigma_\phi(x_i)) - D_\mathrm{KL}(q_\phi(z \vert x_i) \Vert p(z))\;.\] <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/9/autoencoder.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">The variational autoencoder architecture.</figcaption> </figure> <div> <figure class="figure col-sm-4 float-right"> <img src="/assets/img/blog/cs330/9/autoencoder-example.png" class="img-fluid" alt="Alt text."/> </figure> <p>This can very conveniently be expressed with the networks in the figure above. There is an encoder model $q_\phi$ which takes an input $x_i$ and compresses it into a latent space $z$, where noise is added to the latent variable. The original input is then “reconstructed” from the latent variable using $p_\theta(x_i\vert z)$. At inference time, you can generate similar samples to your input simply by sampling multiple $\epsilon$ and reconstructing them! This can also be seen in the image on the right. This was introduced in <d-cite key="kingma2013auto"></d-cite>.</p> </div> <h2 id="conditional-models">Conditional models</h2> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/9/conditional-autoencoder.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Conditional generation with autoencoders..</figcaption> </figure> <p>The idea in <d-cite key="razavi2019generating"></d-cite> stays very similar to variational autoencoders. But now, we will try to model the conditional distribution $p(y\vert x)$ instead of just $p(x)$. The loss stays almost identical, but we just condition on $x_i$:</p> \[\mathcal{L}_i= \mathbb{E}_{z \sim q_\phi}\left[\log p_\theta(y_i \vert x_i, z) + \log p(z \vert x_i)\right] + \mathcal{H}(q_\phi(z \vert x_i))\;.\] <p>Now, $x_i$ can represent image data or whatever is necessary for conditional generation!</p> <hr/>]]></content><author><name>Lars C.P.M. Quaedvlieg</name></author><category term="deep-multi-task-and-meta-learning"/><category term="course"/><summary type="html"><![CDATA[This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. This post will talk about variational inference, which is a way of approximating complex distributions through Bayesian inference. We will go from talking about latent variable models all the way to amortized variational inference!]]></summary></entry><entry><title type="html">CS-330 Lecture 7: Unsupervised Pre-Training: Reconstruction-Based Methods</title><link href="https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-upt-rbm/" rel="alternate" type="text/html" title="CS-330 Lecture 7: Unsupervised Pre-Training: Reconstruction-Based Methods"/><published>2024-03-19T00:00:00+00:00</published><updated>2024-03-19T00:00:00+00:00</updated><id>https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-upt-rbm</id><content type="html" xml:base="https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-upt-rbm/"><![CDATA[<p>The goal of this post is to introduce to <em>widely-used</em> methods for <strong>unsupervised pre-training</strong>, which is essential in many fields nowadays, most notably in the development of foundation models. We also introduce methods that help with <strong>efficient fine-tuning</strong> of pre-trained models! If you missed the previous post, which was about unsupervised pre-training with contrastive learning, you can head over <a href="/blog/2024/cs330-stanford-upt-fsl-cl/">here</a> to view it.</p> <p>As always, since I am still quite new to this blogging thing, reach out to me if you have any feedback on my writing, the flow of information, or whatever! You can contact me through <a href="https://www.linkedin.com/in/lars-quaedvlieg/">LinkedIn</a>. ☺</p> <p>The link to the lecture slides can be found <a href="https://cs330.stanford.edu/materials/cs330_pretraining_reconstruction_2023.pdf">here</a>.</p> <p>Note: The lecture that I have based this post on is probably one of my favourite ones so far. Although we might not discuss the full details of every method, we will introduce a ton of cool things, and I am confident that you can learn a lot from it! In any case, I always reference corresponding papers, so feel free to check those out in addition to this blogpost!</p> <h1 id="quick-recap-and-introduction">Quick recap and introduction</h1> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/8/unsupervised-pretraining.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">The process of doing unsupervised pre-training for few-shot learning.</figcaption> </figure> <p>In the previous post, we introduced the idea of unsupervised pre-training for few-shot learning, as we also highlight in the figure above. Given an <em>unlabelled</em> dataset ${x_i}$, we do some form of unsupervised pre-training to learn a representation of the data. This way, it is easy to fine-tune the model on task-specific problems when we have labelled (for the sake of simplicity) samples.</p> <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/8/constrative-learning.png" class="img-fluid" alt="Alt text."/> </figure> <p>We already talked about <b>contrastive learning</b>, which comes from the idea that similar (positive) samples in a dataset should have similar representations, and differing (negative) ones should be different! After improving different approaches for a while, we introduced <b>SimCLR</b>, which tries to learn these representations by sampling a positive and many negative examples, somehow derived from the original dataset. This is also shown (on a very high level) in the figure on the right.</p> </div> <p>Unfortunately, the main drawback of this method was the large batch size or training time that is required to produce good models, which makes it less favourable for huge unsupervised datasets. We also talked about some newer methods that try to address these issues, but in this post, we will talk about another way to pre-train a model on unsupervised data: <strong>reconstruction-based methods</strong>. As you will see, one advantage of this method is that representations can be learned without explicitly comparing different samples to each other.</p> <p>The intuition behind reconstruction-based methods comes from the idea that a good representation of a sample should be sufficient to <strong>reconstruct</strong> it. In contrast with contrastive learning, this means that we do not need to work about things like sampling enough difficult negative samples and having large batch sizes.</p> <h1 id="autoencoders">Autoencoders</h1> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/8/autoencoder-basic.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">A first idea of an auto-encoder.</figcaption> </figure> <p>Let’s immediately try to think about what a reconstruction-based model could look like. Let’s say we have a model $\hat{x} =f_\theta(x)$, that tries to reconstruct its input. We split the model into two parts:</p> <ol> <li>The encoder, which is responsible for projecting the input, in this case an image, into an embedding $r$.</li> <li>The decoder, which takes the input embedding $r$ and attempts to reconstruct the original sample $x$ from it. Its output is $\hat{x}$.</li> </ol> <p>If the encoder produces a “good” representation of the input with $r$, meaning that $r$ contains enough information to reconstruct $x$, then a reasonably-sized decoder should be able to produce a <strong>reconstruction</strong> $\hat{x}$ that is very close to the input $x$ in some metric space. As a simple loss function, we can then consider a distance measure, such as the $\ell_2$-distance $d(x, \hat{x}) = \Vert x - \hat{x} \Vert^2$.</p> <p>However, try to think about what happens if $r$ can be <em>anything</em>. Is this a good idea?</p> <div class="panel-group"> <div class="panel panel-default"> <div class="panel-heading"> <h4 class="panel-title"> <a data-toggle="collapse" href="#collapse1">Toggle answer.</a> </h4> </div> <div id="collapse1" class="panel-collapse collapse"> <div class="panel-body"><p><b>Answer</b>: No! It might be obvious, but if $r$ can be anything, then we can let $r = x$. In this case, one optimal solution to the encoder and decoder would be to just let $\theta$ be the identity function, since the reconstruction will be perfect.</p></div> </div> </div> </div> <div> <figure class="figure col-sm-4 float-right"> <img src="/assets/img/blog/cs330/8/bottleneck-autoencoder.png" class="img-fluid" alt="Alt text."/> </figure> <p>Instead, we need to ensure that $r$, the encoder output, is a useful, <b>lower-dimensional</b> representation of the input sample $x$. This is done very easily by letting the encoder project the input onto a <b>compact latent representation</b> <d-footnote>With latent representation, we mean that the representation $r$ is an unobserved statistic (e.g. we do not directly observe it) of the input image</d-footnote>. The hope is then that the latent dimensions are forced to represent <b>high-level</b> concepts that <b>generalize</b> to other tasks by filtering out sample-specific noise and keeping track of the useful structure between samples.</p> </div> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/8/fine-tuning-autoencoder.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Procedure of fine-tuning with an auto-encoder for few-shot learning.</figcaption> </figure> <p>In order to do few-shot learning on a trained autoencoder, we only need the encoder. We first project out input sample into the compact latent variable $r$. Then, we can simply add a prediction head that takes this input and maps it to the necessary task-specific output space. This is identical to how we use the representations that we saw in the contrastive learning post. Usually, the encoder is <em>frozen</em> (i.e. its weights are not updated during fine-tuning), and only the prediction head is fine-tuned on the few-shot data.</p> <p>This approach is very simply and expressive, the only choice that we have is the distance metric $d(x, \hat{x})$, and there is no need to select positive and negative pairs! However, we need to design some way to <strong>bottleneck</strong> the model, and in practice, the model generally does not give very good few-shot performance.</p> <p>This lack of few-shot performance mainly comes from the fact that high-level generalizable features are still not really obtained, even when training a compact model. In reality, the models often just try to learn a <strong>hash</strong> of $x$ rather than a <strong>conceptual summary</strong>, so the reconstruction loss is still low but it is not useful for few-shot fine-tuning.</p> <p>There are many existing strategies that try to approach this issue. They encourage the encoder to extract high-level features in the following ways:</p> <ul> <li><strong>Information</strong> bottlenecks: Adding noise to force the model to learn features invariant to the noise. <d-cite key="kingma2013auto"></d-cite></li> <li><strong>Sparsity</strong> bottlenecks: The representation should have zeros in most dimensions, to encourage dimensions to contain useful information. <d-cite key="van2017neural"></d-cite></li> <li><strong>Capacity</strong> bottlenecks: The decoder is limited in capabilities so that the encoder is forced to create useful representations.</li> </ul> <h1 id="masked-autoencoders">Masked autoencoders</h1> <p>Whilst a lot of research has gone, and is still going into designing different bottlenecks, we nowadays stop worrying about designing these bottlenecks and make the problem more difficult to solve. However, if the model is able to solve this problem, we are sure that it <strong>must</strong> have learned a useful representation of the data.</p> <p>This harder problem is addressed by a class of models that are referred to as “<strong>masked autoencoder</strong>”. This term encompasses many of the foundation models that are used in practice nowadays. In this post, we fill focus on two fundamental models: <strong>BERT</strong> and <strong>MAE</strong>, but there are many other models that exist nowadays.</p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/8/masked-autoencoder.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Example of an image input and reconstruction of a masked auto-encoder.</figcaption> </figure> <p>Let’s first talk about this “harder problem”. With regular autoencoders, we bottleneck $r$ to avoid <strong>totally degenerate</strong> solutions (i.e. convergence to the identity function). But what if the task is just “too easy”, and it only admits to unhelpful solutions? In this case, we can try to <strong>mask</strong> a part of the input (and/or output) sample, in order to encourage the model to learn more meaningful features. This solves a more difficult learning task, since the model now has to reconstruct the masked part of the sample with <em>less to no information</em> about it. The general recipe for <strong>pre-training masked autoencoders</strong> is as follows:</p> <ol> <li>Choose a distance metric $d(\cdot, \cdot) \rightarrow \mathbb{R}$ as a loss function.</li> <li>Sample $\tilde{x}_i, y_i \sim \mathrm{mask}(x_i)$, where $\tilde{x}_i, y_i$ are two <strong>disjoint</strong> sub-regions of $x_i$ (or instead, sometimes, $y_i = x_i$).</li> <li> <p>Make prediction $\hat{y}_i = f_\theta(\tilde{x}_i)$.</p> </li> <li>Compute the loss $\mathcal{L}_i= d(y_i, \hat{y}_i)$.</li> </ol> <p>You might wonder how we parameterize $f_\theta$ in this case? While this can depend on the problem, in practice, <strong>Transformers</strong> are nowadays almost exclusively used!</p> <h2 id="bidirectional-encoder-representations-from-transformers-bert">Bidirectional Encoder Representations from Transformers (BERT)</h2> <div> <figure class="figure col-sm-5 float-right"> <img src="/assets/img/blog/cs330/8/BERT.png" class="img-fluid" alt="Alt text."/> </figure> <p>BERT <d-cite key="devlin2018bert"></d-cite> is probably the most famous example of a masked autoencoder for language. It takes a string, which can be multiple sentences as input, and randomly replaces words within this string with special <code>&lt;mask&gt;</code>. The goal of the model is then to reconstruct the masked words given the context, which is the rest of the unmasked sentence. The model itself consists of a bidirectional Transformer, meaning that the mask tokens can <b>attend to any other token</b> in the sequence <d-footnote>I am using the term token here. A token represents the smallest unit of data that you are using, such as a word in text, a pixel (or image patch) in images, or a frame in videos, allowing the a Transformer to process different types of input uniformly.</d-footnote>. Tokenization facilitates the model's understanding and generation of complex, multimodal outputs by analysing patterns across varied data forms.). This is very important, since it means that this method is <b>not</b> autoregressive (e.g. it can look into the “future” of a sentence.</p> </div> <p>The following is an example of how BERT training works with a given input sentence:</p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/8/BERT-masking.png" class="img-fluid" alt="Alt text."/> </figure> <ol> <li>Given the input sentence $x$, we create the masked sentence $\tilde{x}$ that masks the word tokens $y_2, y_6$, and $y_9$ (<em>Biden</em>, <em>president</em>, and <em>was</em>, respectively).</li> <li>We then use the BERT model to produce $\hat{y} = p_\theta(\tilde{x})$. So, for all tokens in the input sentence $\tilde{x}$, BERT outputs a probability distribution.</li> <li> <p>Finally, we use the probabilities over the masked input tokens to compute the loss. In this case, we use <strong>KL-divergence</strong> as a loss function (this can be replaced though by other losses as well though). The loss becomes</p> \[d(y, \hat{y})=\sum_j \mathrm{KL}(y_j \Vert \hat{y}_j) = - \sum_{i \in \{2, 6, 9\}} \log(p_\theta(y_i \vert \tilde{x}))\;.\] </li> </ol> <p>There are also some decisions that BERT makes on the masking. At any time, it selects $15$% tokens from the inputs. Then, $80$% of the time, the input is replaced by a masking token. The other $20$% of the time, the input token is instead replaced by a completely <strong>random token</strong>. However, this can also still be improved, by for example masking <strong>longer</strong> spans of text or selecting <strong>information-dense</strong> spans of text. The specific masking procedure can be vital for good generalization capabilities of the model!</p> <h2 id="masked-autoencoders-for-vision-mae">Masked autoencoders for vision (MAE)</h2> <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/8/mae.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">The masking procedure of MAE.</figcaption> </figure> <p>For vision, a similar model called MAE <d-cite key="he2022masked"></d-cite> exists. It is essentially BERT for vision. It starts by splitting an input image $x$ into multiple patches, which is commonly done to <b>tokenise</b> images before putting them into a transformer. Every patch represents a token. Then, tokens (patches) are randomly masked, just as in BERT, and fed to an encoder. The decoder must then reconstruct the masked patches in the image. There are a few differences with BERT:</p> </div> <ol> <li>Instead of words, we have a <strong>sequence</strong> of image patches.</li> <li>We mask ~$75$% of image patches.</li> <li>We compute representations of <strong>only</strong> the unmasked patches.</li> <li>We insert placeholder patches at the masked locations.</li> <li>We decode the encoded representation back into the original image.</li> </ol> <p>We can fine-tune this model by using the encoded representation of step 2 in the figure above.</p> <p>It is very cool to see that MAEs give <strong>state-of-the-art few-shot image classification performance</strong> among models that are trained using unsupervised pre-training.</p> <div> <figure class="figure col-sm-6 float-left"> <img src="/assets/img/blog/cs330/8/mae-res1.png" class="img-fluid" alt="Alt text."/> </figure> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/8/mae-res2.png" class="img-fluid" alt="Alt text."/> </figure> </div> <p>From the figures above you can observe the following: The unsupervised masked autoencoding recipe works better than pre-training <strong>with labels</strong> on <strong>the same</strong> data! Moreover, when <strong>fine-tuning</strong> the full model (not just <strong>linear probing</strong> <d-footnote>Linear problem is a term that described adding just one linear head on top of the existing model.</d-footnote> on frozen pretrained model), it performs better than (momentum-based) <strong>contrastive learning</strong>!</p> <h1 id="transformers-and-efficient-fine-tuning">Transformers and efficient fine-tuning</h1> <p>We have now seem a glimpse of what Transformer <d-cite key="vaswani2017attention"></d-cite> models can achieve together with a masked autoencoder scheme! If you are unfamiliar with Transformers, we will give a quick overview of the architecture. If you already know about Transformers, you might want to stick around when we talk about efficient fine-tuning later on!</p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/8/transformer.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Source: <a href="https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html">https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html</a>.</figcaption> </figure> <p>For a detailed look into Transformers, I can recommend reading the “<a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>” blog. However, let’s quickly discuss the <strong>encoder</strong> <strong>architecture</strong> from the figure above step-by-step (please ignore the decoder in the figure):</p> <ol> <li>Initially, we have inputs, which are quantified as <strong>tokens</strong>. Some examples of tokens: <ul> <li><strong>Text</strong>: Tokens could be words</li> <li><strong>Images</strong>: Tokens could be patches.</li> <li><strong>Reinforcement Learning:</strong> Tokens could represents the states, actions, rewards, and terminal indicators.</li> </ul> </li> <li>We then put these tokens through their corresponding embedding layers. These layers are modality-specific. This means that if we have image tokens, we probably want to embed them differently (i.e. by using a VQ-VAE or a CNN) than with text tokens (with a lookup table).</li> <li>Since the attention mechanism that we will explain is <strong>permutation invariant</strong>, meaning that its output does not depend on an order of tokens, we will need to add some <strong>positional encoding</strong> to it for sequence-based problems such as language. Do check out <a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> to learn more about positional encodings.</li> <li> <p>We now pass the embedded tokens with positional embeddings through a <strong>multi-head self-attention</strong> mechanism. This mechanism makes tokens “look at each other” to determine how much attention to pay to the other tokens. Let’s get into the formula of self-attention:</p> \[\mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d}})V\;.\] <p>Here, $Q = XW^Q \in \mathbb{R}^{L \times d}$ are the <strong>query</strong> vectors, where $L$ is the number of tokens in the sequence and $d$ is the hidden size of the model. Moreover, $K = XW^K \in \mathbb{R}^{L \times d}$ are the <strong>key</strong> vectors, and finally, $V = XW^V \in \mathbb{R}^{L \times d}$ represent the <strong>values.</strong> We also have that $W^Q, W^K, W^V$ are the learnable weights. In this example, we let the hidden sizes be equal, but this does not necessarily have to be true.</p> <p>Let’s go through this formula step-by-step. The intuition is as follows:</p> <ol> <li>We first compute $Q$, $K$, and $V$. These are projections of the input $X$. The query semantically represents the elements we want to draw comparisons against, the key corresponds to elements we compare the query to, and the value represents the content that we actually want to retrieve or focus on based on these comparisons. Essentially, the mechanism computes the relevance of each key to a given query to determine how much attention to pay to each “value”, enabling the model to dynamically focus on important parts of the input data.</li> <li>We then compute $\mathrm{softmax}(\frac{QK^T}{\sqrt{d}}) \in \mathbb{R}^{L \times L}$. For every token in the row, it essentially computes a softmax-distribution over all the other the other tokens. This can be interpreted as the “amount” of attention to pay to that other token.</li> <li>Finally, we multiply the previous output by $V$, letting the result be of shape $\mathbb{R}^{L\times d}$. This multiplied the probability to pay attention to each tokens to that token’s corresponding value, creating a weighted sum of values. This represents the output of the self-attention, and the new embedding of the tokens!</li> </ol> </li> <li>Now that we have the new token embeddings from self-attention, we do layer normalization and put them through a fully-connected layer. At this point, you could do something like <strong>average or max pooling over all tokens</strong> and attach a head to the resulting vector. You can then fine-tune that head for few-shot learning on that representation!</li> </ol> <p>I hope this short overview of the encoder in Transformers was at least a bit helpful! I know it can be a lot if you haven’t seen it before, so if you’re struggling that’s completely understandable! In that case, I recommend you to check out more comprehensive and intuitive blogposts.</p> <p>For <strong>autoregressive generation</strong> in a Transformer <em>decoder</em>, you can also something very similar. The “main” difference is to do mask future tokens in the attention so that your attention mechanism isn’t look at future tokens. You can easily do this by manually setting the attention score before doing the softmax operation to $-\infty$ for those future tokens.</p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/8/vit.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Architecture of the Vision Transformer model.</figcaption> </figure> <p>This idea can easily be extended to image-based tokens, which was introduced in the Vision Transformer (ViT) paper <d-cite key="vaswani2017attention"></d-cite> There are some subtle differences with the original Transformer, especially in the encoding and positional embeddings, but the idea stays the same:</p> <ol> <li>Tokenize and embed your input.</li> <li>Feed it through a Transformer encoder to get a representation for each token.</li> <li>Perform pooling or something similar to get a vector representation. It is also possible to prepend a special token (i.e. <code class="language-plaintext highlighter-rouge">[CLS]</code> in BERT) to use as a final vector representation. The model should learn to put the useful information into the embedding of that special token.</li> <li>Put a head on that final representation to perform fine-tuning for few-shot learning!</li> </ol> <h2 id="low-rank-adaptation-of-language-models-lora">Low-rank adaptation of language models (LoRA)</h2> <p>Now that we know how to set up the Transformer encoder, we should ask ourselves how to fine-tune a pre-trained model. There are so many possible options, which are critical to the performance of our final model:</p> <ul> <li><strong>Freeze</strong> the pre-trained backbone.</li> <li><strong>Fine-tune</strong> everything.</li> <li>Fine-tune <strong>some</strong> parameters?</li> <li>Freeze the pre-trained backbone and inject <strong>new</strong> parameters?</li> </ul> <p>In this section, we will focus on LoRA <d-cite key="hu2021lora"></d-cite>, which is very commonly used in practice to this day (March 2024). The key idea is that we wish to fine-tune our model just <strong>a little bit</strong>, so that we do not get rid of the useful knowledge that our pre-trained model has. For <strong>Large Language Models</strong> (LLMs), we also want to avoid the need to store a <strong>new version</strong> of <strong>every single</strong> parameter in the model.</p> <p>In order to get an intuition of this idea, we go back to the <strong>associative memory</strong> view of the linear transformation. The linear transformation $W$ can be decomposed into $W = \sum_r v_ru_r^T$ for an r-rank matrix $W$ (with orthogonal $u_r$ by singular value decomposition). For this reason, we show the following:</p> \[Wx = \left(\sum_r v_ru_r^T\right)x = \sum_r v_r(u_r^T x)\;.\] <p>From this decomposition, it can be interpreted that $Wx$ produces a sum over <strong>memories</strong> in $v_r$, which are weighted by the memory <strong>relevance</strong> $u_r^T x$. Here, each $u_r^T$ is a <strong>key</strong>.</p> <p>If we wish to only change the model <strong>a little bit</strong>, as we previously described, we can try to only make a <strong>low-rank change</strong> to $W$. With LoRA, you compute the new weights as follows:</p> \[W_\mathrm{ft} = W_0 + AB^T\;.\] <p>Here, $W_\mathrm{ft} \in \mathbb{R}^{d \times d}$ are the fine-tuned parameters, $W_0 \in \mathbb{R}^{d \times d}$ are the initial parameters, and $AB^T$ is a <strong>new low-rank residual (fine-tuned)</strong>. Note that $A,B \in \mathbb{R}^{d \times p}$. It should thus be added to the old parameters. In practice, you initialize both $AB^T$ to zeros, since it is easier to fine-tune the model from the point where the model weights are $W_\mathrm{ft} = W_0 + 0 = W_0\;.$ Since you do not get any gradient if you set $A=B=0$, you can initialize only one to zeros and the other randomly.</p> <p>With LoRA, you only need to store $2\cdot d\cdot p$ new parameters instead of the $2\cdot d^2$ of a completely new model.</p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/8/efficient-tuning.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Other ways of performing parameter-efficient fine-tuning.</figcaption> </figure> <p>There are many more ways of “lightweight” fine-tuning models, which are evaluated in the <strong>T-Few</strong> paper <d-cite key="tunstall2022efficient"></d-cite>. If you are interested, I encourage you to go through it! They also show that lightweight fine-tuning can be better for few-shot learning than in-context learning with models of $10$ to $100$ times bigger on the experiments that they performed!</p> <h1 id="autoregressive-models">Autoregressive models</h1> <p>There are some downsides to masked autoencoders. For example, you need to pick the <code class="language-plaintext highlighter-rouge">mask</code> to apply to the inputs, you are only using ~$15$% of the examples for training, and it is difficult to sample from.</p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/8/autoregressive.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Masking and next-token generation for autoregressive models.</figcaption> </figure> <p>The idea of autoregressive models is very simple. What if we just <strong>predict the next token</strong>? This way, you do not need to select a specific masking strategy, but you rather <strong>mask tokens that are in the future</strong> of a newly processed token. We show an example of this masking (denoted by the $-$) in the figure above. On the right side in this figure, you can see the model $p_\theta(x_t\vert x_{&lt;t})$ that tries to predict the next token given the past ones.</p> <p>Note that autoregressive models are just masked autoencoders with a specific masking function. There is also research that has been done into different masking schemes, with this paper <d-cite key="goyal2024think"></d-cite> being my favourite yet. They basically improve the memory capabilities of LLMs by including pause tokens.</p> <p>These models form the basis for almost every single foundation model that is currently out there. We will briefly look into a case study for a multimodal autoregressive model called <strong>Flamingo</strong>.</p> <h2 id="flamingo">Flamingo</h2> <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/8/flamingo_sum.png" class="img-fluid" alt="Alt text."/> </figure> <p>This paper <d-cite key="alayrac2022flamingo"></d-cite> shows that building a multimodal autoregressive model <b>from scratch</b> is a bad idea. Instead, they propose the idea of fine-tuning pre-trained models together with multimodal data to <b>combine</b> these models into a multimodal model.</p> </div> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/8/flaming-arch.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Masking and next-token generation for autoregressive models.</figcaption> </figure> <p>The model architecture processes <strong>interleaved visual and textual data</strong> using a series of Vision Encoders, Perceiver Resamplers, <code class="language-plaintext highlighter-rouge">GATED XATTN-DENSE</code> blocks, and LM blocks to produce text output. The Vision Encoders, which are pretrained and frozen, transform images into a compatible representation, while the Perceiver Resamplers turns this <strong>spatiotemporal representation</strong> into a <strong>fixed-sized set of visual tokens</strong>. The model then integrates this visual information with text-based inputs using the <code class="language-plaintext highlighter-rouge">GATED XATTN-DENSE</code> blocks that enable cross-modality attention and interaction, complemented by LM blocks tailored for text understanding. This architecture allows Flamingo to generate text outputs that reflect a combined understanding of both the visual context provided by images and the semantics of the accompanying text.</p> <div> <figure class="figure col-sm-7 float-right"> <img src="/assets/img/blog/cs330/8/flamingo-res.png" class="img-fluid" alt="Alt text."/> </figure> <p>The cool thing is that you can now do in-context few-shot learning on sequences that <b>freely mix text and images</b>! This enables few-shot captioning, visual question-answering, etc. They also show that few-shot Flamingo performs approximately <b>as well as non-few-shot state-of-the-art models</b> (fine-tuned on the whole training set)!</p> </div> <hr/>]]></content><author><name>Lars C.P.M. Quaedvlieg</name></author><category term="deep-multi-task-and-meta-learning"/><category term="course"/><summary type="html"><![CDATA[This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this post is to introduce to widely-used methods for unsupervised pre-training, which is essential in many fields nowadays, most notably in the development of foundation models. We also introduce methods that help with efficient fine-tuning of pre-trained models!]]></summary></entry><entry><title type="html">CS-330 Lecture 6: Unsupervised Pre-Training: Contrastive Learning</title><link href="https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-upt-fsl-cl/" rel="alternate" type="text/html" title="CS-330 Lecture 6: Unsupervised Pre-Training: Contrastive Learning"/><published>2024-03-16T00:00:00+00:00</published><updated>2024-03-16T00:00:00+00:00</updated><id>https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-upt-fsl-cl</id><content type="html" xml:base="https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-upt-fsl-cl/"><![CDATA[<p>The goal of this lecture is to understand the intuition, design choices, and implementation of <strong>contrastive learning</strong> for unsupervised representation learning. We will also talk about the relationship between contrastive learning and meta learning! If you missed the previous lecture, which was about non-parametric few-shot learning, you can head over <a href="/blog/2024/cs330-stanford-fsl-ml/">here</a> to view it.</p> <p>As always, since I am still quite new to this blogging thing, reach out to me if you have any feedback on my writing, the flow of information, or whatever! You can contact me through <a href="https://www.linkedin.com/in/lars-quaedvlieg/">LinkedIn</a>. ☺</p> <p>The link to the lecture slides can be found <a href="https://cs330.stanford.edu/materials/cs330_contrastive_2023.pdf">here</a>.</p> <h2 id="quick-introduction">Quick introduction</h2> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/7/fine-tune-example.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Example of the amount of data needed for transfer learning through fine-tuning.</figcaption> </figure> <p>So far we have talked about the idea of few-shot learning via meta learning. In this problem, you are given a set of tasks $\mathcal{T}_1, \cdots, \mathcal{T}_n$ to train on, and wish to solve a new task $\mathcal{T}_\mathrm{test}$ more quickly, effectively, and stably. Before starting with meta learning, we discussed the idea of using transfer learning via fine-tuning for this problem, but the performance of this method is very dependent on the amount of data, as you can see on in figure above. Instead, we proposed three different types of meta learning to help quickly adapt to new tasks: black-box meta learning, optimization-based meta learning, and non-parametric meta learning.</p> <p>These methods were shown to work especially well when there are <strong>many tasks available</strong> for a problem. But, when you only have few tasks, meta learning might not be a good approach to the problem due to risks of overfitting and having insufficient diversity in your data. Let’s take this even further. What if you only have <strong>one batch</strong> of <strong>unlabelled data</strong>?</p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/7/unsupervised-pretraining.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">The process of doing unsupervised pre-training for few-shot learning.</figcaption> </figure> <p>In this case, meta learning might not be a good approach to the problem. Instead, we will look into <strong>unsupervised representation learning for few-shot learning</strong>. In the figure above, we describe the process of training a model for this problem on a high level. Given a dataset of unlabelled data ${x_i}$, we want to do unsupervised pre-training to get an initial model. Once we have obtained this model, we then wish to fine-tune it on a task-specific dataset $\mathcal{D}_j^\mathrm{tr}$, to get a task-specific predictor.</p> <p>You might have already noticed that this procedure is very similar to the way that <strong>large language models</strong> are trained. They are first pre-trained on a huge corpus of language data, and then fine-tuned for specific purposes (i.e. alignment, mathematics, etc.).</p> <p>In this course, we will talk about two approaches to this problem:</p> <ol> <li>Contrastive learning.</li> <li>Reconstruction-based methods.</li> </ol> <p>In this post, we will focus on contrastive learning, and we will discuss the reconstruction-based methods in the next one!</p> <h2 id="contrastive-learning">Contrastive learning</h2> <div> <figure class="figure col-sm-3 float-right"> <img src="/assets/img/blog/cs330/7/similar-reps.png" class="img-fluid" alt="Alt text."/> </figure> <p>The idea behind contrastive learning is that <b>similar examples should have similar representations</b>, and different examples should have different representations. When you have a batch of unsupervised data, you can decide on a semantic meaning of similarity and then learn the data representations as embeddings from a model. The steps would roughly be as follows:</p> <ol> <li>Select or generate examples that are semantically similar.</li> <li>Train an encoder where similar examples are closer in the representation space than non-similar examples.</li> </ol> </div> <p>Let’s start out with a simple approach. We are trying to learn an model $f_\theta(x)$, which embeds a datapoint $x$ into some representation. As a loss function, we decide use the following:</p> \[\min_\theta \sum_{(x_i, x_j)}\Vert f_\theta(x_i) - f_\theta(x_j)\Vert^2\;.\] <div> <figure class="figure col-sm-5 float-right"> <img src="/assets/img/blog/cs330/7/basic-embed-space.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Abstract example of the embedding space.</figcaption> </figure> <p>This loss function tries to minimize the distance of the embeddings of <b>similar datapoints</b> $x_i$ <b>and</b> $x_j$. However, do you think this loss function performs well? Well, you might be able to see that one possible optimal solution to this loss function would just be to let $f_\theta(x) = 0$. This would mean that <b>all</b> datapoints are mapped to the same representation, even very different datapoints. For this reason, the loss should also incorporate an element to <b>push apart differing samples</b>. You need to both compare and contrast!</p> </div> <p>We present this idea in the figure on the right. In the embedding space, similar samples should be close, whilst differing samples should should be far apart. The key design choices here are choosing what to compare/contrast, and which contrastive loss you use.</p> <p>Whilst the ideas work for all kinds of unlabelled data, we will focus on images (or videos) in the remainder of this post. Recalling that similar examples should have similar representations, we discuss a few ways to measure similarity in images.</p> <p>The most straightforward way to assign similarity is by looking at class labels. This is very related to the Siamese networks and Prototypical networks that we saw in the previous post. However, for unsupervised data, this is not possible. Instead, there are many different approaches that <strong>create new samples from one sample</strong>. Below are some examples <d-cite key="oord2018representation"></d-cite><d-cite key="chen2020simple"></d-cite>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <p><b>Patch-based.</b></p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/7/patches.png" class="img-fluid" alt="Alt text."/> </figure> <p>Given an image, it is possible to split it into image patches, and to let image patches that are close to each other have a similar representation.</p> </div> <div class="col-sm mt-3 mt-md-0"> <p><b>Augmentation-based.</b></p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/7/augments.png" class="img-fluid" alt="Alt text."/> </figure> <p>Given an image, it is also possible to augment it in some way (i.e. by flipping, cropping, etc.), and letting those sample be similar to each other.</p> </div> <div class="col-sm mt-3 mt-md-0"> <p><b>Temporally-based.</b></p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/7/videos.png" class="img-fluid" alt="Alt text."/> </figure> <p>Given a video, it is <i>often</i> possible to let frames that are temporally close have a similar representation. Of course this depends on the nature of the video.</p> </div> </div> <p>As you can see, defining similarity is usually pretty problem-specific. A simple example in text would be something like <em>bag of words</em> depending on the task, or permutations with a similar semantic meaning.</p> <div> <figure class="figure col-sm-4 float-right"> <img src="/assets/img/blog/cs330/7/mid-embed-space.png" class="img-fluid" alt="Alt text."/> </figure> <p>Now that we have a way of defining similarity across samples, we can take a look at modifying the loss function to push apart differing samples. One common loss function is the <b>triplet loss</b>, introduced in <d-cite key="schroff2015facenet"></d-cite>, which simply tries to push away unrelated samples:</p> </div> \[\min_\theta \sum_{(x, x^+, x^-)}\max(0,\Vert f_\theta(x) - f_\theta(x^+)\Vert^2 - \Vert f_\theta(x) - f_\theta(x^-)\Vert^2 + \epsilon)\;.\] <p>If you only consider $l_\theta(x, x^+, x^-) = \Vert f_\theta(x) - f_\theta(x^+)\Vert^2 - \Vert f_\theta(x) - f_\theta(x^-)\Vert^2$, this loss function would be unbounded, since it can decrease indefinitely. By introducing $\max(0, \cdots + \epsilon)$, you ensure that the values of $l_\theta(x, x^+, x^-)$ that affect the loss are bounded up to some margin $-\epsilon$. This implicitly defines <strong>how far apart</strong> you want your samples to be when comparing related versus unrelated samples.</p> <p>This approach is <em>very</em> similar to Siamese networks, which classifies a pair $(x, x^\prime)$ as the same class if $\Vert f(x) - f(x^\prime)\Vert^2$ is small. The key difference is that contrastive learning <strong>learns a metric space</strong>, and not just a classifier.</p> <p>Unfortunately, the Triplet loss has a downside: In order for it to be effective, you need to find <em>difficult</em> negatively similar examples, which can be very challenging. It is important to find difficult negative samples, since very obviously different ones will already be far apart and have a zero loss, meaning the model is not going to be learning anything from that negative sample.</p> <div> <figure class="figure col-sm-5 float-right"> <img src="/assets/img/blog/cs330/7/adv-embed-space.png" class="img-fluid" alt="Alt text."/> </figure> <p>One approach to finding difficult negative samples is called <b>hard negative mining</b>. It essentially just looks through a list of negative samples and tries to see which ones are close your sample in the embedding space. This brings us to the idea of <b>sampling multiple negatives</b> in order to contrast with more difficult negative samples. This is depicted in the figure on the right.</p> </div> <p>The loss function then becomes an $N$-way classification problem, and it generalizes the triplet loss to using multiple negatives:</p> \[\mathcal{L}_\mathrm{N-way}(\theta) = -\sum_z \log \left[ \frac{\exp(-d(z, z^+))}{\sum_i\exp(-d(z, z_i^-)) + \exp(-d(z, z^+))} \right]\;.\] <p>Notice that the goal of this loss is to distinguish the similar sample from all of the negatives with some distance measure of your learned metric space $d(\cdot, \cdot)$, such as a Euclidean loss or negative cosine similarity.</p> <p>This approach was taken in <d-cite key="sohn2016improved"></d-cite> and <d-cite key="chen2020simple"></d-cite>, but in practice people often use a slight modification of this loss function, which is shown below:</p> \[\begin{align*} \mathcal{L}(\theta) &amp;= -\sum_z \log \left[ \frac{\exp(-d(z, z^+))}{\sum_i\exp(-d(z, z_i^-))} \right] \\ &amp;= \sum_z \left[ \exp(-d(z, z^+)) + \log\sum_i\exp(-d(z, z_i^-)) \right]\;. \end{align*}\] <p>This loss is usually preferred, since you really only want to push away negative examples, not the similar one as well. As you can see in the equivalent formula shown above, this is exactly what it is doing.</p> <h3 id="the-simclr-algorithm">The SimCLR algorithm</h3> <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/7/simclr.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Training process of the SimCLR algorithm.</figcaption> </figure> <p>We will now talk about a way of sampling negative examples so we can compute this loss. There is an algorithm called <b>SimCLR</b> which is proposed in <d-cite key="chen2020simple"></d-cite>, that does exactly this. We visualize this approach in the figure on the right. It is composed of the following steps:</p> <ol> <li>Sample a minibatch of examples $x_1, \cdots, x_N$.</li> <li><b>Augment</b> each example <i>twice (design choice)</i> to get $\tilde{x}_1, \cdots, \tilde{x}_N, \tilde{x}_{N+1}, \cdots, \tilde{x}_{2N}$.</li> <li><b>Embed</b> examples with $f_\theta$ to get $\tilde{z}_1, \cdots, \tilde{z}_N, \tilde{z}_{N+1}, \cdots, \tilde{z}_{2N}$.</li> <li>Compute all <b>pairwise distances</b> $d(z_i, z_j) = -\frac{z_i^Tz_j}{\Vert z_i\Vert\Vert z_j\Vert}$ (negative cosine similarity).</li> <li>Update <b>pairwise distances</b> $\theta$ with respect to $\mathcal{L}_\mathrm{N-way}(\theta)$.</li> </ol> </div> <div> <figure class="figure col-sm-5 float-left"> <img src="/assets/img/blog/cs330/7/simclr-perf.png" class="img-fluid" alt="Alt text."/> </figure> <figure class="figure col-sm-5 float-left"> <img src="/assets/img/blog/cs330/7/simclr-efficiency.png" class="img-fluid" alt="Alt text."/> </figure> </div> <p><b>After pre-training</b> the function $f_\theta$, we can either train a classifier on top of the representations that it produces, or choose to fine-tune the entire network. The performance of this method was benchmarked on ImageNet classification, where the model was fine-tuned using only $1$% of all labels (~$12.8$ images per class) or $10$% of all labels. The other part of the dataset was used as unsupervised pre-training data. It shows a substantial improvement over training from scratch, and also improvements over other methods, especially in the $1$% label setting.</p> <p>In their experiments, they did note that it was important to use a <b>large batch size</b> (larger than 256), since it leads to longer needed training (more than 600 epochs) in order to get a good performance.</p> <h3 id="theoretical-properties-of-contrastive-learning">Theoretical properties of contrastive learning</h3> <p>One reason that <strong>contrastive learning needs a large batch size</strong>, is that the summation over the entire dataset in the $\mathcal{L}_\mathrm{N-way}(\theta)$ loss function will dominate for very close samples. However, if your batch size is too small, you might not include those similar hard examples. This is related to the previous problem of subsampling hard negatives. We will show this mathematically below.</p> <p>We will rewrite the loss function using a minibatch $\mathcal{B}$ and find a lower bound using <strong>Jensen’s inequality</strong>:</p> \[\begin{align*} &amp; \exp(-d(z, z^+)) + \log\sum_n\exp(-d(z, z_n^-)) \\ \geq \: &amp;\exp(-d(z, z^+)) + \sum_{\mathcal{B}} \log\sum_{n \in \mathcal{B}}\exp(-d(z, z_n^-))\;. \end{align*}\] <p>This shows that our training objective that uses minibatches actually solves a <strong>lower bound on the original objective</strong>. This means that we might not actually be minimizing our original objective. However, the larger the batch size, the closer the lower bound gets to the original objective function. Can you see why? <d-footnote>Answer: We can express $\sum_n = \sum_\mathcal{B}\sum_{n\in\mathcal{B}}$. If you let you minibatch be the entire dataset, then we only have one minibatch, so the $\sum_{\mathcal{B}}$ in the lower bound will disappear and turn into objective function. However, in the worst case with a batch size of 1, we have only one value for $\sum_{n \in \mathcal{B}}$ and many sums over minibatches $\sum_{\mathcal{B}}$, meaning that the lower bound will be the further away from the initial objective. </d-footnote></p> <h3 id="recent-works-in-contrastive-learning">Recent works in contrastive learning</h3> <p>There are some papers that try to tackle the problem of optimizing this lower bound:</p> <ul> <li>One idea is to store representations from previous batches using a form of momentum during training. It’s not completely correct, but they show it obtains good results with a batch size of $256$ <d-cite key="he2020momentum"></d-cite>.</li> <li>It is also possible to predict representations of the same image under different augmentation. In this case, you do not require any negatively similar examples <d-cite key="grill2020bootstrap"></d-cite>. It’s more of a predictive approach, but does not have a nice contrastive representation.</li> </ul> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/7/imagenet.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Progress over the years on the ImageNet benchmark for self-supervised learning.</figcaption> </figure> <p>The image above shows results on the ImageNet benchmark over the past years, and contrastive methods (i.e. MoCo v3) are still close to <strong>state-of-the-art for self-supervised pre-training for visual data</strong>.</p> <p>In this post we have mainly focussed on augmentation-based methods. However, for many applications, we do not have well-engineered augmentations.</p> <ol> <li>A recent work <d-cite key="tamkin2020viewmaker"></d-cite> at ICLR in 2021 tries to <em>learn</em> the augmentations in an adversarial manner. It is competitive with SimCLR on image data and obtains good results on speech and sensor data.</li> <li> <p>Furthermore, <em>time-contrastive learning</em> on videos has been shown as effective for robotics pre-training, as presented in a paper <d-cite key="nair2022r3m"></d-cite> presented at CoRL in 2022. The method of this paper has been depicted in the figure below.</p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/7/time-constrastive-learning.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Process of the time-contrastive learning for robotics pre-training paper.</figcaption> </figure> </li> <li> <p>Finally, the popular CLIP paper <d-cite key="agarwal2021evaluating"></d-cite> uses <em>image-text</em> contrastive pre-training to produce robust zero-shot models. It learns a representation of images and a representation of text, and can tell which images and captions go together (positive samples), and which ones should be pushed apart (negative samples). It shows good zero-shot transfer to out-of-distribution tasks.</p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/7/clip.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Example of how CLIP works and the performance on out-of-distribution tasks.</figcaption> </figure> </li> </ol> <p>In summary, contrastive learning is a general and effective framework to do unsupervised pre-trained for few-shot adaptation. It does not require generative modelling and can incorporate domain knowledge through augmentations and similarity. However, it can be difficult to select negative samples, it often requires a large batch size for training, and is currently most successful with augmentations.</p> <h3 id="contrastive-learning-as-meta-learning">Contrastive learning as meta learning</h3> <p>Many of the equations that we saw in this post look similar to the ones that we saw in the previous post about non-parametric meta learning. It is actually possible to create a meta learning algorithm that works <em>similarly</em> to the contrastive approaches that we have seen today. Let’s formulate the problem as a meta learning problem:</p> <ol> <li>Given an unlabelled dataset ${x_i}$.</li> <li>Create a class $y_i$ from each datapoint via data augmentation $\mathcal{D}_i := \{\tilde{x}_i, \tilde{x}_i^\prime, \cdots\}.$</li> <li>Run any meta learning algorithm on this dataset.</li> </ol> <p>There is a paper that goes in depth into similarities of SimCLR with Prototypical networks for meta learning, and shows the methods differ in the following ways:</p> <ul> <li>SimCLR samples <em>one task</em> per minibatch, whereas meta learning usually samples multiple.</li> <li>SimCLR compares <em>all pairs</em> of samples, whereas meta learning compares query examples only to support examples and not to query other examples.</li> </ul> <p>In the table below, they also show that both representations transfer similarly well between different datasets.</p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/7/meta-learning-ref.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Performance of Prototypical networks and SimCLR on different unsupervised few-shot learning problems.</figcaption> </figure> <hr/>]]></content><author><name>Lars C.P.M. Quaedvlieg</name></author><category term="deep-multi-task-and-meta-learning"/><category term="course"/><summary type="html"><![CDATA[This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to understand the intuition, design choices, and implementation of contrastive learning for unsupervised representation learning. We will also talk about the relationship between contrastive learning and meta learning!]]></summary></entry><entry><title type="html">CS-330 Lecture 5: Few-Shot Learning via Metric Learning</title><link href="https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-fsl-ml/" rel="alternate" type="text/html" title="CS-330 Lecture 5: Few-Shot Learning via Metric Learning"/><published>2024-03-14T00:00:00+00:00</published><updated>2024-03-14T00:00:00+00:00</updated><id>https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-fsl-ml</id><content type="html" xml:base="https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-fsl-ml/"><![CDATA[<p>The goal of this lecture is to understand the third form of meta learning: <strong>non-parametric few-shot learning</strong>. We will also compare the three different methods of meta learning. Finally, we give practical examples of meta learning, in domains such as <strong>imitation learning</strong>, <strong>drug discovery</strong>, <strong>motion prediction</strong>, and <strong>language generation</strong>! If you missed the previous lecture, which was about optimization-based meta learning, you can head over <a href="/blog/2024/cs330-stanford-obml/">here</a> to view it.</p> <p>As always, since I am still new to this blogging thing, reach out to me if you have any feedback on my writing, the flow of information, or whatever! You can contact me through <a href="https://www.linkedin.com/in/lars-quaedvlieg/">LinkedIn</a>. ☺</p> <p>The link to the lecture slides can be found <a href="https://cs330.stanford.edu/materials/cs330_nonparametric_2023.pdf">here</a>.</p> <h2 id="quick-recap">Quick recap</h2> <p>So far, we have discussed two approaches to meta learning: black-box meta learning, and optimization-based meta learning.</p> <ol> <li> <p><a href="/blog/2024/cs330-stanford-bbml-icl/">Black-box meta learning</a>.</p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/6/bbml_recap.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Computation pipeline for black-box meta learning.</figcaption> </figure> <p>In <em>black-box meta learning</em>, we attempt to train some sort of meta-model to output task-specific parameters or contextual information, which can then be used by another model to solve that task. We saw that this method is <strong>very expressive</strong> (e.g. it can model many tasks). However, it also requires solving a <strong>challenging optimization problem</strong>, which is incredibly data-inefficient.</p> </li> <li> <p><a href="/blog/2024/cs330-stanford-obml/">Optimization-based meta learning</a>.</p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/6/obml_recap.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Computation pipeline for optimization-based meta learning.</figcaption> </figure> <p>We then talked about <em>optimization-based meta learning</em>, which embeds an optimization process within the inner learning process. This way, you can learn to find parameters to a model such that optimizing these parameters to specific tasks is as effective and efficient as possible. We saw that model-agnostic meta learning <strong>preserves expressiveness</strong> over tasks, but it remains <strong>memory-intensive</strong>, and requires solving a <strong>second-order optimization</strong> problem.</p> </li> </ol> <h2 id="non-parametric-few-shot-learning">Non-parametric few-shot learning</h2> <p>In the previous two approaches to meta learning, we only talked about parametric methods <d-footnote>A parametric model assumes a specific form for the underlying function between variables, using a finite number of parameters, while a non-parametric model makes fewer assumptions about the function form, potentially using an infinite number of parameters to model the data more flexibly.</d-footnote>. However, what if we can avoid the optimization process in the inner learning loop of optimization-based meta learning methods? If this is possible, we do not have to solve a second-order optimization problem anymore. For this reason, we will look into replacing the parametric models in the inner learning loop with <strong>non-parametric models</strong>, which don’t require to be optimized. Specifically, we will try to <strong>use parametric meta learners that produce non-parametric learners</strong>.</p> <div> <figure class="figure col-sm-5 float-right"> <img src="/assets/img/blog/cs330/6/parametric_example.png" class="img-fluid" alt="Alt text."/> </figure> <p>One benefit of non-parametric methods is that they generally work well in low data regimes, making it a great opportunity for few-shot learning problems at meta-test time. Nevertheless, during meta-training time, we would still like to use a parametric learner to exploit the potentially large amounts of data.</p> </div> <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/6/l2_loss_example.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Comparison between $\ell_2$-distances of two augmented images to the original.</figcaption> </figure> <p>The key idea behind non-parametric approaches is to compare the task-specific test data to the data in the train dataset. We will continue using the example of the few-shot image classification problem, as in the previous posts. If you want to compare images to each other, you need to come up with a certain <b>metric</b> to do so.</p> </div> <p>The simplest idea might be to utilize the $\ell_2$-distance. Unfortunately, it is not that simple. If you look at the figure above, you can see the image of a woman on the right and two augmented versions on the left. When you calculate the $\ell_2$-distance between the original image and the augmented image, the distance between the blurry image and the original one is smaller than the other distortion, even though this may resemble the original image more. For this problem, you could use a different metric, such as a <em>perceptual loss function</em> <d-cite key="johnson2016perceptual"></d-cite>, but in general, it might be worthwhile to learn the metric from the data.</p> <p>In this post, we will discuss three different ways of doing metric learning, starting with the easiest and building our way up. Firstly, we will talk about the most basic model: Siamese networks.</p> <h3 id="siamese-networks">Siamese networks</h3> <figure class="figure col-sm-12 float-right"> <img src="/assets/img/blog/cs330/6/siamese_network.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Example architecture of a Siamese network. It takes two images as an input and outputs a binary label whether they belong to the same class or not.</figcaption> </figure> <p>With a Siamese network, the goal is to learn whether two images belong to the same class or not. The input to the model is two images, and it outputs whether it thinks they belong to the same class. However, the penultimate activations correspond to a learned distance metric between these images. At meta-train time, you are simply trying to minimize the binary cross-entropy loss.</p> <p>At meta-test time, you need to compare the test image $x_\mathrm{test}$ against every image in the test-time training dataset $\mathcal{D}^\mathrm{tr}$, and then select the class of the image that has the highest probability. This corresponds to the equation below (for simplicity of the equation, we assume that only one sample will have $f_\theta(x_j^\mathrm{test}, x_k) &gt; 0.5$). Furthermore, $1$ corresponds to the indicator function.</p> \[\hat{y}_j^\mathrm{test} := \sum_{(x_k, y_k) \sim \mathcal{D}^\mathrm{tr}} 1(f_\theta(x_j^\mathrm{test}, x_k) &gt; 0.5)y_k\;.\] <p>With this method, there is a mismatch between meta-training and meta-testing. During meta-training, you are solving a binary classification problem, whilst during meta-testing, you are solving an $N$-way classification problem. You cannot phrase meta-training in the same way, since the indicator function $1$ makes $\hat{y}_j^\mathrm{test}$ non-differentiable. We will try to resolve this by introducing <strong>matching networks</strong>.</p> <h3 id="matching-networks">Matching networks</h3> <p>In the previous equation above, we saw that at meta-test time, we use the class of the most similar training example as the estimate of the test sample. In order to get rid of the mismatch between meta-training and meta-testing, we can rephrase the meta-testing objective similarly to what we saw. Let’s say we instead modify the procedure to use a mix of class predictions as a class estimate. This would result in the equation below.</p> \[\hat{y}_j^\mathrm{test} := \sum_{(x_k, y_k) \sim \mathcal{D}^\mathrm{tr}} f_\theta(x_j^\mathrm{test}, x_k)y_k\;.\] <p>At meta-train time, we can now use the same objective, and backpropagate through the cross-entropy loss $y_j^\mathrm{test} \log(\hat{y}_j^\mathrm{test}) + (1-y_j^\mathrm{test})\log(1-\hat{y}_j^\mathrm{test})$. This way, both meta-training and meta-testing are aligned with the same procedure. Our meta-training process would become:</p> <ol> <li>Sample task $\mathcal{T}_i$.</li> <li>Sample two images per class, giving $D_i^\mathrm{tr}, D_i^\mathrm{test}$.</li> <li> <p>Compute $\hat{y}^\mathrm{test} = \sum_{(x_k, y_k) \sim \mathcal{D}^\mathrm{tr}_i} f_\theta(x^\mathrm{test}, x_k)y_k$.</p> </li> <li>Backpropagate the loss with respect to $\theta$.</li> </ol> <figure class="figure col-sm-12 float-right"> <img src="/assets/img/blog/cs330/6/matching_network.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Example architecture of a matching network. It takes the entire training dataset as an input with a testing image and predicts the most likely class for the testing image given the training data.</figcaption> </figure> <p>This idea corresponds to so-called “matching networks” <d-cite key="vinyals2016matching"></d-cite>. Here, we embed each training image into some latent space using a bidirectional LSTM $g_\theta$. Then, we encode the test image using a shared convolutional encoder $h_\theta$ and perform the dot product between the latent training vectors and the latent test vector, resulting in $f_\theta(x^\mathrm{ts}, x_k)$. Finally, we take the dot products with the labels to obtain the prediction $\hat{y}^\mathrm{ts}.$ This way meta-training and meta-testing match, which resulted in a better performance than something like Siamese networks.</p> <p>Let’s stand still with what we’re doing for a second and think about how this approach is non-parametric. If we recall from parametric models, we would always compute task-specific parameters $\phi_i \leftarrow f_\theta(\mathcal{D}_i^\mathrm{tr})$. However, now have integrated the parameters $\phi$ out by computing $\hat{y}_j^\mathrm{test} := \sum_{(x_k, y_k) \sim \mathcal{D}^\mathrm{tr}} f_\theta(x_j^\mathrm{test}, x_k)y_k$ directly by comparing to the training dataset, making it non-parametric.</p> <div> <figure class="figure col-sm-4 float-right"> <img src="/assets/img/blog/cs330/6/matching_network_disadvantage.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Example of a disadvantage of a matching network.</figcaption> </figure> <p>In the meta-training procedure described, we would sample two images per class. But what would happen if we sampled more than two images (ignoring potential class imbalance)? Well, with matching networks, each sample of each class is evaluated independently with $f_\theta(x_j^\mathrm{test}, x_k)$ instead of together. This could lead to strange results if the majority of a class has a low confidence but there is an outlier with a high confidence, overpowering the correct label. This can be depicted in the right figure. Imagine if you want to predict the label of the black square. The dot-product score with the red sample might be so high that it overpowers the other samples, even though it is more likely part of the blue class. We will try to resolve this by calculating prototypical embeddings that average class information.</p> </div> <h3 id="prototypical-models">Prototypical models</h3> <div> <figure class="figure col-sm-5 float-right"> <img src="/assets/img/blog/cs330/6/proto_model_example.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Example of a prototype network, which uses aggregation over the embeddings of each class.</figcaption> </figure> <p>Prototypical models <d-cite key="snell2017prototypical"></d-cite> will work quite similarly to what we have previously seen, but try to aggregate class information in order to prevent outliers. The figure on the right depicts this. Formally, we introduce class prototypes $c_n = \frac{1}{K} \sum_{(x,y)\in\mathcal{D}_i^\mathrm{tr}} 1(y_k=n)f_\theta(x_k)$. After we compute these class-averaged embeddings, a model will try to estimate the class of the test point by using something like Softmax probability, resulting in the equation below, where $d$ was the Euclidean or Cosine distance. Nevertheless, it could even be a learned network as we have previously seen.</p> </div> \[p_\theta(y=n|x) = \frac{\exp(-d(f_\theta(x), c_n))}{\sum_{n^\prime}\exp(-d(f_\theta(x), c_{n^\prime}))}\;.\] <p>As opposed to the matching networks, we are now using the same embedding function $f_\theta$ for both the training and testing datapoints.</p> <h3 id="more-advanced-models">More advanced models</h3> <p>The models that we talked about today are already quite expressive for non-parametric meta learning, but they all do <strong>some form of embedding followed by nearest-neighbours</strong>. However, sometimes you might need to reason about more complex relationships between datapoints. Let’s briefly discuss a few more recent works that approach this problem.</p> <ol> <li><strong>Relation networks</strong> <d-cite key="sung2018learning"></d-cite>. <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/6/relation_net.png" class="img-fluid" alt="Alt text."/> </figure> <p>The idea is to learn non-linear relation modules on the embedding. They first embed the images and then compute this relation score, which corresponds to the distance function $d$ that we saw with prototypical models.</p> </div> </li> <li><strong>Infinite mixture of prototypes</strong> <d-cite key="allen2019infinite"></d-cite>. <div> <figure class="figure col-sm-3 float-right"> <img src="/assets/img/blog/cs330/6/mixture_of_prots.png" class="img-fluid" alt="Alt text."/> </figure> <p>The idea is to learn an infinite mixture of prototypes, which is useful when classes are not easy to cluster nicely. For example, some breeds of cats might look similar to dogs, which would not be good when averaging class embeddings. In this case, we can have multiple prototypes per class.</p> </div> </li> <li><strong>Graph neural networks</strong> <d-cite key="garcia2017few"></d-cite>. <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/6/message_passing_npm.png" class="img-fluid" alt="Alt text."/> </figure> <p>The idea is to do message passing on the embeddings instead of doing something as simple as nearest neighbours. This way, you can figure out relationships between different examples (i.e. by learning edge weights), and do more complex aggregation.</p> </div> </li> </ol> <h2 id="properties-of-meta-learning-algorithms">Properties of meta-learning algorithms</h2> <p>Now that we have seen all three different types of meta learning algorithms, we can compare each approach to see which problems might benefit from which approach. Let’s first quickly summarize all approaches.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <p><b>Black-box meta learning.</b></p> <figure class="figure col-sm-6"> <img src="/assets/img/blog/cs330/6/bbml_model_small.png" class="img-fluid" alt="Alt text."/> </figure> <p>$y^\mathrm{ts} = f_\theta(\mathcal{D}_i^\mathrm{tr}, x^\mathrm{ts})$.</p> </div> <div class="col-sm mt-3 mt-md-0"> <p><b>Optimization-based meta learning.</b></p> <p>$y^\mathrm{ts} = f_\mathrm{MAML}(\mathcal{D}_i^\mathrm{tr}, x^\mathrm{ts}) = f_{\phi_i}(x^\mathrm{ts})$, where $\phi_i = \theta - \alpha \nabla_\theta \mathcal{L}(\theta, \mathcal{D}^\mathrm{tr})$.</p> </div> <div class="col-sm mt-3 mt-md-0"> <p><b>Non-parametric meta learning.</b></p> <p>$y^\mathrm{ts} = f_\mathrm{PN}(\mathcal{D}_i^\mathrm{tr}, x^\mathrm{ts}) = \mathrm{softmax}(-d(f_\theta(x^\mathrm{ts}), c_n))$, where $c_n = \frac{1}{K} \sum_{(x,y)\in\mathcal{D}_i^\mathrm{tr}} 1(y_k=n)f_\theta(x_k)$.</p> </div> </div> <p>As you can see, all these methods share this perspective of a computational graph that we discussed in earlier posts. You can easily mix-and-match different components of these computation graphs. Below are some examples of paper that try this:</p> <ol> <li>Gradient descent on relation network embeddings.</li> <li>Both condition on data and run gradient descent <d-cite key="rusu2018meta"></d-cite>.</li> <li>Model-agnostic meta learning, but initialize last layer as a prototype network during meta-training <d-cite key="triantafillou2019meta"></d-cite>.</li> </ol> <p>Let’s make a table of the benefits and downsides of each method that we have discussed up to this point in the series:</p> <table> <thead> <tr> <th>Black-box</th> <th>Optimization-based</th> <th>Non-parametric</th> </tr> </thead> <tbody> <tr> <td>[+] Complete expressive power</td> <td>[~] Expressive for very deep models (in a supervised learning setting)</td> <td>[+] Expressive for most architectures</td> </tr> <tr> <td>[-] Not consistent</td> <td>[+] Consistent, reduces to gradient descent</td> <td>[~] Consistent under certain conditions</td> </tr> <tr> <td>[+] Easy to combine with a variety of learning problems</td> <td>[+] Positive inductive bias at the start of meta learning, handles varying and large number of classes well</td> <td>[+] Entirely feedforward, computationally fast and easy to optimize</td> </tr> <tr> <td>[-] Challenging optimization problem (no inductive bias at initialization)</td> <td>[-] Second-order optimization problem</td> <td>[-] Harder to generalize for varying number of classes</td> </tr> <tr> <td>[-] Often data-inefficient</td> <td>[-] Compute- and memory-intensive</td> <td>[-] So far limited to classification</td> </tr> </tbody> </table> <ul> <li><strong>Expressive power</strong>: The ability of $f$ to model a range of learning procedures.</li> <li><strong>Consistency</strong>: Learned learning procedure will monotonically improve with more data</li> <li><strong>Uncertainty awareness</strong>: Ability to reason about ambiguity during learning.</li> </ul> <p>We have not yet discussed the uncertainty awareness of methods, but it plays an important role in active learning, calibrated uncertainty, reinforcement learning, and principled Bayes approaches. We will discuss this later on in the series!</p> <h2 id="examples-of-meta-learning-in-practice">Examples of meta learning in practice</h2> <p>In this section, we will very briefly talk about 6 different problem settings where meta learning has been used, some of which we have already seen in previous posts. This should give you a good idea of some different applications, and show you that it can be utilized in many different domains.</p> <h3 id="land-cover-classification">Land-cover classification</h3> <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/6/paper_land_cover.png" class="img-fluid" alt="Alt text."/> </figure> <p>The goal of this paper <d-cite key="russwurm2020meta"></d-cite> is to classify and segment satellite images in different regions of the world. Every region corresponds to a task, and the datasets are thus images from a particular region. The problem is that manually segmenting this data is expensive, so the authors use meta learning to quickly be able to segment new regions given limited training data on these regions.</p> <p><b>Model</b>: Optimization-based (model-agnostic meta learning)</p> </div> <h3 id="student-feedback-generation">Student feedback generation</h3> <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/6/paper_student_feedback_results.png" class="img-fluid" alt="Alt text."/> </figure> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/6/paper_student_feedback_example.png" class="img-fluid" alt="Alt text."/> </figure> <p>The goal of this paper <d-cite key="wu2021prototransformer"></d-cite> is to automatically provide students with feedback on coding assignments for high-quality Computer Science education. The different tasks corresponded to different rubrics for different assignments or exams. The datasets were then constructed of the solutions of the students (in this paper, they were always Python programs).</p> <p><b>Supervised baseline</b>: Train a classifier per task, using same pre-trained CodeBERT <d-cite key="feng2020codebert"></d-cite>.</p> <p>Outperforms supervised learning by 8-17%, and more accurate than human TA on held-out rubric! However, there is room for improvement on a held-out exam.</p> <p><b>Model</b>: Non-parametric (prototypical network with pre-trained Transformer, task information, and side information).</p> </div> <h3 id="low-resource-molecular-property-prediction">Low-resource molecular property prediction</h3> <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/6/paper_molecule_results.png" class="img-fluid" alt="Alt text."/> </figure> <p>The goal of this paper <d-cite key="nguyen2020meta"></d-cite> is to predict certain chemical properties and activities of different molecules in Silico models, which could potentially be useful for low-resolution drug discovery problems. The tasks here correspond to different chemical properties and activations, and the corresponding datasets are different instances of these properties and activations.</p> <p><b>Model</b>: Optimization-based MAML, first-order MAML, and an ANIL Gated graph neural net base model.</p> </div> <h3 id="one-shot-imitation-learning">One-shot imitation learning</h3> <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/6/paper_imitation.png" class="img-fluid" alt="Alt text."/> </figure> <p>The goal of this paper <d-cite key="yu2018one"></d-cite> is to do one-shot imitation learning for object manipulation by using video demonstrations of a human. The tasks would be different manipulation problems. The training dataset would be the human demonstration, and the testing dataset would be the tele-operated demonstration.</p> <p><b>Note</b>: See that they training and testing datasets do not need to be sampled independently from the overall dataset for meta learning to work!</p> <p><b>Model</b>: Model-agnostic meta learning with learned inner loss function.</p> </div> <h3 id="dermatological-image-classification">Dermatological image classification</h3> <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/6/paper_derm_res.png" class="img-fluid" alt="Alt text."/> </figure> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/6/paper_derm.png" class="img-fluid" alt="Alt text."/> </figure> <p>The goal of this paper <d-cite key="prabhuprototypical"></d-cite> is to perform dermatological image classification that is good for all different skin conditions, which are the tasks in this case. The datasets consist of images of these skin conditions from different people.</p> <p><b>Model</b>: Non-parametric prototype networks with multiple prototypes per class using clustering objective.</p> <p>Results show that the clustering prototype networks perform better than normal ones and competitive against a ResNet model that is pre-trained on ImageNet and fine-tuned on 200 classes with balancing. This is a very strong baseline with access to more info during training, and it requires re-training for new classes.</p> </div> <h3 id="few-shot-human-motion-prediction">Few-shot human motion prediction</h3> <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/6/paper_motion.png" class="img-fluid" alt="Alt text."/> </figure> <p>The goal of this paper <d-cite key="gui2018few"></d-cite> is to do few-shot motion prediction using meta learning, which could potentially be useful for autonomous driving and human-robot interaction. The tasks are different humans and different motivation. The corresponding train dataset $\mathcal{D}^\mathrm{tr}_i$ is composed of the past $K$ seconds of the motion, and the test set $\mathcal{D}^\mathrm{test}_i$ is composed of the future second(s) of the motion.</p> <p><b>Note</b>: See that they training and testing datasets do not need to be sampled independently from the overall dataset for meta learning to work!</p> <p><b>Model</b>: Optimization-based/black-box hybrid, MAML with additional learned update rule and a recurrent neural net base model.</p> </div> <hr/>]]></content><author><name>Lars C.P.M. Quaedvlieg</name></author><category term="deep-multi-task-and-meta-learning"/><category term="course"/><summary type="html"><![CDATA[This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to to understand the third form of meta learning: non-parametric few-shot learning. We will also compare the three different methods of meta learning. Finally, we give practical examples of meta learning, in domains such as imitation learning, drug discovery, motion prediction, and language generation!]]></summary></entry><entry><title type="html">CS-330 Lecture 4: Optimization-Based Meta-Learning</title><link href="https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-obml/" rel="alternate" type="text/html" title="CS-330 Lecture 4: Optimization-Based Meta-Learning"/><published>2024-03-10T00:00:00+00:00</published><updated>2024-03-10T00:00:00+00:00</updated><id>https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-obml</id><content type="html" xml:base="https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-obml/"><![CDATA[<p>The goal of this lecture is to understand the <strong>basics of optimization-based meta learning</strong> techniques. You will also learn about the <strong>trade-offs</strong> between black-box and optimization-based meta learning! If you missed the previous lecture, which was about black-box meta learning and in-context learning with GPT-3, you can head over <a href="/blog/2024/cs330-stanford-bbml-icl/">here</a> to view it.</p> <p>As always, since I am still new to this blogging thing, reach out to me if you have any feedback on my writing, the flow of information, or whatever! You can contact me through <a href="https://www.linkedin.com/in/lars-quaedvlieg/">LinkedIn</a>. ☺</p> <p>The link to the lecture slides can be found <a href="https://cs330.stanford.edu/materials/cs330_optbased_metalearning_2023.pdf">here</a>.</p> <h2 id="overall-approach">Overall approach</h2> <p>In the previous post, we looked into black-box meta learning. To recap, this approach attempts to output task-specific parameters or contextual information with some meta-model. One major benefit of this approach is its <strong>expressiveness</strong>. However, it also requires solving a challenging optimization problem, which is incredibly data-inefficient.</p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/5/oml_approach.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Computation pipeline for optimization-based meta learning.</figcaption> </figure> <p>In this post, we will focus on <strong>optimized-based meta learning</strong>. The key idea behind this is to <strong>embed optimization</strong> inside the inner learning process. In the figure above, an example of this idea is depicted. With some initial model $f_\theta$, we will run gradient descent on the datapoints in $\mathcal{D}_i^\mathrm{tr}$ to produce the task-specific network with parameters $\phi_i$. In summary, the goal will be to find the model parameters $\theta$ such that optimizing these parameters to specific tasks is as <strong>effective</strong> and <strong>efficient</strong> as possible.</p> <div> <figure class="figure col-sm-6 float-right"> <img src="/assets/img/blog/cs330/5/fine_tune_loss.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Results on model fine-tuning with respect to dataset size.</figcaption> </figure> <p>As recalled from a previous post, we are trying to do a similar thing in fine-tuning. Specifically, in fine-tuning, we are using a pre-trained model with parameters $\theta$ to find new task-specific parameters $\phi \leftarrow \theta - \alpha \nabla_\theta \mathcal{L}(\theta, \mathcal{D}^\mathrm{tr})$. We also saw that fine-tuning often performed much better than training from scratch. However, fine-tuning generally needs a lot of data in order to adapt well to a new task, meaning it doesn’t perform well with few-shot learning.</p> </div> <p>You may now see that our proposed optimization-based meta learning approach tries to optimize pre-trained parameters $\theta$, such that it does well in the few-shot regime, unlike transfer learning through fine-tuning. If we now adapt the meta learning objective function from the previous lecture with this fine-tuning, we obtain the following:</p> \[\min_\theta \sum_{\mathcal{T}_i}\mathcal{L}(\theta - \alpha \nabla_\theta\mathcal{L}(\theta, \mathcal{D}^\mathrm{tr}_i), \mathcal{D}^\mathrm{test}_i)\;.\] <p>As you can see in the equation, you are trying to find the parameters $\theta$, such that performing fine-tuning $\theta - \alpha \nabla_\theta\mathcal{L}(\theta, \mathcal{D}^\mathrm{tr}_i)$ on a dataset $\mathcal{D}^\mathrm{tr}_i$ of a task $\mathcal{T}_i$ performs well on the task-specific test set $\mathcal{D}_i^\mathrm{test}$. If these datasets are small, the objective function will still need to optimize pre-training the model for effective fine-tuning on these tasks, meaning that it might work in the few-shot regime.</p> <p>We can write down the general training pipeline with the following steps:</p> <ol> <li>Sample a task $\mathcal{T}_i$.</li> <li>Sample disjoint datasets $\mathcal{D}_i^\mathrm{tr}$ and $\mathcal{D}_i^\mathrm{test}$ from $\mathcal{D}_i$.</li> <li>Optimize $\phi_i \leftarrow \theta - \alpha \nabla_\theta\mathcal{L}(\theta, \mathcal{D}_i^\mathrm{tr})$.</li> <li>Update $\theta$ using $\nabla_\theta \mathcal{L}(\phi_i, \mathcal{D}_i^\mathrm{test})$.</li> </ol> <p>Notice that we only optimize $\phi_i$ to then use it to update parameters $\theta$. This means that $\phi_i$ is discarded and re-computed at each iteration. Additionally, we can optimize for different parameters, such as $\alpha$ as well by including it as a parameter in $\theta$. Besides computational efficiency of this, there is another problem. If we expand $\nabla_\theta \mathcal{L}(\phi_i, \mathcal{D}_i^\mathrm{test})$, we get <strong>second-order</strong> derivates. Let’s show this below:</p> \[\begin{align*} \nabla_\theta\mathcal{L}(\phi_i, \mathcal{D}_i^\mathrm{test}) &amp;= \nabla_{\bar{\phi}}\mathcal{L}(\bar{\phi}, \mathcal{D}_i^\mathrm{test})\bigg|_{\bar{\phi} = \phi_i} \frac{\partial\phi_i}{\partial\theta} \\ &amp;= \nabla_{\bar{\phi}}\mathcal{L}(\bar{\phi}, \mathcal{D}_i^\mathrm{test})\bigg|_{\bar{\phi} = \phi_i}\left(I - \alpha \nabla^2_\theta\mathcal{L}(\theta, \mathcal{D}^\mathrm{tr}_i)\right)\;. \end{align*}\] <p>Unfortunately, we can see the Hessian $\nabla^2_\theta\mathcal{L}(\theta, \mathcal{D}^\mathrm{tr}_i)$. We would have to compute this, but luckily, $\nabla_{\bar{\phi}}\mathcal{L}(\bar{\phi}, \mathcal{D}_i^\mathrm{test})\vert_{\bar{\phi} = \phi_i}$ is a row vector. Due to Hessian-vector products, we can compute $\nabla_\theta\mathcal{L}(\phi_i, \mathcal{D}_i^\mathrm{test})$ without computing the whole Hessian. Let’s show this below:</p> \[\begin{align*} g(x + \Delta x) &amp;\approx g(x) + H(x)\Delta x \\ g(x + rv) &amp;\approx g(x) + rH(x)v \\ H(x)v &amp;\approx \frac{g(x+rv) - g(x)}{r}\;. \end{align*}\] <p>In the first line, we are using a simple Taylor expansion to approximate the function $g(x+\Delta x)$, which is the gradient function. Then, we rewrite $\Delta x$ as with scalar $r$ and vector $v$. We can finally rearrange the equation to get to our result. The last line shows that we can approximate a Hessian-vector product by using <strong>two gradient evaluations</strong>. Whilst this is still more than one, it is much more computationally efficient than computing the entire Hessian matrix.</p> <p>Since $\nabla_{\bar{\phi}}\mathcal{L}(\bar{\phi}, \mathcal{D}_i^\mathrm{test})\vert_{\bar{\phi} = \phi_i}\left(I - \alpha \nabla^2_\theta\mathcal{L}(\theta, \mathcal{D}^\mathrm{tr}_i)\right) = v -\alpha vH$, you can see that we can use the previous result to compute the gradient, rather then computing the whole Hessian.</p> <p>A common misconception is that you will get higher than 2nd-order derivatives when computing multiple iterations of $\phi_i \leftarrow \theta - \alpha \nabla_\theta\mathcal{L}(\theta, \mathcal{D}_i^\mathrm{tr})$. However, this is not true, since the derivatives will be sequential rather than nested. You can try this yourself if you want to test your knowledge! If you do this correctly, you will see that doing more iterations will increase the amount of memory used <strong>linearly</strong>, and the amount of compute necessary <strong>linearly</strong> as well. In practice, usually you do not need more than 5 inner gradient steps, which is usually fine for few-shot learning tasks. In future posts, we will discuss methods that work for hundreds of inner gradient steps.</p> <div> <figure class="figure col-sm-5 float-right"> <img src="/assets/img/blog/cs330/5/maml_viz.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Visualization of the parameter space of MAML.</figcaption> </figure> <p>This approach, which is called <b>Model-Agnostic Meta-Learning</b><d-cite key="finn2017model"></d-cite> (MAML), is also represented in the figure on the right. Here, $\phi_1^*, \phi_2^*, \phi_3^*$ are the optimal parameters for tasks $1, 2, 3$.</p> </div> <p>At meta-test time, we can follow the following steps, which are very similar to meta-training:</p> <ol> <li>Sample a task $\mathcal{T}_i$.</li> <li>Given training dataset $\mathcal{D}_j^\mathrm{tr}$.</li> <li>Optimize $\phi_j \leftarrow \theta - \alpha \nabla_\theta\mathcal{L}(\theta, \mathcal{D}_j^\mathrm{tr})$.</li> <li>Make predictions on new datapoints $f_{\phi_j}(x)$.</li> </ol> <hr/> <h2 id="compare-optimization-based-vs-black-box">Compare: optimization-based vs. black-box</h2> <p>In the previous section, we got some intuition behind the Model-Agnostic Meta-Learning algorithm. For now, it looks like this method is completely different from black-box meta learning, but let’s spend some times trying to understand the connection between the two. Let’s write down both objectives again.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <p><b>Black-box meta learning</b></p> <p>$y^\mathrm{ts} = f_\mathrm{black-box}(\mathcal{D}^\mathrm{tr}_i, x^\mathrm{ts})$.</p> </div> <div class="col-sm mt-3 mt-md-0"> <p><b>Model-agnostic meta learning</b> (MAML)</p> <p>$y^\mathrm{ts} = f_\mathrm{MAML}(\mathcal{D}^\mathrm{tr}_i, x^\mathrm{ts}) = f_{\phi_i}(x^\mathrm{ts})$, where $\phi_i = \theta - \alpha \nabla_\theta \mathcal{L}(\theta ,\mathcal{D}_i^\mathrm{tr})$.</p> </div> </div> <p>If you look at both equations, both equations look more similar than it may seem. Both methods are still only functions of $\mathcal{D}^\mathrm{tr}_i$ and $x^\mathrm{ts}$. The main difference is that MAML uses an <strong>“embedded” gradient operator</strong> in its <strong>computation graph</strong>, which is just a directed graph of all computations that is done by the function.</p> <p>Keeping this idea of a computation graph in mind, you might think of the idea to mix and match different components of the computation graph. This idea has been explored in various works. For example, one paper learned the initialization parameters $\theta$, but replaced the gradient updates with a learned network<d-cite key="ravi2016optimization"></d-cite>, meaning that $\phi_i = \theta - f(\theta, \mathcal{D}_i^\mathrm{tr}, \nabla_\theta\mathcal{L})$. It turns out this approach is not very practical, but it has a nice conceptual meaning that goes along well with the idea of a computation graph.</p> <h3 id="comparing-performances">Comparing performances</h3> <p>We now wish to compare the two different approaches to each other. First, let’s think about the following: If we have a test task $\mathcal{T}_j$ that is a bit different from the tasks that we trained our models on, which approach do you think will be better?</p> <p>We hope to see that optimization-based meta learning performs better in this case, because it’s explicitly optimizing the effectiveness of fine-tuning to new tasks. With black-box meta learning, we are essentially just training a model to output model parameters or context, which does not guarantee to work for unseen tasks (though we hope it will!).</p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/5/adaptation_res.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Results of different meta learning algorithms on augmented samples from the Omniglot dataset.</figcaption> </figure> <p>We will test this by looking at the Omniglot image classification problem again. However, we will try to see what happens when we vary the tasks after training the models on them. In this case, digits from the datasets are warped to see a form of out-of-distribution performance. In the figure above, which is taken from<d-cite key="finn2017meta"></d-cite>, you can see that, as expected, an optimization-based method like MAML performs much better than black-box-based approaches like SNAIL <d-cite key="mishra2017simple"></d-cite> or MetaNet <d-cite key="munkhdalai2017meta"></d-cite> for these types of problems.</p> <p>You might think that this structure comes at a cost of expressiveness. But, in <d-cite key="finn2017meta"></d-cite>, they showed that MAML can approximate any function of $\mathcal{D}^\mathrm{tr}_i, x^\mathrm{ts}$ given some assumptions (i.e. you need a very deep network for this to hold). However, despite these assumptions, this result shows that MAML benefits of the inductive bias of optimizing fine-tuning explicitly without losing any of the expressive power.</p> <hr/> <h2 id="challenges-and-solutions">Challenges and solutions</h2> <p>Whilst optimization-based meta learning methods may sound perfect from this post, there are quite some challenges when using these methods. We will list some of them and discuss them in more detail.</p> <ol> <li> <p><strong>Bi-level optimization can exhibit instabilities.</strong></p> <p>This can be the case because you have nested optimizations that are heavily dependent on each other. Some unexpected result in one level of the optimization can result in an unexpected effect, which will then be propagated throughout training. There are multiple simple tricks that can be used to try to stabilize training:</p> <ul> <li>Automatically learn inner vector learning rate, tune outer learning rate <d-cite key="li2017meta"></d-cite> <d-cite key="behl2019alpha"></d-cite>.</li> <li>Optimize only a subset of the parameters in the inner loop <d-cite key="zhou2018deep"></d-cite> <d-cite key="zintgraf2019fast"></d-cite>.</li> <li>Decouple inner learning rate, batch normalization statistics per-step <d-cite key="antoniou2018train"></d-cite>.</li> <li>Introduce context variables for increased expressive power <d-cite key="finn2017one"></d-cite> <d-cite key="zintgraf2019fast"></d-cite>.</li> </ul> </li> <li> <p><strong>Backpropagating through many inner gradient steps is compute- &amp; memory-intensive.</strong></p> <p>As we said before, the amount of inner gradient steps with MAML is usually only around 5, due to the amount of compute and memory required to optimize over such a computation graph. Again, there are some tricks to try to address this, but we will discuss more options in the lecture about large-scale meta learning.</p> <ul> <li>(Crudely) approximate $\frac{\delta\phi_i}{\delta\theta}$ as the identity matrix <d-cite key="nichol2018reptile"></d-cite>. This seems to work surprisingly well for simple few-shot learning problems, but it doesn’t scale to more complex problems.</li> <li>Only optimize the last layer of weights using ridge regression, logistic regression <d-cite key="bertinetto2018meta"></d-cite>, or support vector machines <d-cite key="lee2019meta"></d-cite>. This can lead to a closed form convex optimization problem on top of the meta-learned features.</li> <li>Derive the meta-gradient using the implicit function theorem <d-cite key="rajeswaran2019meta"></d-cite>. This way you can compute the full gradient without differentiating through the entire computation graph of the multiple iterations.</li> </ul> </li> </ol> <p>Let’s summarize the upsides and downsides:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <p><b>Upsides</b></p> <ul> <li>Positive inductive bias at the start of meta learning.</li> <li>Extrapolates better via structure of optimization.</li> <li>Maximally expressive for a sufficiently deep network.</li> <li>Model-agnostic!</li> </ul> </div> <div class="col-sm mt-3 mt-md-0"> <p><b>Downsides</b></p> <ul> <li>Typically requires second-order optimization.</li> <li>Can be compute and/or memory intensive.</li> <li>Can be prohibitively expensive for large models.</li> </ul> </div> </div> <hr/> <h2 id="case-study-of-land-cover-classification">Case study of land cover classification</h2> <p>We will now study an example of optimization-based meta learning for a pretty cool problem: land cover classification <d-cite key="russwurm2020meta"></d-cite>! Imagine that you are given a bunch of <strong>satellite data of different terrains</strong>, and your goal is to predict <strong>what the land is used for</strong> by segmenting it into different classes. Research like this can for example be used to understand how different climates change over time, or urban planning. Unfortunately, it is very expensive to label these images, leading to small datasets.</p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/5/landmark_problem.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Depiction of the use of MAML for land cover classification.</figcaption> </figure> <p>As you can see in the image above, we have terrains from different regions of the world, which can look very different, but probably still share some geological structural similarities. Given this description, try to think to yourself: Do you think meta learning would be a good approach to this problem?</p> <p>If we let our tasks correspond to these different regions, we can try to use optimization-based meta learning to learn to effectively and efficiently fine-tune to unknown regions of the world! For these new regions, we would manually segment small amount of data, and then use our model to take care of the rest.</p> <p>The paper looks at two datasets:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <p>SEN12MS <d-cite key="schmitt2019sen12ms"></d-cite>, which contains geographic metadata, making it easy to construct good meta-train, meta-val, and meta-test sets.</p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/5/sen12ms.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Example of data in the SEN12MS dataset.</figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <p>DeepGlobe <d-cite key="demir2018deepglobe"></d-cite>, which did not provide this metadata. Instead, clustering was used to create datasets of similar terrains.</p> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/5/deepglobe.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Example of data in the DeepGlobe dataset.</figcaption> </figure> </div> </div> <figure class="figure col-sm-12"> <img src="/assets/img/blog/cs330/5/use_case_res.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Results of different types of approaches to the land cover classification problem.</figcaption> </figure> <p>The results of using MAML on these two datasets is shown in the figure above. As you can see, MAML is more efficient and effective than (dark blue) pre-training on meta-training data and fine-tuning and (light blue) training on the new terrain from scratch. Hopefully these results give you a good idea of the potential of meta learning for few-shot learning!</p> <hr/>]]></content><author><name>Lars C.P.M. Quaedvlieg</name></author><category term="deep-multi-task-and-meta-learning"/><category term="course"/><summary type="html"><![CDATA[This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to understand the basics of optimization-based meta learning techniques. You will also learn about the trade-offs between black-box and optimization-based meta learning!]]></summary></entry><entry><title type="html">CS-330 Lecture 3: Black-Box Meta-Learning &amp;amp; In-Context Learning</title><link href="https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-bbml-icl/" rel="alternate" type="text/html" title="CS-330 Lecture 3: Black-Box Meta-Learning &amp;amp; In-Context Learning"/><published>2024-03-03T00:00:00+00:00</published><updated>2024-03-03T00:00:00+00:00</updated><id>https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-bbml-icl</id><content type="html" xml:base="https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-bbml-icl/"><![CDATA[<p>The goal of this lecture is to learn how to <strong>implement black-box meta-learning</strong> techniques. We will also talk about a <strong>case study of GPT-3</strong>! If you missed the previous lecture, which was about transfer learning by fine-tuning and meta learning, you can head over <a href="/blog/2024/cs330-stanford-tl-ml/">here</a> to view it.</p> <p>As always, since I am still new to this blogging thing, reach out to me if you have any feedback on my writing, the flow of information, or whatever! You can contact me through <a href="https://www.linkedin.com/in/lars-quaedvlieg/">LinkedIn</a>. ☺</p> <p>The link to the lecture slides can be found <a href="https://cs330.stanford.edu/materials/cs330_metalearning_bbox_2023.pdf">here</a>.</p> <h2 id="black-box-adaptation-approaches">Black-box adaptation approaches</h2> <figure class="figure col-sm-12 float-right"> <img src="/assets/img/blog/cs330/4/omniglot.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Example of the Omniglot dataset.</figcaption> </figure> <p>The content of this section will build on the general recipe for meta-learning problems that we saw in the previous lecture. In order to explain it, we will use the example of the Omniglot dataset <d-cite key="lake2019omniglot"></d-cite>, which is a dataset of 1,623 characters from 50 different alphabets. In this problem, every alphabet would refer to a different task. In our example, we will do 3-way 1-shot learning, meaning that our sampled datasets consist of 3 classes with 1 example per class at every step. One iteration of the black-box meta-training process then has the following steps:</p> <ol> <li>Sample task $\mathcal{T}_i$ or a mini-batch of tasks. In our case, this would correspond to generating the language(s).</li> <li>From the selected language(s), we sample disjoint datasets $\mathcal{D}_i^\mathrm{tr}$ and $\mathcal{D}_i^\mathrm{test}$ from $\mathcal{D}_i$. In our example, this will be a disjoint dataset with 3 samples of characters for every language alphabet.</li> </ol> <div> <figure class="figure col-sm-5 6 float-right"> <img src="/assets/img/blog/cs330/4/param_lstm.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Basic model architecture for black-box meta learning.</figcaption> </figure> <p>Now that we have these datasets, our goal is to train a neural network to represent $\phi_i = f_\theta(\mathcal{D}_i^\mathrm{tr})$. After computing these task parameters given a sampled training dataset, we can predict the test targets with $y^\mathrm{ts} = g_{\phi_i}(x^\mathrm{ts})$. An example of how such a model could work, is depicted in the figure above. Here, we are using a sequence model for $f_\theta$, which generates the parameters $\phi_i$. However, you can use all your fancy architectures that can handle a varying number of input sample. This is necessary due to varying dataset lengths.</p> </div> <p>After computing $y^\mathrm{ts}$, we can do backpropagation of the loss that is generated with the this test dataset. The full optimization objective is shown in the equation below:</p> \[\min_\theta \sum_{\mathcal{T}_i} \sum_{(x,y) \sim \mathcal{D}^\mathrm{test}_i} - \log g_{\phi_i}(y\vert x) = \min_\theta \sum_{\mathcal{T}_i}\mathcal{L}(f_\theta(\mathcal{D}^\mathrm{tr}_i), \mathcal{D}^\mathrm{test}_i)\;.\] <p>Notice that we are optimizing the parameters $\theta$. The task-specific parameters $\phi_i$ are generated by $f_\theta(\mathcal{D}_i^\mathrm{tr})$, and so they are not updated. Also note that the loss is calculated with respect to the sampled <strong>test dataset</strong>! This is no problem, since it makes sense to evaluate on new tasks for meta learning.</p> <p>Now that you understand the architecture, we can write down the last two steps of the meta-training process:</p> <ol> <li>Compute $\phi_i \leftarrow f_\theta(\mathcal{D}_i^\mathrm{tr})$.</li> <li>Update $\theta$ using $\nabla_\theta \mathcal{L}(\phi_i, \mathcal{D}_i^\mathrm{test})$.</li> </ol> <h3 id="a-more-scalable-architecture">A more scalable architecture</h3> <p>However, we run into an issue. How do we let the model $f_\theta$ output another model’s parameters $\phi_i$? Not only can this be quite tricky to do, it also does not scale to larger parameter vectors $\phi_i$! Can you think of an alternative way of going this?</p> <figure class="figure col-sm-12 float-right"> <img src="/assets/img/blog/cs330/4/better_model.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">More scalable architecture for black-box meta-learning.</figcaption> </figure> <p>Instead of letting $f_\theta$ output $\phi_i$, we instead output a hidden state $h_i$, which is a low-dimensional vector that is supposed to represent contextual task information from the training dataset. If you recall the different ways of conditioning that we saw for multi-task learning, you can see that we can train a model end-to-end by conditioning as $y^\mathrm{ts} = g_{\phi}(x^\mathrm{ts} \vert h_i)$. Now, notice that we have a general set of parameters $\phi$ for $g$; it does not need to be task-specific anymore, since we are already conditioning on task information. In the figure above, $\theta$ are the parameters of the sequence model, and $\phi$ are the parameters of the convolutional network.</p> <p>❗One problem that sometimes occurs with this architecture, is that the model learns to <strong>ignore conditioning on $h_i$.</strong> In that case, it is essentially just learning to memorize, and not using the training dataset. In order to avoid that, you can randomize the numerical label assignment to the target variables when sampling the datasets $\mathcal{D}^{tr}_i$ and $\mathcal{D}^{test}_i$. If the numerical label is different each time, it cannot just memorize the sample from the testing set.</p> <h3 id="black-box-adaptation-architectures">Black-box adaptation architectures</h3> <p>The architecture that we just presented was more-or-less first proposed on the Omniglot dataset at ICML in 2016 <d-cite key="santoro2016meta"></d-cite>. It used LSTMs with Neural Turing Machines (which are not used anymore nowadays). Since then, a lot of new architectures have been proposed.</p> <p>At ICML 2018, an architecture called the DeepSet architecture <d-cite key="garnelo2018conditional"></d-cite> was published. The idea is to pass all your dataset samples through a feedforward neural network to get an embedding of each sample, and then average those. This way, you have a permutation-invariant model which is still model-agnostic. Given some conditions on the width and depth of the network, these models can represent any permutation-invariant function.</p> <p>There are quite some more papers that used other external memory mechanisms <d-cite key="munkhdalai2017meta"></d-cite>, or convolutions and attention <d-cite key="mishra2017simple"></d-cite>.</p> <p>Unfortunately, these models are still quite limited in capabilities against “difficult” datasets, as you can see in the table below.</p> <figure class="figure col-sm-12 float-right"> <img src="/assets/img/blog/cs330/4/bmml_results.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Results of a model trained with black-box meta-learning.</figcaption> </figure> <p>In summary, some benefits of black-box meta learning are its <strong>expressiveness</strong>, how easy it is to combine with a <strong>variety of learning problems</strong> (such as SL or RL). Nonetheless, it is a <strong>challenging optimization problem</strong> for a <strong>complex model</strong>, and it is often <strong>data-inefficient</strong>.</p> <hr/> <h2 id="case-study-of-gpt-3">Case study of GPT-3</h2> <p>With the rise of research on in-context learning, especially with foundation models, GPT-3 <d-cite key="brown2020language"></d-cite> is a good example of a black-box meta-learner, trained on language generation tasks. We can represent the task-specific datasets $\mathcal{D}_i^\mathrm{tr}$ as a sequence of characters, and $\mathcal{D}_i^\mathrm{test}$ as the following sequence of characters. This way, $\mathcal{D}_i^\mathrm{tr}$ is what the model is being conditioned on (its context), and $\mathcal{D}_i^\mathrm{test}$ is what it has to generate.</p> <p>The meta-training dataset consists of crawled data from the internet, English-language Wikipedia, and two books corpora, with a giant Transformer architecture as its network (175 billion parameters, 96 layers, 3.2M batch size).</p> <p>For these datasets, there are a multitude of different tasks such as, but definitely not limited to, <strong>spelling correction</strong>, <strong>simple math problems</strong>, or <strong>translating between languages</strong>. By encoding every task as text, the authors are able to obtain meta-training data incredibly easily.</p> <figure class="figure col-sm-12 float-right"> <img src="/assets/img/blog/cs330/4/gpt3_pipeline.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Abstract representation of the training meta-train pipeline of GPT-3.</figcaption> </figure> <p>In the case of GPT-3, text generation, also known as in-context learning, represents the inner loop of the optimization process. The outer loop represents the model optimizing across different tasks, which is very similar to the process that we saw in the previous section.</p> <p>With this model, you can easily do few-shot learning by adding examples in text form to the context of the model. Even through the model is far from perfect, its results are extremely impressive. It is also no oracle and can fail in unintuitive ways! If there is anything we have learned from recent research, it is that <strong>the choice of $\mathcal{D}^\mathrm{tr}$ at test time matters</strong> (welcome to the world of prompt engineering).</p> <p>It is also interesting to think about what is needed for <strong>few-shot learning to emerge</strong> when training a model. This is an active research are, but it seems that (1) temporal correlation in your data with dynamic meaning of words, and (2) large model capacities definitely seems to make a difference here <d-cite key="chan2022data"></d-cite>.</p> <hr/>]]></content><author><name>Lars C.P.M. Quaedvlieg</name></author><category term="deep-multi-task-and-meta-learning"/><category term="course"/><summary type="html"><![CDATA[This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to learn how to implement black-box meta-learning techniques. We will also talk about a case study of GPT-3!]]></summary></entry><entry><title type="html">CS-330 Lecture 2: Transfer Learning and Meta-Learning</title><link href="https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-tl-ml/" rel="alternate" type="text/html" title="CS-330 Lecture 2: Transfer Learning and Meta-Learning"/><published>2024-03-03T00:00:00+00:00</published><updated>2024-03-03T00:00:00+00:00</updated><id>https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-tl-ml</id><content type="html" xml:base="https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-tl-ml/"><![CDATA[<p>The goal of this lecture is to learn how to transfer knowledge from one task to another, discuss what it means for two tasks to share a common structure, and start thinking about meta learning. If you missed the previous lecture, which was about multi-task learning, you can head over <a href="/blog/2024/cs330-stanford-mtl/">here</a> to view it.</p> <p>As always, since I am still new to this blogging thing, reach out to me if you have any feedback on my writing, the flow of information, or whatever! You can contact me through <a href="https://www.linkedin.com/in/lars-quaedvlieg/">LinkedIn</a>. ☺</p> <p>The link to the lecture slides can be found <a href="https://cs330.stanford.edu/materials/cs330_finetune_transfer_meta_learning_problem_setup_2023.pdf">here</a>.</p> <h2 id="transfer-learning">Transfer learning</h2> <p>In contrast to multi-task learning, which tackles several tasks $\mathcal{T}_1, \cdots, \mathcal{T}_i$ simultaneously, transfer learning takes a sequential approach. It focuses on mastering a specific task $\mathcal{T}_b$ after the knowledge has been acquired from source task(s) $\mathcal{T}_a$. A common assumption is that $\mathcal{D}_a$ cannot be accessed during the transfer.</p> <p>Transfer learning is a valid solution to multi-task learning, because it can sequentially apply knowledge from one task to another. This is unlike multi-task learning, which requires simultaneous learning of all tasks.</p> <p>It is advantageous in the case of a large dataset $\mathcal{D}_a$, where continuous retraining is not feasible. Transfer learning makes sense here by utilizing the acquired knowledge without the need for repetitive training on the large dataset. Additionally, when you do not need to train two tasks simultaneously, you might opt to solve it them sequentially using transfer learning.</p> <h3 id="transfer-learning-through-fine-tuning">Transfer learning through fine-tuning</h3> \[\phi \leftarrow \theta - \alpha \nabla_\theta \mathcal{L}(\theta, \mathcal{D}_\mathrm{tr})\;.\] <p>One method of transfer learning involves the fine-tuning of a pre-trained model with parameters $\theta$. This process starts with a model whose parameters have been <b>initially trained on a large, diverse dataset</b>, such as ImageNet <d-cite key="huh2016makes"></d-cite>. The usefulness of fine-tuning lies in its ability to adapt these pre-trained parameters to a new task $\mathcal{T}_b$ by continuing the training process with a dataset $\mathcal{D}_\mathrm{tr}$ specific to that task. Typically, this involves many iterations of gradient descent steps, where the pre-trained model's parameters $\theta$ are updated by moving in the direction that minimizes the loss $\mathcal{L}$. This optimization process is depicted in the equation above.</p> <p>When utilizing fine-tuning for transfer learning, there are several common design choices that are usually considered:</p> <ul> <li>Opting for a <strong>lower learning rate</strong> to prevent the overwriting of the knowledge captured during pre-training.</li> <li>Employing even <strong>smaller learning rates for the earlier layers</strong> of the network, preserving more generic features.</li> <li>Initially <strong>freezing the early layers</strong> of the model, then gradually unfreezing them as training progresses.</li> <li><strong>Reinitializing the last layer</strong> to tailor it to the specifics of the new task.</li> <li><strong>Searching over hyperparameters</strong> using cross-validation to find the optimal configuration.</li> <li>Making smart choices about the <strong>model architecture</strong>.</li> </ul> <figure class="figure col-sm-11 float-right"> <img src="/assets/img/blog/cs330/3/pretraining_datasets.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center"> Aggregate performance of a model across 10 finetuning datasets when it is (i) randomly initialized (ii) pretrained on upstream corpus (BookWiki) (iii) pretrained on the finetuning dataset itself.</figcaption> </figure> <p>❗However, this common knowledge does not always hold true. For example, when using unsupervised pre-trained objectives, you may not require diverse data for pre-training. The figure above shows that a model that was pretrained on the downstream dataset performs similarly to an upstream corpus such as BookWiki <d-cite key="krishna2022downstream"></d-cite>.</p> <figure class="figure col-sm-11 float-right"> <img src="/assets/img/blog/cs330/3/layer_tuning.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center"> How fine-tuning different layers has different effects.</figcaption> </figure> <p>Furthermore, depending on the downstream task, it may be better to tune the first or middle layers, rather than the last layers. For example, for image corruption, it makes more sense to fine-tune the first layer of the model, since it’s more of an input-level shift in data distribution <d-cite key="lee2022surgical"></d-cite>. The figure below shows more of these examples. Chelsea’s advice is to first <strong>train the last layer</strong>, and then <strong>fine-tune the entire network</strong> <d-cite key="kumar2022fine"></d-cite>, since fine-tuning can distort pre-trained features.</p> <figure class="figure col-sm-12 float-right"> <img src="/assets/img/blog/cs330/3/tl_dataset_size.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">The effect of fine-tuning with different dataset sizes.</figcaption> </figure> <p>However, one big disadvantage to fine-tuning is that <strong>it does not work well for very small target datasets</strong>. An example of this can be seen in the figure above. Luckily, this is where meta learning comes into play!</p> <hr/> <h2 id="introduction-to-meta-learning">Introduction to meta learning</h2> <p>With transfer learning, we initialize a model and then hope that it helps to solve the target task, by for example fine-tuning it. With meta-learning, we are asking the question of whether we can <strong>explicitly optimize for transferability</strong>. Thus, given a set of training tasks, can we optimize the ability to learn these tasks quickly, so that we can learn <em>new</em> tasks quickly too.</p> <p>When learning a task, we are very roughly mapping a task dataset to a set of model parameters through a function $\mathcal{D}^\mathrm{tr}_i \rightarrow \theta$. In meta learning, we are asking whether we can optimize this function for a small $\mathcal{D}_i^\mathrm{tr}$.</p> <p>There are two ways to view meta-learning algorithms:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <p><b>(1) Mechanistic view</b></p> <p>Construct a deep network that can read in an entire dataset and make predictions for new datapoints. Training this network uses a meta-dataset, which itself consists of many datasets, each for a different task.</p> </div> <div class="col-sm mt-3 mt-md-0"> <p><b>(2) Probabilistic view</b></p> <p>Extract shared prior knowledge from a set of tasks that allows for efficient learning of new tasks. Then, learning a new task uses this prior and a (small) training set to infer the most likely posterior parameters.</p> </div> </div> <h3 id="a-probabilistic-view-on-meta-learning">A probabilistic view on meta learning</h3> <div> <figure class="figure col-sm-5 float-right"> <img src="/assets/img/blog/cs330/3/mtl_graphical_model.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Graphical model of multi-task- and meta-learning.</figcaption> </figure> <p>Expanding on the probabilistic (Bayesian) view of meta learning, let’s look at a graphical model for multi-task and meta learning. To quickly recap what a graphical model represents, consider two random variables $X$ and $Y$. If there is an arrow from $X$ to $Y$ in the graphical model, it means that $p(Y\vert X) \neq P(Y)$, meaning that the random variable $Y$ is dependent on $X$. Furthermore, you can nest certain variables (the rounded squared squares with $i$ and $j$) in the figure on the right, if you would like to repeat them for different indices.</p> </div> <p>Now we can start interpreting the graphical model for multi-task learning. Merging the training and testing sets, we immediately see that the target variable of datapoint $j$ for task $i$, denoted as $y_{i,j}$, is dependent on both the input data $x_{i,j}$ and the task-specific “true” parameter(s) $\phi_i$. The only difference between the training and testing data is that the target variables of the test dataset are not observed, whilst the others are all latent variables (unobserved). As we saw, for each task, we have task-specific true parameters $\phi_i$. However, if we share some <strong>common structure</strong> between multiple tasks, we can condition these parameters on this common structure. Hence, $\theta$ defines the parameters related to the shared structure between different tasks.</p> <p>This shared structure means that task parameters $\phi_{i_1}, \phi_{i_2}$ become independent when conditioning on the shared parameters $\theta$: $\phi_{i_1} \perp \phi_{i_2} \vert \theta$. Furthermore, the entropy of $p(\phi_i \vert \theta)$ is lower than $p(\phi_i)$, as there is less distributional noise from common structure.</p> <p>Let’s now have a thought exercise. If we can identify $\theta$, then when should learning $\phi_i$ be faster than learning from scratch? Let’s think about one extreme. If the shared information fully describes the task-specific information, we would see that $p(\phi_i \vert \theta) = p(\phi_i \vert \phi_i) = 1$. From that, we can see that it is faster if there is a lot of common information about the task that is captured by $\theta$. In a more general case, if the entropy $\mathcal{H}(p(\phi_i\vert\theta)) = 0$, meaning that you can predict $\phi_i$ with 100% accuracy given $\theta$, you can learn the fastest. However, this does not necessarily mean that $\phi_i = \theta$!</p> <p>From all of this, we can define <strong>structure</strong> as a <strong>statistical dependence</strong> on <strong>shared latent information $\theta$.</strong> Let’s now see some examples of the type of information that $\theta$ may contain.</p> <ul> <li>In a multi-task sinusoid problem, $\theta$ corresponds to the family of sinusoid functions, which is everything except the phase and amplitude.</li> <li>In a multi-language machine translation problem, $\theta$ corresponds to the family of all language pairs.</li> </ul> <p>Note that $\theta$ is <strong>narrower</strong> than the space of all possible functions!</p> <p>We will discuss this probabilistic view on meta learning more in later lectures, but for the remainder of this lecture, we will switch back to the mechanistic view.</p> <h3 id="how-does-meta-learning-work">How does meta learning work?</h3> <figure class="figure col-sm-12 float-right"> <img src="/assets/img/blog/cs330/3/ml_example.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Example of a meta-learning object classification problem.</figcaption> </figure> <p>Let’s consider an image classification problem, where you have different tasks. In the problem above, the different tasks contain different images to classify. For the (meta-)training process, we have tasks $\mathcal{T}_1, \mathcal{T}_2, \cdots, \mathcal{T}_n$ and we would like to do meta-testing on a new task $\mathcal{T}_\mathrm{test}$. The goal is to learn to solve task $\mathcal{T}_\mathrm{test}$ more quickly than from scratch. We can then test after training on the few examples from the new (in this case testing) task $\mathcal{T}_\mathrm{test}$. Of course, this problem settings generalizes to any other machine learning problem like regression, language generation, skill learning, etc.</p> <p>The <b>key assumption</b> here is that meta-training tasks and meta-testing tasks are drawn i.i.d. from the same task distribution $\mathcal{T}_1, \cdots, \mathcal{T}_n,\mathcal{T}_\mathrm{test} \sim p(\mathcal{T})$, meaning that tasks must share structure.</p> <p>Analogous to more data in machine learning, the more tasks, the better! You can say that meta learning is transfer learning with many source tasks.</p> <p>The following is some terminology for different things you will hear when talking about meta learning:</p> <ul> <li>The task-specific training set $\mathcal{D}_i^\mathrm{tr}$ is often referred to as the <em>support set</em> or the <em>context</em>.</li> <li>The task test dataset $\mathcal{D}_i^\mathrm{test}$ is called the <em>query set</em>.</li> <li><em>k-shot learning</em> refers to learning with <strong>k</strong> examples per class.</li> </ul> <h3 id="a-general-recipe-for-meta-learning-algorithms">A general recipe for meta learning algorithms</h3> <p>Let’s formalize meta supervised learning in a mechanistic view. We are looking for a function $y^\mathrm{ts} = f_\theta(\mathcal{D}^\mathrm{tr}, x^\mathrm{ts})$, which is trained on the data $\{\mathcal{D}_i\}_{i=1,\cdots,n}$. This formulation reduces the meta-learning problem to the design and optimization of $f_\theta$.</p> <p>To approach a problem using meta learning, you will need to decide on two steps:</p> <ol> <li>What is my form of $f_\theta(\mathcal{D}^\mathrm{tr}, x^\mathrm{ts})$?</li> <li>How do I optimize the meta-parameters $\theta$ with respect to the maximum-likelihood objective using meta-training data.</li> </ol> <p>The following lectures will focus on core methods for meta learning and unsupervised pre-trained methods!</p> <hr/>]]></content><author><name>Lars C.P.M. Quaedvlieg</name></author><category term="deep-multi-task-and-meta-learning"/><category term="course"/><summary type="html"><![CDATA[This lecture is part of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to learn how to transfer knowledge from one task to another, discuss what it means for two tasks to share a common structure, and start thinking about meta learning.]]></summary></entry><entry><title type="html">CS-330 Lecture 1: Multi-Task Learning</title><link href="https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-mtl/" rel="alternate" type="text/html" title="CS-330 Lecture 1: Multi-Task Learning"/><published>2024-03-02T00:00:00+00:00</published><updated>2024-03-02T00:00:00+00:00</updated><id>https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-mtl</id><content type="html" xml:base="https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-mtl/"><![CDATA[<p>The goal of this lecture is to understand the key design decisions when building multi-task learning systems. Since I am still new to this blogging thing, reach out to me if you have any feedback on my writing, the flow of information, or whatever! You can contact me through <a href="https://www.linkedin.com/in/lars-quaedvlieg/">LinkedIn</a>. ☺</p> <p>The link to the lecture slides can be found <a href="https://cs330.stanford.edu/materials/cs330_multitask_transfer_2023.pdf">here</a>.</p> <h2 id="problem-statement">Problem statement</h2> <p>We will first establish some notation that will be used throughout the course. Let’s first introduce the single-task supervised learning problem.</p> \[\min_\theta \mathcal{L}(\theta, \mathcal{D}), \quad \text{s.t.} \quad \mathcal{D} = \{(x,y)_k\}\;.\] <p>Here, $\mathcal{L}$ is the loss function, $\theta$ are the model parameters and $\mathcal{D}$ is the dataset. A typical example of a loss function would be the negative log-likelihood function $\mathcal{L}(\theta, \mathcal{D}) = - \mathbb{E}\left[\log f_\theta(y\vert x)\right]$.</p> <p>We can formally define a <strong>task</strong> as follows<strong>:</strong></p> \[\mathcal{T}_i := \{p_i(x), p_i(y\vert x), \mathcal{L}_i\}\;.\] <p>Here, $p_i(x)$ is the input data distribution, $p_i(y\vert x)$ is the distribution of the target variable(s), and $\mathcal{L}_i$ is a task-specific loss function (can of course be the same for different tasks). The corresponding task datasets are $\mathcal{D}_i^\mathrm{tr} := \mathcal{D}_i$ and $\mathcal{D}_i^\mathrm{test}$.</p> <p>Some examples of tasks:</p> <ul> <li>Multi-task classification ($\mathcal{L_i}$ the same for each task)</li> <ul> <li>Per-language handwriting recognition.</li> <li>Personalized spam filter.</li> </ul> <li>Multi-label learning ($\mathcal{L_i}$ and $p_i(x)$ the same for each task)</li> <ul> <li>Face attribute recognition.</li> <li>Scene understanding. <div class="col-sm-5 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/cs330/2/weighted_mtl_objective-480.webp 480w,/assets/img/blog/cs330/2/weighted_mtl_objective-800.webp 800w,/assets/img/blog/cs330/2/weighted_mtl_objective-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/cs330/2/weighted_mtl_objective.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </li> </ul> </ul> <p>It is important to realize that $\mathcal{L}_i$ might change across tasks, for example when mixing discrete from continuous data or if there are multiple metrics that you care about.</p> <hr/> <h2 id="models-objectives-optimization">Models, objectives, optimization</h2> <p>One way of helping a model identify different tasks would be to condition the model function by a task descriptor $z_i$: $f_\theta(y\vert x, z_i)$. This could be anything ranging from user features, language descriptions, or formal task specifications. The next subsections will focus on how to condition the model, which objective should be used, and how the objective should be optimized.</p> <h3 id="model">Model</h3> <figure class="figure col-sm-10 float-right"> <img src="/assets/img/blog/cs330/2/mult_gating.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Network architecture for task-specific independent subnetworks.</figcaption> </figure> <p>Let’s first think about how we can condition on the task in order to share <strong>as little information</strong> as possible. The answer to this is simple: you can create a function that uses multiplicative gating with a one-hot encoding of the task . The model function would be $f_\theta(y \vert x, z_i) = \sum_j \mathbb{1}(z_i=j)f_{\theta_i}(x)$. This results in independent training with a single network per tasks; there are no shared parameters. This can be seen in the figure above.</p> <p>On the other extreme, you could simply concatenate $z_i$ with the input and/or activations in the model. In this case, all parameters are shared (except the ones directly following $z_i$, in case it is one-hot).</p> <p>This give rise to a question: can you phrase the multi-task learning objective parameters $\theta = \theta_\mathrm{sh} \cup \theta_i$, where $\theta_\mathrm{sh}$ are shared parameters and $\theta_i$ are task-specific parameters? Our objective function becomes the following:</p> \[\min_{\theta_\mathrm{sh}, \theta_1, \cdots, \theta_T} \sum_{i=1}^T \mathcal{L}_i(\theta_\mathrm{sh} \cup \theta_i, \mathcal{D}_i)\;.\] <p>In this case, choosing how to condition on $z_i$ is equivalent to choosing how and where to share model parameters. We will now look into some basic ways to condition a model.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/cs330/2/concat_cond-480.webp 480w,/assets/img/blog/cs330/2/concat_cond-800.webp 800w,/assets/img/blog/cs330/2/concat_cond-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/cs330/2/concat_cond.png" class="img-fluid" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figcaption class="figure-caption text-center">Concatenation-based conditioning.</figcaption> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/cs330/2/additive_cond-480.webp 480w,/assets/img/blog/cs330/2/additive_cond-800.webp 800w,/assets/img/blog/cs330/2/additive_cond-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/cs330/2/additive_cond.png" class="img-fluid" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figcaption class="figure-caption text-center">Additive conditioning.</figcaption> </div> </div> <p><br/>Can you see why additive conditioning in this way is equivalent to concatenation-based conditioning? Hint: think about how matrix multiplication splits the parameters when concatenating<d-footnote>You can find the solution to this question in the <a href="https://cs330.stanford.edu/materials/cs330_multitask_transfer_2023.pdf">lecture slides</a> (slide 13).</d-footnote>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/cs330/2/multi_head-480.webp 480w,/assets/img/blog/cs330/2/multi_head-800.webp 800w,/assets/img/blog/cs330/2/multi_head-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/cs330/2/multi_head.png" class="img-fluid" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figcaption class="figure-caption text-center">Multi-head architecture conditioning.</figcaption> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/cs330/2/mult_cond-480.webp 480w,/assets/img/blog/cs330/2/mult_cond-800.webp 800w,/assets/img/blog/cs330/2/mult_cond-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/cs330/2/mult_cond.png" class="img-fluid" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figcaption class="figure-caption text-center">Multiplicative conditioning.</figcaption> </div> </div> <p><br/></p> <p>One benefit of multiplicative conditioning is that you have this multiplicative gating, allowing more expressiveness per layer. It generalizes independent networks and independent heads.</p> <p>There are more complex conditioning techniques, and a lot of research has gone into this topic, such as Cross-Stitch Networks <d-cite key="misra2016cross"></d-cite>, Multi-Task Attention Network <d-cite key="liu2019end"></d-cite>, Deep Relation Networks <d-cite key="dai2017detecting"></d-cite>, Perceiver IO <d-cite key="jaegle2021perceiver"></d-cite>, and more.</p> <p>Unfortunately, these design choices are <strong>problem dependent</strong>, largely guided by <strong>intuition</strong> or <strong>knowledge</strong> about the problem, and currently more of an <strong>art</strong> than a science.</p> <h3 id="objectives">Objectives</h3> <p>We already saw a previous example of a multi-task objective function. Let’s start with the vanilla multi-task learning (MTL) objective: $\min_\theta \sum_{i=1}^T \mathcal{L}_i(\theta, \mathcal{D_i})$. Let’s now show some other ways to construct multi-task objective functions.</p> <ol> <li> <p>Weighted multi-task learning (manually based on priority or dynamically adjust weights throughout training):</p> \[\min_\theta \sum_{i=1}^T w_i \mathcal{L}_i(\theta, \mathcal{D_i})\;.\] </li> <li> <p>Minimax multi-task learning to optimize for the worst-case task loss (useful in robustness or fairness):</p> \[\min_\theta \max_i \mathcal{L}_i(\theta, \mathcal{D_i})\;.\] </li> <li> <p>You can use various <strong>heuristics</strong> to construct your objective function. One example is to encourage gradients to have similar magnitudes across tasks.</p> </li> </ol> <h3 id="optimization">Optimization</h3> <p>For the vanilla MTL objective, a basic training approach follows the following steps:</p> <ol> <li>Sample mini-batch of tasks $\mathcal{B} = {\mathcal{T}_i}$.</li> <li>Sample mini-batch of datapoints for each task $\mathcal{D}^b_i \sim \mathcal{D}_i$.</li> <li>Compute mini-batch loss $\hat{\mathcal{L}}(\theta, \mathcal{B}) = \sum_{\mathcal{T}_k \in \mathcal{B}} \mathcal{L}_k(\theta, \mathcal{D}_k^b)$.</li> <li>Backpropagate the loss to compute $\nabla_\theta \hat{\mathcal{L}}$.</li> <li>Perform a step of gradient descent with some optimizer.</li> <li>Repeat from step 1.</li> </ol> <p>This process ensures that tasks are sampled uniformly, regardless of data quantities. However, it is important to ensure that the task labels, and the loss function, are on the same scale.</p> <hr/> <h2 id="challenges">Challenges</h2> <p>There are multiple challenges that come with multi-task learning.</p> <ol> <li> <p><strong>Negative transfer</strong>: Sometimes independent subnetworks work better than parameter sharing. This could be due to <strong>optimization challenges</strong> (cross-task interference or tasks learning at different rates), or <strong>limited representational capacity</strong> (multi-task networks often need to be <em>much larger</em> than their single-task counterparts).</p> <p>In the case of negative transfer, you should share less across tasks. You can also add a regularization term to the objective function, to allow <em>soft parameter sharing</em>:</p> \[\min_{\theta_\mathrm{sh}, \theta_1, \cdots, \theta_T} \sum_{i=1}^T \mathcal{L}_i(\theta_\mathrm{sh} \cup \theta_i, \mathcal{D}_i) + \lambda \sum_{i^\prime = 1}^T \left\Vert \theta_i - \theta_i^\prime \right\Vert\;.\] <p>This allows for more fluid degrees of parameters sharing. However, it does add another set of hyperparameters, and it more memory intensive.</p> </li> <li><strong>Overfitting</strong>: You might not be sharing enough parameters. Since multi-task learning is equivalent to a form of regularization, the solution could be to share more parameters.</li> <li><strong>Having many tasks</strong>: You might wonder how to train all tasks together and which ones will be complementary. Unfortunately, no closed-form solution exists for measuring task similarity. Nevertheless, there are ways to approximate it from one training run <d-cite key="fifty2021efficiently"></d-cite> <d-cite key="xie2024doremi"></d-cite>.</li> </ol> <hr/> <h2 id="case-study-of-real-world-multi-task-learning">Case study of real-world multi-task learning</h2> <p>In this case study, we will discuss the paper “Recommending What Video to Watch Next: A Multitask Ranking System” <d-cite key="zhao2019recommending"></d-cite>. They introduce a large scale multi-objective ranking system for recommending what video to watch next on an industrial video sharing platform. The system faces many real-world challenges, including the presence of multiple competing ranking objectives, as well as implicit selection biases in user feedback.</p> <p>The framework is constructed as follows:</p> <ul> <li><strong>Inputs</strong>: What the user is currently watching (query video) and user features</li> </ul> <p>The procedure is the following:</p> <ol> <li>Generate a few hundred of <strong>candidate videos</strong> (by pooling videos from multiple candidate generation algorithms such as matching topics of the query video, videos frequently watched with the query video, and others).</li> <li><strong>Rank</strong> the candidates.</li> <li><strong>Serve</strong> the top ranking videos to the user.</li> </ol> <p>The central topic of this paper is the ranking system. The authors decide that the inputs to the ranking model are the <strong>query video</strong>, <strong>candidate video</strong>, and <strong>context features</strong>. The model attempts to output a weighted combination of <strong>engagement</strong> and <strong>satisfaction</strong> predictions, which results in a ranking score. The score weights are manually tuned.</p> <div> <figure class="figure col-sm-7 float-right"> <img src="/assets/img/blog/cs330/2/expert_model.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Multi-gate Mixture-of-Expert architecture.</figcaption> </figure> <p>On choice for the model architecture is a “shared-bottom model”, which has some shared bottom layers which split into separate heads for each task. However, this will harm learning when the correlation between tasks is low. Instead, they opt for a form of soft-parameter sharing that they call <b>Multi-gate Mixture-of-Experts</b> (MMoE). As you can see in the figure, this architecture allows different parts of the network to “specialize” in certain tasks as experts. For each task, an attention-like score is computed that decides which combination of experts should be used.</p> </div> <p>Formally, let’s call the expert networks $f_i(x)$. We then decide which expert to use for input $x$ and task $k$ by computing $g^k(x) = \mathrm{softmax}(W_{g^k}x)$. The features are then computed from the selected experts as $f_k(x) = \sum_{i=1}^n g_{(i)}^k(x)f_i(x)$. The output can finally be denoted by $y_k = h^k(f^k(x))$.</p> <p>In the paper, they trained them model in temporal order, running training continuously to consume newly arriving data. They perform online A/B testing in comparison to the production system based on some live metrics, and stress that model <strong>computational efficiency matters</strong>.</p> <div> <figure class="figure col-sm-7 float-right"> <img src="/assets/img/blog/cs330/2/paper_results.png" class="img-fluid" alt="Alt text."/> <figcaption class="figure-caption text-center">Results from different model configurations.</figcaption> </figure> <p>From the results, you can see that this sort of architecture definitely helps. Furthermore, they found that there was a 20% change of <b>gating polarization</b> during distributed training. This means that not all experts are utilized equally and there is a bias to some expert(s). They utilized drop-out on the experts to counteract this problem.</p> </div> <hr/>]]></content><author><name>Lars C.P.M. Quaedvlieg</name></author><category term="deep-multi-task-and-meta-learning"/><category term="course"/><summary type="html"><![CDATA[This is the first lecture of the CS-330 Deep Multi-Task and Meta Learning course, taught by Chelsea Finn in Fall 2023 at Stanford. The goal of this lecture is to understand the key design decisions when building multi-task learning systems.]]></summary></entry><entry><title type="html">CS-330: Deep Multi-Task and Meta Learning - Introduction</title><link href="https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-introduction/" rel="alternate" type="text/html" title="CS-330: Deep Multi-Task and Meta Learning - Introduction"/><published>2024-03-01T00:00:00+00:00</published><updated>2024-03-01T00:00:00+00:00</updated><id>https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-introduction</id><content type="html" xml:base="https://lars-quaedvlieg.github.io//blog/2024/cs330-stanford-introduction/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>The course <a href="https://cs330.stanford.edu/">CS 330: Deep Multi-Task and Meta Learning</a>, by <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>, is taught on a yearly basis and discusses the foundations and current state of multi-task learning and meta learning.</p> <p><strong>:warning: Note:</strong> I am discussing the content of the edition in Fall 2023, which no longer includes reinforcement learning. If you are interested in this, I will be auditing <a href="https://cs224r.stanford.edu/">CS 224R Deep Reinforcement Learning</a> later this spring, which is also taught by <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>.</p> <p>In an attempt to improve my writing skills and provide useful summaries/voice my opinions, I have decided to discuss the content of every lecture in this blog. In this post, I will give an overview of the course and why it is important for AI, especially now.</p> <p>This course will focus on solving problems that are composed of multiple tasks, and studies how structure that arises from these multiple tasks can be leveraged to learn more efficiently/effectively, including:</p> <ul> <li>Self-supervised pre-training for downstream few-shot learning and transfer learning.</li> <li>Meta-learning methods that aim to learn efficient learning algorithms that can learn new tasks quickly.</li> <li>Curriculum and lifelong learning, where the problem requires learning a sequence of tasks, leveraging their shared structure to enable knowledge transfer.</li> </ul> <hr/> <h2 id="lectures">Lectures</h2> <p>The lecture schedule of the course is as follows:</p> <ol> <li><a href="/blog/2024/cs330-stanford-mtl/">Multi-task learning</a></li> <li><a href="/blog/2024/cs330-stanford-tl-ml/">Transfer learning &amp; meta learning</a></li> <li><a href="/blog/2024/cs330-stanford-bbml-icl/">Black-box meta-learning &amp; in-context learning</a></li> <li><a href="/blog/2024/cs330-stanford-obml/">Optimization-based meta-learning</a></li> <li><a href="/blog/2024/cs330-stanford-fsl-ml/">Few-shot learning via metric learning</a></li> <li><a href="/blog/2024/cs330-stanford-upt-fsl-cl/">Unsupervised pre-training for few-shot learning (contrastive)</a></li> <li><a href="/blog/2024/cs330-stanford-upt-rbm/">Unsupervised pre-training for few-shot learning (generative)</a></li> <li>Advanced meta-learning topics (task construction)</li> <li>Variational inference</li> <li>Bayesian meta-learning</li> <li>Advanced meta-learning topics (large-scale meta-optimization)</li> <li>Lifelong learning</li> <li>Domain Adaptation and Domain Generalization</li> <li>Frontiers &amp; Open Challenges</li> </ol> <p>I am excited to start discussing these topics in greater detail! Check this page regularly for updates, since I will link to new posts whenever they are available!</p> <hr/> <h2 id="why-multi-task-and-meta-learning">Why multi-task and meta-learning?</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/cs330/1/robotics_example-480.webp 480w,/assets/img/blog/cs330/1/robotics_example-800.webp 800w,/assets/img/blog/cs330/1/robotics_example-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/cs330/1/robotics_example.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Robots are embodied in the real world, and must generalize across tasks. In order to do so, they need some common sense understanding and supervision can’t be taken for granted.</p> <p>Earlier robotics and reinforcement research mainly focused on problems that required learning a task from scratch. This problem is even present in other fields, such as object detection or speech recognition. However, as opposed to these problems, <strong>humans are generalists</strong> that exploit common structures to solve new problems more efficiently.</p> <p>Going beyond the case of generalist agents, deep multi-task and meta learning useful for any problems where a <strong>common structure</strong> can benefit the efficiency or effectiveness of a model. It can be impractical to develop models for each specific task (e.g. each robot, person, or disease), especially if the data that you have access to for these individual tasks is <strong>scarce</strong>.</p> <p>If you need to <strong>quickly learn something new</strong>, you need to utilize prior experiences (e.g. few-shot learning) to make decisions.</p> <p>But why now? Right now, with the speed of research advancements in AI, many researchers are looking into utilizing multi-model information to develop their models. Especially in robotics, foundation models seem <strong>the</strong> topic in 2024, and many advancements have been made in the past year <d-cite key="zhao2023learning"></d-cite>, <d-cite key="open_x_embodiment_rt_x_2023"></d-cite>, <d-cite key="octo_2023"></d-cite>, <d-cite key="brohan2023rt"></d-cite>.</p> <hr/> <h2 id="what-are-tasks">What are tasks?</h2> <p>Given a dataset $\mathcal{D}$ and loss function $\mathcal{L}$, we hope to develop a model $f_\theta$. Different tasks can be used to train this model, with some simple examples being objects, people, objectives, lighting conditions, words, languages, etc.</p> <p>The <strong>critical assumption</strong> here is that different tasks must share some common structure. However, in practice, this is very often the case, even for tasks that seem unrelated. For example the laws of physics and the rules of English can be shared among many tasks.</p> <ol> <li>The multi-task problem: Learn <strong>a set of tasks</strong> more quickly or more proficiently than learning them independently.</li> <li>Given data on previous task(s), learn <strong>a new task</strong> more quickly and/or more proficiently.</li> </ol> <blockquote> <p>Doesn’t multi-task learning reduce to single-task learning?</p> </blockquote> <p>This is indeed the case when aggregating data across multiple tasks, which is actually one approach to multi-task learning. However, what if you want to learn new tasks? And how do you tell the model which task to do? And what if aggregating doesn’t work?</p> <hr/>]]></content><author><name>Lars C.P.M. Quaedvlieg</name></author><category term="deep-multi-task-and-meta-learning"/><category term="course"/><summary type="html"><![CDATA[I have been incredibly interested in the recent wave of multimodal foundation models, especially in robotics and sequential decision-making. Since I never had a formal introduction to this topic, I decided to audit the Deep Multi-Task and Meta Learning course, which is taught yearly by Chelsea Finn at Stanford. I will mainly document my takes on the lectures, hopefully making it a nice read for people who would like to learn more about this topic!]]></summary></entry></feed>