@article{kingma2013auto,
    title={Auto-encoding variational bayes},
    author={Kingma, Diederik P and Welling, Max},
    journal={arXiv preprint arXiv:1312.6114},
    year={2013}
}

@article{van2017neural,
    title={Neural discrete representation learning},
    author={Van Den Oord, Aaron and Vinyals, Oriol and others},
    journal={Advances in neural information processing systems},
    volume={30},
    year={2017}
}

@article{devlin2018bert,
    title={Bert: Pre-training of deep bidirectional transformers for language understanding},
    author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
    journal={arXiv preprint arXiv:1810.04805},
    year={2018}
}

@inproceedings{he2022masked,
    title={Masked autoencoders are scalable vision learners},
    author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
    booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
    pages={16000--16009},
    year={2022}
}

@article{vaswani2017attention,
    title={Attention is all you need},
    author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
    journal={Advances in neural information processing systems},
    volume={30},
    year={2017}
}

@article{dosovitskiy2020image,
    title={An image is worth 16x16 words: Transformers for image recognition at scale},
    author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
    journal={arXiv preprint arXiv:2010.11929},
    year={2020}
}

@article{hu2021lora,
    title={Lora: Low-rank adaptation of large language models},
    author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
    journal={arXiv preprint arXiv:2106.09685},
    year={2021}
}

@article{tunstall2022efficient,
    title={Efficient few-shot learning without prompts},
    author={Tunstall, Lewis and Reimers, Nils and Jo, Unso Eun Seo and Bates, Luke and Korat, Daniel and Wasserblat, Moshe and Pereg, Oren},
    journal={arXiv preprint arXiv:2209.11055},
    year={2022}
}

@inproceedings{goyal2024think,
    title={Think before you speak: Training Language Models With Pause Tokens},
    author={Sachin Goyal and Ziwei Ji and Ankit Singh Rawat and Aditya Krishna Menon and Sanjiv Kumar and Vaishnavh Nagarajan},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=ph04CRkPdC}
}

@inproceedings{alayrac2022flamingo,
    title={Flamingo: a Visual Language Model for Few-Shot Learning},
    author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katherine Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andrew Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
    booktitle={Advances in Neural Information Processing Systems},
    editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
    year={2022},
    url={https://openreview.net/forum?id=EbMuimAbPbs}
}